{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9LvTFLmXQjl"
   },
   "source": [
    "# **Tutorial on Object detection**\n",
    "\n",
    "In this tutorial we will implement some of the steps required to build a two-stage object detection system following the [Faster-RCNN ](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)framework (Ren, Shaoqing, et al. \"Faster r-cnn: Towards real-time object detection with region proposal networks.\" Advances in neural information processing systems. 2015.).\n",
    "\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1Uw3SCTw_81eJbHFCCrMwCJCYC-EWlsMW\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Schematic of a general Faster RCNN architecture.</b> Source: https://towardsdatascience.com/faster-rcnn-object-detection-f865e5ed7fc4<br> </center></caption>\n",
    "\n",
    "\n",
    "\n",
    "The code base for these exercises is relying on the [MM-Detect](https://github.com/open-mmlab/mmdetection) toolbox. (Chen, Kai, et al. \"MMDetection: Open mmlab detection toolbox and benchmark.\" arXiv preprint arXiv:1906.07155 (2019).)\n",
    "\n",
    "In this setting, we will apply what we have learnt regarding two-stage object detection schemes for the detection of white matter lesions using the training data used for lecture 5. We will first mount the google drive and download the data as in lecture 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prj2VZwzafOJ"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1eN8lKId0SvJ8Z-DKDHo5coN8iZCTI2gX\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>First step: Import libraries</b><br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3704,
     "status": "ok",
     "timestamp": 1614679796095,
     "user": {
      "displayName": "Virginia Fernandez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2-wSJVduwlL_y-ZWh72X1gSpTnf056ew0V1dc=s64",
      "userId": "03836377914834337303"
     },
     "user_tz": 0
    },
    "id": "TuhrnxqeX9gN",
    "outputId": "f09a6f90-55da-4853-dddf-88c65e1faaea"
   },
   "outputs": [],
   "source": [
    "!pip install torchio\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TorchIO read, preprocess, sample, augment, and write 3D medical images. https://github.com/fepegar/torchio\n",
    "import torchio\n",
    "from torchio import Image, Subject\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Deep Learning library\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Torchvision consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import RandomCrop\n",
    "\n",
    "from PIL import Image as impil\n",
    "from PIL import ImageDraw as imdraw\n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import cm\n",
    "from scipy.ndimage.measurements import label \n",
    "\n",
    "# The device used can be changed by clicking on Runtime - Change runtime type - GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6911,
     "status": "ok",
     "timestamp": 1614679799546,
     "user": {
      "displayName": "Virginia Fernandez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2-wSJVduwlL_y-ZWh72X1gSpTnf056ew0V1dc=s64",
      "userId": "03836377914834337303"
     },
     "user_tz": 0
    },
    "id": "46QVKqI7N5P4",
    "outputId": "af2e75ea-ccd9-440f-f055-3dfc0d829a7a"
   },
   "outputs": [],
   "source": [
    "file_download_link = \"https://github.com/KCL-BMEIS/AdvancedMachineLearningCourse/blob/main/Week8_Object_Detection/Data/data.tar?raw=true\"\n",
    "!wget -O data.tar --no-check-certificate \"$file_download_link\"\n",
    "\n",
    "# Decompress files\n",
    "!tar -xf data.tar\n",
    "\n",
    "file_download_link = \"https://github.com/KCL-BMEIS/AdvancedMachineLearningCourse/blob/main/Week8_Object_Detection/Data/models.tar?raw=true\"\n",
    "!wget -O models.tar --no-check-certificate \"$file_download_link\"\n",
    "\n",
    "# Decompress files\n",
    "!tar -xf models.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH9IqfbEbq0E"
   },
   "source": [
    "We define below the function we need to get our training set and prepare it for future use and initialize the weights of our network(s).\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SsvX_M6sYz6"
   },
   "outputs": [],
   "source": [
    "def sorted_aphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c\n",
    "    ) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "def getiosubjects(path_dir):\n",
    "    subjects_list = sorted_aphanumeric(os.listdir(path_dir + '/flair/'))\n",
    "    subjects_list = [s for s in subjects_list if s[0:2] != \"._\"] # We need to get rid of these files\n",
    "    iosubjects = []\n",
    "    for slices in subjects_list:\n",
    "        subject = torchio.Subject(\n",
    "          flair=torchio.ScalarImage(path_dir + '/flair/' + slices),\n",
    "          label=torchio.LabelMap(path_dir + '/labels/wmh' +  slices.split('FLAIR')[1])\n",
    "        )\n",
    "        iosubjects.append(subject)\n",
    "\n",
    "    return iosubjects\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def multi_apply(func, *args, **kwargs):\n",
    "    pfunc = partial(func, **kwargs) if kwargs else func\n",
    "    map_results = map(pfunc, *args)\n",
    "    return tuple(map(list, zip(*map_results)))\n",
    "\n",
    "def freeze_model(model):\n",
    "    model.eval()\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = False\n",
    "\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    model.train()\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = True \n",
    "\n",
    "def unmap(data, count, inds, fill=0):\n",
    "    \"\"\" Unmap a subset of item (data) back to the original set of items (of\n",
    "    size count) \"\"\"\n",
    "    if data.dim() == 1:\n",
    "        ret = data.new_full((count,), fill)\n",
    "        ret[inds] = data\n",
    "    else:\n",
    "        new_size = (count,) + data.size()[1:]\n",
    "        ret = data.new_full(new_size, fill)\n",
    "        ret[inds, :] = data\n",
    "    return ret\n",
    "\n",
    "path = ''\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "training_dir ='data' \n",
    "training_subjects = getiosubjects(training_dir)\n",
    "\n",
    "# Training Sets\n",
    "dataset_training = torchio.SubjectsDataset(training_subjects)\n",
    "\n",
    "training_loader = DataLoader(dataset_training, shuffle=True, batch_size=6)\n",
    "\n",
    "def initialize_weights(*models):\n",
    "    for model in models:\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE8QClCWaDs_"
   },
   "source": [
    "The module \"torchio.ImagesDataset\" can read through DICOM or Nifti Images and returns a dictionary when you call method \"getitem\". That dictionary contains the different modalities or labels passed when you create the Dataset (i.e., 'FLAIR', 'T1', 'label') as keys, and each item of that dictionary is another dictionary with, at least:\r\n",
    "- 'data': torch.tensor with the image itself\r\n",
    "- 'affine': affine transformation of the Nifti or DICOM Image\r\n",
    "- 'path': path to the image\r\n",
    "- 'type': whether it is a label or an intensity image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4684,
     "status": "ok",
     "timestamp": 1614679799895,
     "user": {
      "displayName": "Virginia Fernandez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2-wSJVduwlL_y-ZWh72X1gSpTnf056ew0V1dc=s64",
      "userId": "03836377914834337303"
     },
     "user_tz": 0
    },
    "id": "gsBDTsDXataP",
    "outputId": "93ac91ad-2f22-4f38-f0ff-4a01d1d67fa8"
   },
   "outputs": [],
   "source": [
    "# If you want to explore the fields of what torchio.ImagesDataset returns, do:\r\n",
    "item = dataset_training.__getitem__(0)\r\n",
    "\r\n",
    "print(\"Each item contains... \", item.keys(), '\\n')\r\n",
    "\r\n",
    "print(\"And if you go to each of the elements...\" , '\\n')\r\n",
    "\r\n",
    "print(\"...you've got the image...\")\r\n",
    "print(item['flair']['data'], '\\n')\r\n",
    "\r\n",
    "print(\"...you've got the type: \", item['flair']['type'], '\\n')\r\n",
    "\r\n",
    "print(\"...you've got the affine transformation...\")\r\n",
    "print(item['flair']['affine'], '\\n')\r\n",
    "\r\n",
    "print(\"... and you've got the path:\", item['flair']['path'], '\\n')\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2_SWx3nc3nn"
   },
   "source": [
    "# **Part 1 - Defining ground truth boxes and generating anchors** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhcuGMGA0jXo"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1A0VrdkcOSoJrZK16GtUwfsrrX_JoiV9E\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Creating bounding boxes based on the segmentations</b><br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsmfiKFmRt1c"
   },
   "source": [
    "## **Defining GT boxes**\n",
    " The first step consists in defining the boxes to detect from the ground truth segmentation. A box is defined by its top left corner (min_x, min_y) and its bottom right corner (max_x, max_y). In the following snippet, complete `create_gt_from_labels` function so that the function has two outputs: the flattened list of ground truth bounding boxes (shape is #gt_boxes, 5) as a torch tensor where the first index indicates the image to which the box corresponds to and a list of tensor (one tensor for each image in the batch)\n",
    "\n",
    " The function `plot_boxes_image` will be used in the following to plot the different boxes on the image studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHYwv5AW0CXC"
   },
   "outputs": [],
   "source": [
    "def create_gt_from_labels(batch_labels):\n",
    "  # batch_labels should be a list containing our labels. \n",
    "    list_gtbbox_full = []\n",
    "    list_gtflat = []\n",
    "    for (i, gtseg) in enumerate(batch_labels):\n",
    "        # Hint: shape of gtseg: C x H x W (channel, height, width), so 1 x 128 x 128.\n",
    "        # Hint: for a certain binary segmentation map, the function 'label'  imported\n",
    "        # above returns a segmentation map with a different number assigned to each \n",
    "        # separate structure (here, lesion) found on the map. It also returns the number\n",
    "        # of structures.  \n",
    "        temp_label, numb_lab = label(np.squeeze(gtseg)) # We need squeeze because label needs a HxW image,not a 1xHxW. \n",
    "        list_gtbbox = []\n",
    "        for lab in range(1, numb_lab + 1): # Loop along different lesions\n",
    "            seg_temp = (temp_label == lab) # We each of the lesions\n",
    "            indices = np.asarray(np.where(seg_temp == 1)).T # We get all the coordinates of these pixels\n",
    "            mins = np.min(indices, axis=0) # Minimum height and width\n",
    "            maxs = np.max(indices, 0) # Maximum height and width\n",
    "            corners = list(mins) + list(maxs) # [Hmin, Wmin, Hmax, Wmax]\n",
    "            list_gtbbox.append(corners)\n",
    "            list_gtflat.append(list((i,)) + list(mins) + list(maxs)) # [BOX_INDEX, Hmin, Wmin, Hmax, Wmax]\n",
    "        list_gtbbox_full.append(torch.from_numpy(np.asarray(list_gtbbox)))\n",
    "    return torch.from_numpy(np.asarray(list_gtflat)), list_gtbbox_full\n",
    "\n",
    "\n",
    "def plot_boxes_image(batch_images, batch_labels, boxes=None, seg_backbone=None,scores=None): \n",
    "    \"\"\"\n",
    "    This function plots the ground boxes corresponding to the batches presented. \n",
    "    Additional boxes can be presented along with the backbone output and or the scores for the proposed boxes\n",
    "    \"\"\"\n",
    "    gtflat, gtfull = create_gt_from_labels(batch_labels)\n",
    "    list_seg = []\n",
    "    list_rect = []\n",
    "    cmap = cm.get_cmap('YlOrRd', 100)\n",
    "    cmap_values = cmap(np.linspace(0, 1, 100))\n",
    "    \n",
    "    if boxes is not None:\n",
    "        if scores is None:\n",
    "            for (gtlist, bblist, image, label) in zip(gtfull, boxes, batch_images, batch_labels):\n",
    "                transfo = image.numpy()\n",
    "                transfo = (transfo - np.min(transfo))/(np.max(transfo)- np.min(transfo)) * 255\n",
    "                transfo = np.squeeze(transfo.astype('uint8')).T\n",
    "                transfo = np.tile(np.expand_dims(transfo, -1), [1,1,3])\n",
    "                im = impil.fromarray(transfo)\n",
    "                im_seg = impil.fromarray(transfo)\n",
    "                draw = imdraw.Draw(im)\n",
    "                draw_seg = imdraw.Draw(im_seg)\n",
    "                for (i,bbox) in enumerate(gtlist):\n",
    "                    # print(bbox)\n",
    "                    bbox_draw = ((bbox[0], bbox[1]), (bbox[2], bbox[3]))\n",
    "                    draw_seg.rectangle(bbox_draw, fill=None, outline='#00CCCC')\n",
    "                for (i,bbox) in enumerate(bblist):\n",
    "                    # print(bbox)\n",
    "                    bbox_draw = ((bbox[0], bbox[1]), (bbox[2], bbox[3]))\n",
    "                    value_color = 'red'\n",
    "                    draw.rectangle(bbox_draw, fill=None, outline=value_color)\n",
    "                list_seg.append(im_seg)\n",
    "                list_rect.append(im)\n",
    "        if scores is not None:\n",
    "            for (gtlist, bblist, image, label, score) in zip(gtfull, boxes, batch_images, batch_labels, scores):\n",
    "                transfo = image.numpy()\n",
    "                transfo = (transfo - np.min(transfo))/(np.max(transfo)- np.min(transfo)) * 255\n",
    "                transfo = np.squeeze(transfo.astype('uint8')).T\n",
    "                transfo = np.tile(np.expand_dims(transfo, -1), [1,1,3])\n",
    "                im = impil.fromarray(transfo)\n",
    "                im_seg = impil.fromarray(transfo)\n",
    "                draw = imdraw.Draw(im)\n",
    "                draw_seg = imdraw.Draw(im_seg)\n",
    "                for (i,bbox) in enumerate(gtlist):\n",
    "                    # print(bbox)\n",
    "                    bbox_draw = ((bbox[0], bbox[1]), (bbox[2], bbox[3]))\n",
    "                    draw_seg.rectangle(bbox_draw, fill=None, outline='#00CCCC')\n",
    "                for (i,bbox) in enumerate(bblist):\n",
    "                    # print(bbox)\n",
    "                    bbox_draw = ((bbox[0], bbox[1]), (bbox[2], bbox[3]))\n",
    "                    value_score = score[i]\n",
    "                    map_cmap = np.maximum(np.round((value_score - 0.5) * 200),0)\n",
    "                    map_cmap = np.maximum(np.minimum(map_cmap, 99),0)\n",
    "                    value_color = cmap_values[int(map_cmap),:-1] * 255\n",
    "                    draw.rectangle(bbox_draw, fill=None, outline=tuple(value_color.astype(int)))\n",
    "                list_seg.append(im_seg)\n",
    "                list_rect.append(im)\n",
    "                    \n",
    "    else:\n",
    "        for (gtlist, image, label) in zip(gtfull, batch_images, batch_labels):\n",
    "            transfo = image.numpy()\n",
    "            transfo = (transfo - np.min(transfo))/(np.max(transfo)- np.min(transfo)) * 255\n",
    "            transfo = np.squeeze(transfo.astype('uint8')).T\n",
    "            transfo = np.tile(np.expand_dims(transfo, -1), [1,1,3])\n",
    "            im = impil.fromarray(transfo)\n",
    "            im_seg = impil.fromarray(np.squeeze(label.numpy()*255).T)\n",
    "            draw = imdraw.Draw(im)\n",
    "            for bbox in gtlist:\n",
    "                # print(bbox)\n",
    "                bbox_draw = ((bbox[0], bbox[1]), (bbox[2], bbox[3]))\n",
    "                draw.rectangle(bbox_draw, fill=None, outline='red')\n",
    "            \n",
    "            list_seg.append(im_seg)\n",
    "            list_rect.append(im)\n",
    "    fig2 = plt.figure(figsize=(20,10))\n",
    "    if seg_backbone is None:\n",
    "        spec2 = GridSpec(ncols=len(gtfull), nrows=2, figure=fig2)\n",
    "        for i in range(len(list_rect)):\n",
    "            f2_ax1 = fig2.add_subplot(spec2[0, i])\n",
    "            f2_ax1.imshow(list_rect[i])\n",
    "            f2_ax2 = fig2.add_subplot(spec2[1,i])\n",
    "            f2_ax2.imshow(list_seg[i])\n",
    "    else:\n",
    "        seg_back = seg_backbone.detach().numpy()\n",
    "        seg_back = np.squeeze(seg_back)\n",
    "        spec2 = GridSpec(ncols=len(gtfull), nrows=3, figure=fig2)\n",
    "        for i in range(len(list_rect)):\n",
    "            f2_ax1 = fig2.add_subplot(spec2[0, i])\n",
    "            f2_ax1.imshow(list_rect[i])\n",
    "            f2_ax2 = fig2.add_subplot(spec2[1,i])\n",
    "            f2_ax2.imshow(list_seg[i])\n",
    "            f2_ax3 = fig2.add_subplot(spec2[2, i])\n",
    "            f2_ax3.imshow(seg_back[i,:,:].T)\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMnRlUEde3oz"
   },
   "source": [
    "Let us test out if we can produce the appropriate boxes for the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 4811,
     "status": "ok",
     "timestamp": 1614679802336,
     "user": {
      "displayName": "Virginia Fernandez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2-wSJVduwlL_y-ZWh72X1gSpTnf056ew0V1dc=s64",
      "userId": "03836377914834337303"
     },
     "user_tz": 0
    },
    "id": "nG2DQGJK1Dp1",
    "outputId": "36611d01-dce1-4c02-d378-bcb215af9554"
   },
   "outputs": [],
   "source": [
    "# Select a batch in our training set\n",
    "patch_s = iter(training_loader).next()\n",
    "\n",
    "# Define our images and associated ground truth\n",
    "batch_images = patch_s['flair']['data'][..., 0]\n",
    "batch_labels = patch_s['label']['data'][..., 0]\n",
    "# plot our images with associated ground truth label\n",
    "plot_boxes_image(batch_images, batch_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZB5E7YIef4ur"
   },
   "source": [
    "## **Generating anchors**\n",
    "\n",
    "Now that we have defined our ground truth, we need to define the generation of anchors that will be used by the region proposal network (RPN).\n",
    "\n",
    "At a given location, multiple anchors are possible. They are characterised by their basic size, their scale (multiplicator of the basic size), their ratios (ratio between width and height).\n",
    "\n",
    "The class AnchorGenerator allows you to create bounding boxes associated to the different anchors according to these parameters. First we create using `gen_base_anchors` the 9 types of anchors that will then be defined at each cell location of the feature map (via the `grid_anchors` function)\n",
    "\n",
    "In addition to the characteristics of the anchors, the stride parameter indicates the space in between center locations at the image level. This is chosen as the scale reduction between the image map and the feature map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-4a4-GEYhZ3"
   },
   "outputs": [],
   "source": [
    "# Definition of the anchors\n",
    "class AnchorGenerator(object):\n",
    "\n",
    "\n",
    "    def __init__(self, base_size, scales, ratios):\n",
    "        self.base_size = base_size # 2-element tuple or list\n",
    "        self.scales = torch.Tensor(scales)\n",
    "        self.ratios = torch.Tensor(ratios)\n",
    "        self.base_anchors = self.gen_base_anchors()\n",
    "\n",
    "    @property\n",
    "    def num_base_anchors(self):\n",
    "        return self.base_anchors.size(0)\n",
    "\n",
    "    def gen_base_anchors(self):\n",
    "        # Generate the different types of anchors for a given generic location\n",
    "        # Hint: have a look at the Worked example in the slides\n",
    "        # Hint: the coordinates will be relative for now, and then will be applied\n",
    "        # to specific locations (coordinates x,y in the image.)\n",
    "        w = self.base_size[0]\n",
    "        h = self.base_size[1]\n",
    "        x_ctr = 0.5 * (w - 1)\n",
    "        y_ctr = 0.5 * (h - 1)\n",
    "        h_ratios = torch.sqrt(self.ratios) # h/ w\n",
    "        w_ratios = 1 / h_ratios\n",
    "        ws = (w * w_ratios[:, None] * self.scales[None, :]).view(-1)\n",
    "        hs = (h * h_ratios[:, None] * self.scales[None, :]).view(-1)\n",
    "        \n",
    "        base_anchors = torch.stack(\n",
    "            [\n",
    "                x_ctr - 0.5 * (ws - 1), y_ctr - 0.5 * (hs - 1),\n",
    "                x_ctr + 0.5 * (ws - 1), y_ctr + 0.5 * (hs - 1)\n",
    "            ],\n",
    "            dim=-1).round()\n",
    "\n",
    "            # Combinations of ratios and anchors\n",
    "        return base_anchors\n",
    "\n",
    "    def _meshgrid(self, x, y):\n",
    "        xx = x.repeat(len(y))\n",
    "        yy = y.view(-1, 1).repeat(1, len(x)).view(-1)\n",
    "        return xx, yy\n",
    "\n",
    "    def _meshgrid2(self, x, y):\n",
    "        yy = y.repeat(len(x))\n",
    "        xx = x.view(-1, 1).repeat(1, len(y)).view(-1)\n",
    "        return xx, yy\n",
    "\n",
    "    def grid_anchors(self, featmap_size, stride=1):\n",
    "\n",
    "        base_anchors = self.base_anchors\n",
    "        feat_h, feat_w = featmap_size # Careful here: Y is the ROW, X is the COLUMN. \n",
    "        shift_x = torch.arange(0, feat_w) * stride # Coordinates in the row dimension.\n",
    "        shift_y = torch.arange(0, feat_h) * stride # Coordinates in the column dimension. \n",
    "\n",
    "        shift_xx, shift_yy = self._meshgrid2(shift_x, shift_y)\n",
    "\n",
    "        # Now, you will use one grid of these to define each coordinate of the anchor box.\n",
    "        shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)\n",
    "        shifts = shifts.type_as(base_anchors)\n",
    "\n",
    "        # Dimension recap:\n",
    "        # Our shifts are of Kx4 dimensions, where K is the number of different coordinates possible in the feature map.\n",
    "        #   The first feat_w elements correspond to the first row of shifts. \n",
    "        # Anchors are of dimensions Ax4, where A is the number of possible anchors per point.\n",
    "        # Now, we want to merge A and K to obtain all combination of anchors or shifted anchors (dimension [K, A, 4]), \n",
    "        # and then reshape that to have dimensions (KxA, 4), that is, a list of anchors, where the anchors is defined\n",
    "        # by its 4 coordinates. \n",
    "\n",
    "        all_anchors = base_anchors[None, :, :] + shifts[:, None, :] # None adds one dimension. base_anchors was 9 x 4, and like that it's _, 9 x 4.\n",
    "        all_anchors = all_anchors.view(-1, 4) # We combine all the shifts and boxes combinations on a flat vector. \n",
    "        # first A rows correspond to A anchors of (0, 0) in feature map,\n",
    "        # then (0, 1), (0, 2), ...\n",
    "        return all_anchors\n",
    "\n",
    "    def valid_flags(self, featmap_size, valid_size, device=device):\n",
    "        # Based on valid_size, returns a 1s and 0s grid with 1s whenever we\n",
    "        # are in a region where valid_size fits the feature map and 0s out of it. \n",
    "        feat_h, feat_w = featmap_size\n",
    "        valid_h, valid_w = valid_size\n",
    "        assert valid_h <= feat_h and valid_w <= feat_w\n",
    "        valid_x = torch.zeros(feat_w).type(torch.BoolTensor)\n",
    "        valid_y = torch.zeros(feat_h).type(torch.BoolTensor)\n",
    "        valid_x[:valid_w] = 1\n",
    "        valid_y[:valid_h] = 1\n",
    "        valid_xx, valid_yy = self._meshgrid2(valid_x, valid_y)\n",
    "        valid = valid_xx & valid_yy\n",
    "        valid = valid[:,\n",
    "                None].expand(valid.size(0),\n",
    "                             self.num_base_anchors).contiguous().view(-1)\n",
    "        return valid\n",
    "\n",
    "\n",
    "\n",
    "def anchor_inside_flags(flat_anchors,\n",
    "                        valid_flags,\n",
    "                        img_shape,\n",
    "                        allowed_border=0, mask=None):\n",
    "    # We return the valid anchors that are fit within a mask (img_shape. )\n",
    "    img_h, img_w = img_shape[:2]\n",
    "    flat_anchors = flat_anchors.type(torch.LongTensor)\n",
    "    valid_flags = valid_flags.type(torch.BoolTensor)\n",
    "    if allowed_border >= 0:\n",
    "        inside_flags = valid_flags & \\\n",
    "                       (flat_anchors[:, 0] >= -allowed_border).type(\n",
    "                           torch.BoolTensor) & \\\n",
    "                       (flat_anchors[:, 1] >= -allowed_border).type(\n",
    "                           torch.BoolTensor) & \\\n",
    "                       (flat_anchors[:, 2] < img_w + allowed_border).type(\n",
    "                           torch.BoolTensor) & \\\n",
    "                       (flat_anchors[:, 3] < img_h + allowed_border).type(\n",
    "                           torch.BoolTensor)\n",
    "    else:\n",
    "        inside_flags = valid_flags\n",
    "    if mask is not None:\n",
    "        valid_mask = mask.view(-1).repeat_interleave(int(flat_anchors.shape[0]/(\n",
    "            mask.shape[-2]*mask.shape[-1])))\n",
    "        inside_flags *= valid_mask.type(torch.BoolTensor)\n",
    "    return inside_flags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSflL7Sci8fP"
   },
   "source": [
    "Let us visualise where the anchors lay and what they look like. First complete the function the `find_anchors_from_index` function that provide the list of indices for the anchors at a given location provided as x,y coordinates.\r\n",
    "\r\n",
    "Try then to display on the given data and play with the different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2996,
     "status": "ok",
     "timestamp": 1614679805341,
     "user": {
      "displayName": "Virginia Fernandez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2-wSJVduwlL_y-ZWh72X1gSpTnf056ew0V1dc=s64",
      "userId": "03836377914834337303"
     },
     "user_tz": 0
    },
    "id": "-3Uh7YnEPyfy",
    "outputId": "7e9e47d2-b57e-4178-8004-50d341850200"
   },
   "outputs": [],
   "source": [
    "# Plot associated anchors for a given index on img\n",
    "def find_anchors_from_index(x,y,img_shape=[128,128], num_anchors=9):\n",
    "    # This function returns the indices of the anchors associated to a certain\n",
    "    # set of coordinates (x,y) in the feature map. \n",
    "    # Remember that our anchors list has dimensions: (AxK)x4:\n",
    "    # ---- every A elements of the list, we change (X,Y) point in the feature map.\n",
    "    anchors_begin = (x * img_shape[1] + y)* num_anchors\n",
    "    return np.arange(anchors_begin, anchors_begin + num_anchors)\n",
    "\n",
    "def plot_anchors_index(img, idx, size=(3,3), scales=(0.5,1,2), ratios=(0.5,1,2)):\n",
    "    # Plot achors associated to a certain index (X, Y)\n",
    "    img = np.squeeze(img)\n",
    "    anchors_gen = AnchorGenerator((3,3),[0.5,1,2],[0.5,1,2])\n",
    "    anchors_full = anchors_gen.grid_anchors(img.shape, stride=1)\n",
    "    anchors_indices = find_anchors_from_index(idx[0], idx[1], img.shape, 9)\n",
    "    anchors_select = anchors_full[anchors_indices,:]\n",
    "    transfo = (img-np.min(img))/(np.max(img)-np.min(img))*255\n",
    "    transfo = transfo.astype('uint8').T\n",
    "    transfo = np.tile(np.expand_dims(transfo,-1),[1,1,3])\n",
    "    img_pil = impil.fromarray(transfo)\n",
    "    list_img = []\n",
    "    fig2 = plt.figure(figsize=(14,14))\n",
    "    spec2 = GridSpec(3, 3, figure=fig2)\n",
    "    for (i,f) in enumerate(anchors_select):\n",
    "        img_pil = impil.fromarray(transfo)\n",
    "        draw = imdraw.Draw(img_pil)\n",
    "        draw.rectangle(((f[0],f[1]),(f[2],f[3])),fill=None,outline='#00CCCC',width=1)\n",
    "        print(f)\n",
    "        f2_ax1 = fig2.add_subplot(spec2[round(i//3), i%3])\n",
    "        f2_ax1.imshow(img_pil)\n",
    "\n",
    "\n",
    "flair_nii = nib.load('data/flair/FLAIR_2_slice_18.nii.gz')\n",
    "data  = flair_nii.get_data()\n",
    "plot_anchors_index(data, [50,70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgDqhR1-lPjq"
   },
   "source": [
    "# **Part 2 - Defining overlap and assigning boxes and proposals**\n",
    "\n",
    "As of now, we are able to define boxes based from our segmentation and we can generate possible anchors across the image. The next step consists in building the necessary functionalities so as to be able to match ground truth boxes and proposed anchors.\n",
    "\n",
    "To do so we first need to define measures of overlap and in particular the intersection over union.\n",
    "\n",
    "Complete the function `bbox_overlaps` that takes as argument the ground_truth boxes and the proposed anchors and returns the matrix of corresponding IoU measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyc1cBsLtmiu"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1Gne5ugceIT4RKzFbWYMw_2i8lTrCW-HX\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Defining Intersection over Union (IoU) function</b><br> </center></caption>\n",
    "\n",
    "\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1JBzGka2h76Qy9v9S175mRxbPNf1CI_jO\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Intersection over Union (IoU) example</b><br> </center></caption>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkBpTiVlnGee"
   },
   "outputs": [],
   "source": [
    "def bbox_overlaps(bboxes1, bboxes2, mode='iou'):\n",
    "    \"\"\"Calculate overlap between two set of bboxes.\n",
    "\n",
    "    Args:\n",
    "        bboxes1 (Tensor): shape (m, 4) in <x1, y1, x2, y2> format.\n",
    "        bboxes2 (Tensor): shape (n, 4) in <x1, y1, x2, y2> format.\n",
    "        mode (str): \"iou\" (intersection over union) or iof (intersection over\n",
    "            foreground).\n",
    "\n",
    "    Returns:\n",
    "        ious(Tensor): shape (m, n) \"\"\"\n",
    "\n",
    "    assert mode in ['iou', 'iof']\n",
    "    bboxes1 = bboxes1.type(torch.LongTensor)\n",
    "    bboxes2 = bboxes2.type(torch.LongTensor)\n",
    "    rows = bboxes1.size(0) \n",
    "    cols = bboxes2.size(0)\n",
    "    if rows * cols == 0:\n",
    "        return bboxes1.new(rows, cols)\n",
    "\n",
    "    else:\n",
    "        # Take the minimums and maximums of each of boxes being compared.\n",
    "        # The :2 takes coordinates 0 and 1 from the 4, so the upper-left corner coordinates (lt: less than)\n",
    "        # The 2: does the opposite, so it takes those from the lower right corner (the furthest)\n",
    "\n",
    "        lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2] # The None adds an additional dimension. \n",
    "        rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]\n",
    "\n",
    "        wh = (rb - lt + 1).clamp(min=0)  # [rows, cols, 2]\n",
    "        overlap = wh[:, :, 0] * wh[:, :, 1]\n",
    "        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n",
    "                bboxes1[:, 3] - bboxes1[:, 1] + 1)\n",
    "        overlap = overlap.type(torch.FloatTensor)\n",
    "        area1 = area1.type(torch.FloatTensor)\n",
    "\n",
    "        if mode == 'iou':\n",
    "            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n",
    "                    bboxes2[:, 3] - bboxes2[:, 1] + 1)\n",
    "            area2 = area2.type(torch.FloatTensor)\n",
    "            ious = overlap / (area1[:, None] + area2 - overlap)\n",
    "\n",
    "        else:\n",
    "            ious = overlap / (area1[:, None])\n",
    "\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1614679805941,
     "user": {
      "displayName": "Virginia Fernandez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2-wSJVduwlL_y-ZWh72X1gSpTnf056ew0V1dc=s64",
      "userId": "03836377914834337303"
     },
     "user_tz": 0
    },
    "id": "733cvKkpapLN",
    "outputId": "fa2f1ef9-a902-4ed0-87d0-d8b1f6f88c7f"
   },
   "outputs": [],
   "source": [
    "# Example of IOU \r\n",
    "box_1 = torch.tensor([[4, 10, 12, 18], [40, 30, 62, 38]])\r\n",
    "box_2 = torch.tensor([[5, 16, 9, 20], [55, 24, 60, 35]])\r\n",
    "box_3 = torch.tensor([[5, 9, 11, 17],[41, 30, 65, 35]])\r\n",
    "print(\"Box 1 and 2 = \", bbox_overlaps(box_1, box_2, mode = 'iou'))\r\n",
    "print(\"Box 1 and 3 = \",  bbox_overlaps(box_1, box_3, mode = 'iou'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDiGspZYxJWi"
   },
   "source": [
    "We can calculate the IoU between two set of boxes. Complete the required blanks in the `MaxIoUAssigner` class following the assignment algorithm presented in the lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnERSV4fUMuD"
   },
   "source": [
    "The idea behind the following block of code is that for N bounding boxes (proposals) and M ground truths, you will assign the best proposals among the N bounding boxes to the M ground truths.\r\n",
    "You will do this with these two classes:\r\n",
    " \r\n",
    "1.   AssignResult, which contains the final resulting overlaps, and the ground truth indices (from 1 to M) assigned to each of the (1 to N) boxes etc.\r\n",
    "2.   MaxIoUAssigner, which will return an object of the class AssignResult by computing the overlaps of the boxes with the ground truths using assign_wrt_overlaps.\r\n",
    "\r\n",
    "*Notice that you assign a ground truth if the IoU goes above a certain threshold. What if there are bounding boxes that don't match any ground truth? they will be assigned 0 (background). The label -1 will be considered as a \"don't care\" label.*\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4-14kzd1ond"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=17xXgo1fCyfPnn0VRLhPu-T90fDF3QTlV\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Non-Max suppression example</b> Source: https://www.kdnuggets.com/2018/09/object-detection-image-classification-yolo.html <br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn-t8IuUd_aP"
   },
   "source": [
    "\r\n",
    "Read and fill the function. If you need to understand some of the structures that the function assign_wrt_overlaps returns, refer to the picture below the code.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaXfsew-TcKJ"
   },
   "outputs": [],
   "source": [
    "class AssignResult():\n",
    "    \"\"\"\n",
    "    Stores assignments between predicted and truth boxes.\n",
    "\n",
    "    Attributes:\n",
    "        num_gts (int): the number of truth boxes considered when computing this\n",
    "            assignment\n",
    "\n",
    "        gt_inds (LongTensor): for each predicted box indicates the 1-based\n",
    "            index of the assigned truth box. 0 means unassigned and -1 means\n",
    "            ignore.\n",
    "\n",
    "        max_overlaps (FloatTensor): the iou between the predicted box and its\n",
    "            assigned truth box.\n",
    "\n",
    "        labels (None | LongTensor): If specified, for each predicted box\n",
    "            indicates the category label of the assigned truth box.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, num_gts, gt_inds, max_overlaps, labels=None):\n",
    "        self.num_gts = num_gts\n",
    "        self.gt_inds = gt_inds\n",
    "        self.max_overlaps = max_overlaps\n",
    "        self.labels = labels\n",
    "\n",
    "class MaxIoUAssigner():\n",
    "    \"\"\"Assign a corresponding gt bbox or background to each bbox.\n",
    "\n",
    "    Each proposals will be assigned with `-1`, `0`, or a positive integer\n",
    "    indicating the ground truth index.\n",
    "\n",
    "    - -1: don't care\n",
    "    - 0: negative sample, no assigned gt\n",
    "    - positive integer: positive sample, index (1-based) of assigned gt\n",
    "\n",
    "    Args:\n",
    "        pos_iou_thr (float): IoU threshold for positive bboxes.\n",
    "        neg_iou_thr (float or tuple): IoU threshold for negative bboxes.\n",
    "        min_pos_iou (float): Minimum iou for a bbox to be considered as a\n",
    "            positive bbox. Positive samples can have smaller IoU than\n",
    "            pos_iou_thr due to the 4th step (assign max IoU sample to each gt).\n",
    "        gt_max_assign_all (bool): Whether to assign all bboxes with the same\n",
    "            highest overlap with some gt to that gt.\n",
    "        ignore_iof_thr (float): IoF threshold for ignoring bboxes (if\n",
    "            `gt_bboxes_ignore` is specified). Negative values mean not\n",
    "            ignoring any bboxes.\n",
    "        ignore_wrt_candidates (bool): Whether to compute the iof between\n",
    "            `bboxes` and `gt_bboxes_ignore`, or the contrary.\n",
    "        gpu_assign_thr (int): The upper bound of the number of GT for GPU\n",
    "            assign. When the number of gt is above this threshold, will assign\n",
    "            on CPU device. Negative values mean not assign on CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pos_iou_thr,\n",
    "                 neg_iou_thr,\n",
    "                 min_pos_iou=.0,\n",
    "                 gt_max_assign_all=True,\n",
    "                 ignore_iof_thr=-1,\n",
    "                 ignore_wrt_candidates=True,\n",
    "                 gpu_assign_thr=-1):\n",
    "        self.pos_iou_thr = pos_iou_thr\n",
    "        self.neg_iou_thr = neg_iou_thr\n",
    "        self.min_pos_iou = min_pos_iou\n",
    "        self.gt_max_assign_all = gt_max_assign_all\n",
    "        self.ignore_iof_thr = ignore_iof_thr\n",
    "        self.ignore_wrt_candidates = ignore_wrt_candidates\n",
    "        self.gpu_assign_thr = gpu_assign_thr\n",
    "\n",
    "    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n",
    "        \"\"\"Assign gt to bboxes.\n",
    "\n",
    "        This method assign a gt bbox to every bbox (proposal/anchor), each bbox\n",
    "        will be assigned with -1, 0, or a positive number. -1 means don't care,\n",
    "        0 means negative sample, positive number is the index (1-based) of\n",
    "        assigned gt.\n",
    "        The assignment is done in following steps, the order matters.\n",
    "\n",
    "        1. assign every bbox to -1\n",
    "        2. assign proposals whose iou with all gts < neg_iou_thr to 0\n",
    "        3. for each bbox, if the iou with its nearest gt >= pos_iou_thr,\n",
    "           assign it to that bbox\n",
    "        4. for each gt bbox, assign its nearest proposals (may be more than\n",
    "           one) to itself\n",
    "\n",
    "        Args:\n",
    "            bboxes (Tensor): Bounding boxes to be assigned, shape(n, 4).\n",
    "            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n",
    "            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n",
    "                labelled as `ignored`, e.g., crowd boxes in COCO.\n",
    "            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n",
    "\n",
    "        Returns:\n",
    "            :obj:`AssignResult`: The assign result.\n",
    "        \"\"\"\n",
    "\n",
    "        bboxes = bboxes[:, :4].type(torch.IntTensor) # :4 is just in case we've got more than the coordinates. \n",
    "        overlaps = bbox_overlaps(gt_bboxes, bboxes)\n",
    "\n",
    "        # Some bounding boxes are ignored (GT bboxes that contain labels we aren't interested in. )\n",
    "        if (self.ignore_iof_thr > 0) and (gt_bboxes_ignore is not None) and (\n",
    "                gt_bboxes_ignore.numel() > 0):\n",
    "            if self.ignore_wrt_candidates:\n",
    "                ignore_overlaps = bbox_overlaps(\n",
    "                    bboxes, gt_bboxes_ignore, mode='iof')\n",
    "                ignore_max_overlaps, _ = ignore_overlaps.max(dim=1)\n",
    "            else:\n",
    "                ignore_overlaps = bbox_overlaps(\n",
    "                    gt_bboxes_ignore, bboxes, mode='iof')\n",
    "                ignore_max_overlaps, _ = ignore_overlaps.max(dim=0)\n",
    "            overlaps[:, ignore_max_overlaps > self.ignore_iof_thr] = -1\n",
    "\n",
    "        assign_result = self.assign_wrt_overlaps(overlaps, gt_labels)\n",
    "        return assign_result\n",
    "\n",
    "    def assign_wrt_overlaps(self, overlaps, gt_labels=None):\n",
    "        \"\"\"Assign w.r.t. the overlaps of bboxes with gts.\n",
    "\n",
    "        Args:\n",
    "            overlaps (Tensor): Overlaps between k gt_bboxes and n bboxes,\n",
    "                shape(k, n).\n",
    "            gt_labels (Tensor, optional): Labels of k gt_bboxes, shape (k, ).\n",
    "\n",
    "        Returns:\n",
    "            :obj:`AssignResult`: The assign result.\n",
    "        \"\"\"\n",
    "        num_gts, num_bboxes = overlaps.size(0), overlaps.size(1)\n",
    "\n",
    "        # 1. assign -1 by default\n",
    "        # Function new_full: https://www.kite.com/python/docs/torch.Tensor.new_full esentially returns a tensor of size (arg1) filled with value (arg2)\n",
    "        assigned_gt_inds = overlaps.new_full((num_bboxes,),\n",
    "                                             -1,\n",
    "                                             dtype=torch.long)\n",
    "\n",
    "        if num_gts == 0 or num_bboxes == 0:\n",
    "            # No ground truth or boxes, return empty assignment\n",
    "            max_overlaps = overlaps.new_zeros((num_bboxes,))\n",
    "            if num_gts == 0:\n",
    "                # No truth, assign everything to background\n",
    "                assigned_gt_inds[:] = 0\n",
    "            if gt_labels is None:\n",
    "                assigned_labels = None\n",
    "            else:\n",
    "                assigned_labels = overlaps.new_zeros((num_bboxes,),\n",
    "                                                     dtype=torch.long)\n",
    "            return AssignResult(\n",
    "                num_gts,\n",
    "                assigned_gt_inds,\n",
    "                max_overlaps,\n",
    "                labels=assigned_labels)\n",
    "\n",
    "        # for each anchor, which gt best overlaps with it\n",
    "        # for each anchor, the max iou of all gts\n",
    "        max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n",
    "        # for each gt, which anchor best overlaps with it\n",
    "        # for each gt, the max iou of all proposals\n",
    "        gt_max_overlaps, gt_argmax_overlaps = overlaps.max(dim=1)\n",
    "\n",
    "        # 2. assign negative: below\n",
    "        if isinstance(self.neg_iou_thr, float):\n",
    "            # Overlaps that do not match neg_iou_thr (meaning that its overlap with any ground truth is insufficient to match)\n",
    "            assigned_gt_inds[(max_overlaps >= 0)\n",
    "                             & (max_overlaps < self.neg_iou_thr)] = 0\n",
    "        elif isinstance(self.neg_iou_thr, tuple):\n",
    "          # This is in case you consider your background to be corresponding to two overlaps thresholds (i.e. 0.1 - 0.4 overlap: = background)\n",
    "            assert len(self.neg_iou_thr) == 2\n",
    "            assigned_gt_inds[(max_overlaps >= self.neg_iou_thr[0])\n",
    "                             & (max_overlaps < self.neg_iou_thr[1])] = 0\n",
    "\n",
    "        # 3. assign positive: above positive IoU threshold\n",
    "        pos_inds = max_overlaps >= self.pos_iou_thr\n",
    "        assigned_gt_inds[pos_inds] = argmax_overlaps[pos_inds] + 1\n",
    "\n",
    "        # 4. assign fg: for each gt, proposals with highest IoU\n",
    "        for i in range(num_gts):\n",
    "            if gt_max_overlaps[i] >= self.min_pos_iou:\n",
    "                if self.gt_max_assign_all:\n",
    "                    max_iou_inds = overlaps[i, :] == gt_max_overlaps[i]\n",
    "                    assigned_gt_inds[max_iou_inds] = i + 1\n",
    "                else:\n",
    "                    assigned_gt_inds[gt_argmax_overlaps[i]] = i + 1\n",
    "\n",
    "        if gt_labels is not None:\n",
    "            assigned_labels = assigned_gt_inds.new_zeros((num_bboxes,))\n",
    "            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n",
    "            if pos_inds.numel() > 0:\n",
    "                assigned_labels[pos_inds] = gt_labels[\n",
    "                    assigned_gt_inds[pos_inds] - 1]\n",
    "        else:\n",
    "            assigned_labels = None\n",
    "        # return num_gts, assigned_gt_inds, max_overlaps, assigned_labels\n",
    "        return AssignResult(\n",
    "            num_gts, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxB8sVr4P-n7"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1Q-yTmsPe9ZemDYbkN32MvKibIdtUs28K\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPksIK3EwT7b"
   },
   "source": [
    "The following functions and classes are useful for you to sample randomly among your positive and negative samples. Indicate as a comment at which stage this may become useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEvs4EbXqgnH"
   },
   "outputs": [],
   "source": [
    "def ensure_rng(rng=None):\n",
    "    \"\"\"\n",
    "    Simple version of the ``kwarray.ensure_rng``\n",
    "\n",
    "    Args:\n",
    "        rng (int | numpy.random.RandomState | None):\n",
    "            if None, then defaults to the global rng. Otherwise this can be an\n",
    "            integer or a RandomState class\n",
    "    Returns:\n",
    "        (numpy.random.RandomState) : rng -\n",
    "            a numpy random number generator\n",
    "\n",
    "    References:\n",
    "        https://gitlab.kitware.com/computer-vision/kwarray/blob/master/kwarray/util_random.py#L270\n",
    "    \"\"\"\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.mtrand._rand\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.RandomState(rng)\n",
    "    else:\n",
    "        rng = rng\n",
    "    return rng\n",
    "\n",
    "\n",
    "def random_boxes(num=1, scale=1, rng=None):\n",
    "    \"\"\"\n",
    "    Simple version of ``kwimage.Boxes.random``\n",
    "\n",
    "    Returns:\n",
    "        Tensor: shape (n, 4) in x1, y1, x2, y2 format.\n",
    "\n",
    "    References:\n",
    "        https://gitlab.kitware.com/computer-vision/kwimage/blob/master/kwimage/structs/boxes.py#L1390\n",
    "\n",
    "    Example:\n",
    "        >>> num = 3\n",
    "        >>> scale = 512\n",
    "        >>> rng = 0\n",
    "        >>> boxes = random_boxes(num, scale, rng)\n",
    "        >>> print(boxes)\n",
    "        tensor([[280.9925, 278.9802, 308.6148, 366.1769],\n",
    "                [216.9113, 330.6978, 224.0446, 456.5878],\n",
    "                [405.3632, 196.3221, 493.3953, 270.7942]])\n",
    "    \"\"\"\n",
    "    rng = ensure_rng(rng)\n",
    "\n",
    "    tlbr = rng.rand(num, 4).astype(np.float32)\n",
    "\n",
    "    tl_x = np.minimum(tlbr[:, 0], tlbr[:, 2])\n",
    "    tl_y = np.minimum(tlbr[:, 1], tlbr[:, 3])\n",
    "    br_x = np.maximum(tlbr[:, 0], tlbr[:, 2])\n",
    "    br_y = np.maximum(tlbr[:, 1], tlbr[:, 3])\n",
    "\n",
    "    tlbr[:, 0] = tl_x * scale\n",
    "    tlbr[:, 1] = tl_y * scale\n",
    "    tlbr[:, 2] = br_x * scale\n",
    "    tlbr[:, 3] = br_y * scale\n",
    "\n",
    "    boxes = torch.from_numpy(tlbr)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "class SamplingResult():\n",
    "    \"\"\"\n",
    "    Example:\n",
    "        # >>> # xdoctest: +IGNORE_WANT\n",
    "        # >>> from mmdet.core.bbox.samplers.sampling_result import *  # NOQA\n",
    "        # >>> self = SamplingResult.random(rng=10)\n",
    "        # >>> print('self = {}'.format(self))\n",
    "        self = <SamplingResult({\n",
    "            'neg_bboxes': torch.Size([12, 4]),\n",
    "            'neg_inds': tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12]),\n",
    "            'num_gts': 4,\n",
    "            'pos_assigned_gt_inds': tensor([], dtype=torch.int64),\n",
    "            'pos_bboxes': torch.Size([0, 4]),\n",
    "            'pos_inds': tensor([], dtype=torch.int64),\n",
    "            'pos_is_gt': tensor([], dtype=torch.uint8)\n",
    "        })>\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pos_inds, neg_inds, bboxes, gt_bboxes, assign_result,\n",
    "                 gt_flags):\n",
    "        self.pos_inds = pos_inds\n",
    "        self.neg_inds = neg_inds\n",
    "        self.pos_bboxes = bboxes[pos_inds]\n",
    "        self.neg_bboxes = bboxes[neg_inds]\n",
    "        self.pos_is_gt = gt_flags[pos_inds]\n",
    "\n",
    "        self.num_gts = gt_bboxes.shape[0]\n",
    "        self.pos_assigned_gt_inds = assign_result.gt_inds[pos_inds] - 1\n",
    "\n",
    "        if gt_bboxes.numel() == 0:\n",
    "            # hack for index error case\n",
    "            assert self.pos_assigned_gt_inds.numel() == 0\n",
    "            self.pos_gt_bboxes = torch.empty_like(gt_bboxes).view(-1, 4)\n",
    "        else:\n",
    "            if len(gt_bboxes.shape) < 2:\n",
    "                gt_bboxes = gt_bboxes.view(-1, 4)\n",
    "\n",
    "            self.pos_gt_bboxes = gt_bboxes[self.pos_assigned_gt_inds, :]\n",
    "\n",
    "        if assign_result.labels is not None:\n",
    "            self.pos_gt_labels = assign_result.labels[pos_inds]\n",
    "        else:\n",
    "            self.pos_gt_labels = None\n",
    "\n",
    "    @property\n",
    "    def bboxes(self):\n",
    "        return torch.cat([self.pos_bboxes, self.neg_bboxes])\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\"\n",
    "        Change the device of the data inplace.\n",
    "\n",
    "        Example:\n",
    "            >>> self = SamplingResult.random()\n",
    "            >>> print('self = {}'.format(self.to(None)))\n",
    "            >>> # xdoctest: +REQUIRES(--gpu)\n",
    "            >>> print('self = {}'.format(self.to(0)))\n",
    "        \"\"\"\n",
    "        _dict = self.__dict__\n",
    "        for key, value in _dict.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                _dict[key] = value.to(device)\n",
    "        return self\n",
    "\n",
    "    def __nice__(self):\n",
    "        data = self.info.copy()\n",
    "        data['pos_bboxes'] = data.pop('pos_bboxes').shape\n",
    "        data['neg_bboxes'] = data.pop('neg_bboxes').shape\n",
    "        parts = ['\\'{}\\': {!r}'.format(k, v) for k, v in sorted(data.items())]\n",
    "        body = '    ' + ',\\n    '.join(parts)\n",
    "        return '{\\n' + body + '\\n}'\n",
    "\n",
    "    @property\n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of info about the object\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'pos_inds': self.pos_inds,\n",
    "            'neg_inds': self.neg_inds,\n",
    "            'pos_bboxes': self.pos_bboxes,\n",
    "            'neg_bboxes': self.neg_bboxes,\n",
    "            'pos_is_gt': self.pos_is_gt,\n",
    "            'num_gts': self.num_gts,\n",
    "            'pos_assigned_gt_inds': self.pos_assigned_gt_inds,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def random(cls, rng=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rng (None | int | numpy.random.RandomState): seed or state\n",
    "\n",
    "        Kwargs:\n",
    "            num_preds: number of predicted boxes\n",
    "            num_gts: number of true boxes\n",
    "            p_ignore (float): probability of a predicted box assinged to an\n",
    "                ignored truth\n",
    "            p_assigned (float): probability of a predicted box not being\n",
    "                assigned\n",
    "            p_use_label (float | bool): with labels or not\n",
    "\n",
    "        Returns:\n",
    "            AssignResult :\n",
    "\n",
    "        Example:\n",
    "            # >>> from mmdet.core.bbox.samplers.sampling_result import *  # NOQA\n",
    "            # >>> self = SamplingResult.random()\n",
    "            # >>> print(self.__dict__)\n",
    "        \"\"\"\n",
    "        # from mmdet.core.bbox.samplers.random_sampler import RandomSampler\n",
    "        # from mmdet.core.bbox.assigners.assign_result import AssignResult\n",
    "        # from mmdet.core.bbox import demodata\n",
    "        rng = ensure_rng(rng)\n",
    "\n",
    "        # make probabalistic?\n",
    "        num = 32\n",
    "        pos_fraction = 0.5\n",
    "        neg_pos_ub = -1\n",
    "\n",
    "        assign_result = AssignResult.random(rng=rng, **kwargs)\n",
    "\n",
    "        # Note we could just compute an assignment\n",
    "        bboxes = random_boxes(assign_result.num_preds, rng=rng)\n",
    "        gt_bboxes = random_boxes(assign_result.num_gts, rng=rng)\n",
    "\n",
    "        if rng.rand() > 0.2:\n",
    "            # sometimes algorithms squeeze their data, be robust to that\n",
    "            gt_bboxes = gt_bboxes.squeeze()\n",
    "            bboxes = bboxes.squeeze()\n",
    "\n",
    "        if assign_result.labels is None:\n",
    "            gt_labels = None\n",
    "        else:\n",
    "            gt_labels = None  # todo\n",
    "\n",
    "        if gt_labels is None:\n",
    "            add_gt_as_proposals = False\n",
    "        else:\n",
    "            add_gt_as_proposals = True  # make probabalistic?\n",
    "\n",
    "        sampler = RandomSampler(\n",
    "            num,\n",
    "            pos_fraction,\n",
    "            neg_pos_ubo=neg_pos_ub,\n",
    "            add_gt_as_proposals=add_gt_as_proposals,\n",
    "            rng=rng)\n",
    "        self = sampler.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n",
    "        return self\n",
    "\n",
    "\n",
    "#  Assigner and sampler\n",
    "class BaseSampler():\n",
    "\n",
    "    def __init__(self,\n",
    "                 num,\n",
    "                 pos_fraction,\n",
    "                 neg_pos_ub=-1,\n",
    "                 add_gt_as_proposals=False,\n",
    "                 **kwargs):\n",
    "        self.num = num\n",
    "        self.pos_fraction = pos_fraction\n",
    "        self.neg_pos_ub = neg_pos_ub\n",
    "        self.add_gt_as_proposals = add_gt_as_proposals\n",
    "        self.pos_sampler = self\n",
    "        self.neg_sampler = self\n",
    "\n",
    "    def _sample_pos(self, assign_result, num_expected, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def _sample_neg(self, assign_result, num_expected, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def sample(self,\n",
    "               assign_result,\n",
    "               bboxes,\n",
    "               gt_bboxes,\n",
    "               gt_labels=None,\n",
    "               **kwargs):\n",
    "        \"\"\"Sample positive and negative bboxes.\n",
    "\n",
    "        This is a simple implementation of bbox sampling given candidates,\n",
    "        assigning results and ground truth bboxes.\n",
    "\n",
    "        Args:\n",
    "            assign_result (:obj:`AssignResult`): Bbox assigning results.\n",
    "            bboxes (Tensor): Boxes to be sampled from.\n",
    "            gt_bboxes (Tensor): Ground truth bboxes.\n",
    "            gt_labels (Tensor, optional): Class labels of ground truth bboxes.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`SamplingResult`: Sampling result.\n",
    "\n",
    "        Example:\n",
    "            # >>> from mmdet.core.bbox import RandomSampler\n",
    "            # >>> from mmdet.core.bbox import AssignResult\n",
    "            # >>> from mmdet.core.bbox.demodata import ensure_rng, random_boxes\n",
    "            # >>> rng = ensure_rng(None)\n",
    "            # >>> assign_result = AssignResult.random(rng=rng)\n",
    "            # >>> bboxes = random_boxes(assign_result.num_preds, rng=rng)\n",
    "            # >>> gt_bboxes = random_boxes(assign_result.num_gts, rng=rng)\n",
    "            # >>> gt_labels = None\n",
    "            # >>> self = RandomSampler(num=32, pos_fraction=0.5, neg_pos_ub=-1,\n",
    "            # >>>                      add_gt_as_proposals=False)\n",
    "            # >>> self = self.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n",
    "        \"\"\"\n",
    "        if len(bboxes.shape) < 2:\n",
    "            bboxes = bboxes[None, :]\n",
    "\n",
    "        bboxes = bboxes[:, :4]\n",
    "\n",
    "        gt_flags = bboxes.new_zeros((bboxes.shape[0],), dtype=torch.uint8)\n",
    "        if self.add_gt_as_proposals and len(gt_bboxes) > 0:\n",
    "            if gt_labels is None:\n",
    "                raise ValueError(\n",
    "                    'gt_labels must be given when add_gt_as_proposals is True')\n",
    "            bboxes = torch.cat([gt_bboxes, bboxes], dim=0)\n",
    "            assign_result.add_gt_(gt_labels)\n",
    "            gt_ones = bboxes.new_ones(gt_bboxes.shape[0], dtype=torch.uint8)\n",
    "            gt_flags = torch.cat([gt_ones, gt_flags])\n",
    "\n",
    "        num_expected_pos = int(self.num * self.pos_fraction)\n",
    "        pos_inds = self.pos_sampler._sample_pos(\n",
    "            assign_result, num_expected_pos, bboxes=bboxes, **kwargs)\n",
    "        # We found that sampled indices have duplicated items occasionally.\n",
    "        # (may be a bug of PyTorch)\n",
    "        pos_inds = pos_inds.unique()\n",
    "        num_sampled_pos = pos_inds.numel()\n",
    "        num_expected_neg = self.num - num_sampled_pos\n",
    "        if self.neg_pos_ub >= 0:\n",
    "            _pos = max(1, num_sampled_pos)\n",
    "            neg_upper_bound = int(self.neg_pos_ub * _pos)\n",
    "            if num_expected_neg > neg_upper_bound:\n",
    "                num_expected_neg = neg_upper_bound\n",
    "        neg_inds = self.neg_sampler._sample_neg(\n",
    "            assign_result, num_expected_neg, bboxes=bboxes, **kwargs)\n",
    "        neg_inds = neg_inds.unique()\n",
    "\n",
    "        sampling_result = SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n",
    "                                         assign_result, gt_flags)\n",
    "        return sampling_result\n",
    "\n",
    "\n",
    "class RandomSampler(BaseSampler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num,\n",
    "                 pos_fraction,\n",
    "                 neg_pos_ub=-1,\n",
    "                 add_gt_as_proposals=False,\n",
    "                 **kwargs):\n",
    "        super(RandomSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n",
    "                                            add_gt_as_proposals)\n",
    "\n",
    "        self.rng = ensure_rng(kwargs.get('rng', None))\n",
    "\n",
    "    def random_choice(self, gallery, num):\n",
    "        \"\"\"Random select some elements from the gallery.\n",
    "\n",
    "        It seems that Pytorch's implementation is slower than numpy so we use\n",
    "        numpy to randperm the indices.\n",
    "        \"\"\"\n",
    "        assert len(gallery) >= num\n",
    "        if isinstance(gallery, list):\n",
    "            gallery = np.array(gallery)\n",
    "        cands = np.arange(len(gallery))\n",
    "        self.rng.shuffle(cands)\n",
    "        rand_inds = cands[:num]\n",
    "        if not isinstance(gallery, np.ndarray):\n",
    "            rand_inds = torch.from_numpy(rand_inds).long().to(gallery.device)\n",
    "        return gallery[rand_inds]\n",
    "\n",
    "    def _sample_pos(self, assign_result, num_expected, **kwargs):\n",
    "        \"\"\"Randomly sample some positive samples.\"\"\"\n",
    "        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n",
    "        if pos_inds.numel() != 0:\n",
    "            pos_inds = pos_inds.squeeze(1)\n",
    "        if pos_inds.numel() <= num_expected:\n",
    "            return pos_inds\n",
    "        else:\n",
    "            return self.random_choice(pos_inds, num_expected)\n",
    "\n",
    "    def _sample_neg(self, assign_result, num_expected, **kwargs):\n",
    "        \"\"\"Randomly sample some negative samples.\"\"\"\n",
    "        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n",
    "        if neg_inds.numel() != 0:\n",
    "            neg_inds = neg_inds.squeeze(1)\n",
    "        if len(neg_inds) <= num_expected:\n",
    "            return neg_inds\n",
    "        else:\n",
    "            return self.random_choice(neg_inds, num_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rrppnJGeaLz"
   },
   "source": [
    "This can come handy to ensure that you've got a **balance** in the data you feed to your network. If you have got 100000 negative samples, and 100 positive samples, you will suffer from a severe class imbalance. Being able to draw positive and negative samples allows you to create balanced batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKhmuLMitnIq"
   },
   "source": [
    "# **Part 3 - From boxes to learnable delta and back**\n",
    "\n",
    "\n",
    "For the regression, or bounding box adjustment layer, we output 4 predictions: the deltas &delta;_x, &delta;_y, &delta;_w, &delta;_h which we will apply to the anchors to get the final proposals. \n",
    "\n",
    "Source: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/\n",
    "\n",
    "In other words:\n",
    "\n",
    "In order to appropriately learn the shifts for the different boxes, based on the assignment we need to transform the proposed boxes into the delta values to learn. Conversely, from a set of delta values and boxes we need to be able to amend these boxes. The two following functions that you have to complete perform these functionalities. \n",
    "\n",
    "Remember that delta are defined as follow \n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "You can test out that things work appropriately on a few examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZmoEnRP3w6O"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1kCYtYLqADe3uiKYQsGwbGAgN-JwutzGv\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Transforming the bounding boxes to deltas and vice versa</b><br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-iGpKCbtU7U"
   },
   "outputs": [],
   "source": [
    "def bbox2delta(proposals, gt, means=[0, 0, 0, 0], stds=[1, 1, 1, 1]):\n",
    "    assert proposals.size() == gt.size()\n",
    "\n",
    "    proposals = proposals.float()\n",
    "\n",
    "    gt = gt.float()\n",
    "    # Defining the centre of the proposed boxes\n",
    "    px = (proposals[..., 0] + proposals[..., 2]) * 0.5\n",
    "    py = (proposals[..., 1] + proposals[..., 3]) * 0.5\n",
    "    # Defining height and width of the proposed boxes\n",
    "    pw = proposals[..., 2] - proposals[..., 0] + 1.0\n",
    "    ph = proposals[..., 3] - proposals[..., 1] + 1.0\n",
    "\n",
    "    # Defining the centre of the ground_truth boxes\n",
    "    gx = (gt[..., 0] + gt[..., 2]) * 0.5\n",
    "    gy = (gt[..., 1] + gt[..., 3]) * 0.5\n",
    "    # Defining height and width of the ground truth boxes\n",
    "    gw = gt[..., 2] - gt[..., 0] + 1.0\n",
    "    gh = gt[..., 3] - gt[..., 1] + 1.0\n",
    "\n",
    "    # Defining delta \n",
    "    dx = (gx - px) / pw\n",
    "    dy = (gy - py) / ph\n",
    "    dw = torch.log(gw / pw)\n",
    "    dh = torch.log(gh / ph)\n",
    "    deltas = torch.stack([dx, dy, dw, dh], dim=-1)\n",
    "\n",
    "    \n",
    "    means = deltas.new_tensor(means).unsqueeze(0)\n",
    "    stds = deltas.new_tensor(stds).unsqueeze(0)\n",
    "    deltas = deltas.sub_(means).div_(stds)\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def delta2bbox(rois,\n",
    "               deltas,\n",
    "               means=[0, 0, 0, 0],\n",
    "               stds=[1, 1, 1, 1],\n",
    "               max_shape=None,\n",
    "               wh_ratio_clip=1 / 1000):\n",
    "    \"\"\"\n",
    "    Apply deltas to shift/scale base boxes.\n",
    "\n",
    "    Typically the rois are anchor or proposed bounding boxes and the deltas are\n",
    "    network outputs used to shift/scale those boxes.\n",
    "\n",
    "    Args:\n",
    "        rois (Tensor): boxes to be transformed. Has shape (N, 4)\n",
    "        deltas (Tensor): encoded offsets with respect to each roi.\n",
    "            Has shape (N, 4). Note N = num_anchors * W * H when rois is a grid\n",
    "            of anchors. Offset encoding follows [1]_.\n",
    "        means (list): denormalizing means for delta coordinates\n",
    "        stds (list): denormalizing standard deviation for delta coordinates\n",
    "        max_shape (tuple[int, int]): maximum bounds for boxes. specifies (H, W)\n",
    "        wh_ratio_clip (float): maximum aspect ratio for boxes.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: boxes with shape (N, 4), where columns represent\n",
    "            tl_x, tl_y, br_x, br_y.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 4)\n",
    "    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 4)\n",
    "    denorm_deltas = deltas * stds + means\n",
    "    dx = denorm_deltas[:, 0::4] #\n",
    "    dy = denorm_deltas[:, 1::4]\n",
    "    dw = denorm_deltas[:, 2::4]\n",
    "    dh = denorm_deltas[:, 3::4]\n",
    "    max_ratio = np.abs(np.log(wh_ratio_clip)) # We set a maximum expansion ratio for the deltas. \n",
    "    dw = dw.clamp(min=-max_ratio, max=max_ratio)\n",
    "    dh = dh.clamp(min=-max_ratio, max=max_ratio)\n",
    "    # Compute center of each roi\n",
    "    px = ((rois[:, 0] + rois[:, 2]) * 0.5).unsqueeze(1).expand_as(dx)\n",
    "    py = ((rois[:, 1] + rois[:, 3]) * 0.5).unsqueeze(1).expand_as(dy)\n",
    "    # Compute width/height of each roi\n",
    "    pw = (rois[:, 2] - rois[:, 0] + 1.0).unsqueeze(1).expand_as(dw)\n",
    "    ph = (rois[:, 3] - rois[:, 1] + 1.0).unsqueeze(1).expand_as(dh)\n",
    "    # Use exp(network energy) to enlarge/shrink each roi\n",
    "    gw = pw * dw.exp()\n",
    "    gh = ph * dh.exp()\n",
    "   \n",
    "    # Use network energy to shift the center of each roi\n",
    "    gx = torch.addcmul(px, 1, pw, dx)  # gx = px + pw * dx\n",
    "    gy = torch.addcmul(py, 1, ph, dy)  # gy = py + ph * dy\n",
    "    # Convert center-xy/width/height to top-left, bottom-right\n",
    "    x1 = gx - gw * 0.5 + 0.5\n",
    "    y1 = gy - gh * 0.5 + 0.5\n",
    "    x2 = gx + gw * 0.5 - 0.5\n",
    "    y2 = gy + gh * 0.5 - 0.5\n",
    "\n",
    "    # making sure that the indices are well ordered\n",
    "    x1_fin = torch.min(x1,x2)\n",
    "    x2_fin = torch.max(x1,x2)\n",
    "    y1_fin = torch.min(y1,y2)\n",
    "    y2_fin = torch.max(y1,y2)\n",
    "    if max_shape is not None:  # Just in case we want to limit the shape. \n",
    "        x1_fin = x1_fin.clamp(min=0, max=max_shape[1] - 1)\n",
    "        y1_fin = y1_fin.clamp(min=0, max=max_shape[0] - 1)\n",
    "        x2_fin = x2_fin.clamp(min=0, max=max_shape[1] - 1)\n",
    "        y2_fin = y2_fin.clamp(min=0, max=max_shape[0] - 1)\n",
    "    bboxes = torch.stack([x1_fin, y1_fin, x2_fin, y2_fin], dim=-1).view_as(deltas)\n",
    "    return bboxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZd_lRLez1Mq"
   },
   "source": [
    "# **Part 4 - Implementing our networks**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnBclsOf3CTv"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1KaSU2OUsCMKjk8fYd49_R35ouomPmhCo\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Implementing Networks and Loss functions</b><br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb0QHRZmuz7m"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1027vJCH49nMyB_hyigUIUvXqWZgE0g3-\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Schematic of a general Faster RCNN architecture.</b> Source: https://towardsdatascience.com/faster-rcnn-object-detection-f865e5ed7fc4<br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgidjdKH10xg"
   },
   "source": [
    "## **Backbone**\n",
    "A UNet backbone network similar to the one used for lecture 5 is used for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUoTkyzL1DnP"
   },
   "outputs": [],
   "source": [
    "class _EncoderBN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k, padd, dropout=False):\n",
    "        super(_EncoderBN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=k,\n",
    "                               padding=padd)\n",
    "        self.BN1a = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=k,\n",
    "                               padding=padd)\n",
    "        self.BN2a = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.type(torch.float32)\n",
    "        x = F.leaky_relu(self.BN1a(self.conv1(x)), inplace=True)\n",
    "        x = F.leaky_relu(self.BN2a(self.conv2(x)), inplace=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _DecoderBN(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(_DecoderBN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.BN1a = nn.BatchNorm2d(middle_channels)\n",
    "        self.conv2 = nn.Conv2d(middle_channels, middle_channels, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.BN2a = nn.BatchNorm2d(middle_channels)\n",
    "        self.convT = nn.ConvTranspose2d(middle_channels, out_channels,\n",
    "                                        kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.BN1a(self.conv1(x)), inplace=True)\n",
    "        x = F.leaky_relu(self.BN2a(self.conv2(x)), inplace=True)\n",
    "        return self.convT(x)\n",
    "\n",
    "\n",
    "class prefinalBN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(prefinalBN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.BN1a = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.BN2a = nn.BatchNorm2d(out_channels)\n",
    "        nn.InstanceNorm1d\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.BN1a(self.conv1(x)), inplace=True)\n",
    "        x = F.leaky_relu(self.BN2a(self.conv2(x)), inplace=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ADABN(nn.Module):\n",
    "    def __init__(self, num_classes, num_channels):\n",
    "        super(ADABN, self).__init__()\n",
    "\n",
    "        self.enc1 = _EncoderBN(num_channels, 64, 5, 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.enc2 = _EncoderBN(64, 96, 3, 1)\n",
    "        self.enc3 = _EncoderBN(96, 128, 3, 1)\n",
    "        self.enc4 = _EncoderBN(128, 256, 3, 1)\n",
    "        self.center = _DecoderBN(256, 512, 256)\n",
    "        self.dec4 = _DecoderBN(512, 256, 128)\n",
    "        self.dec3 = _DecoderBN(256, 128, 96)\n",
    "        self.dec2 = _DecoderBN(96 * 2, 96, 64)\n",
    "        self.dec1 = prefinalBN(128, 64)\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"What gets in backbone\")\n",
    "        print(x.shape)\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = F.dropout(self.enc4(self.pool(enc3)))\n",
    "\n",
    "        center = self.center(self.pool(enc4))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([center, enc4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, enc3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, enc2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, enc1], 1))\n",
    "\n",
    "        final = self.final(dec1)\n",
    "\n",
    "        print(\"What gets out\")\n",
    "        print(final.shape)\n",
    "        return (final, enc1, enc2, enc3, enc4, center, dec4, dec3, dec2, dec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu6jO1uH0E6d"
   },
   "source": [
    "## **RPN Head**\n",
    "The RPN Head is defined as convolutional layer whose output going through a ReLU activation function is shared by a regression branch and a classification branch each of which is expressed as a convolutional layer of kernel 1 with respectively 4 * num_anchors and 2 * num_anchors channels as output. Following these guidelines, complete the network below.\n",
    "\n",
    "Why 2xnum_anchors for the classification? The RPN is only there to suggest **positive** boxes. Thus, you only one the classes to be 0 and 1 for this task, independently of how many classes you've got on the RCNN side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XeIxT260o2e"
   },
   "outputs": [],
   "source": [
    "class RPNHead(nn.Module):\n",
    "    def __init__(self, num_classes, num_channels, num_feat, num_anchors):\n",
    "        super(RPNHead, self).__init__()\n",
    "        self.num_anchors = num_anchors \n",
    "        self.in_channels = num_channels\n",
    "        self.feat_channels = num_feat # The feature map from where we infer the classification and the regions have self.feat_channels (Dim: W_X x W_Y x C)\n",
    "        self.cls_out_channels = num_classes\n",
    "\n",
    "        self.rpn_conv = nn.Conv2d(\n",
    "            self.in_channels, self.feat_channels, 3, padding=1) # Last branch before branching classificatio + regression\n",
    "        self.rpn_cls = nn.Conv2d(self.feat_channels,\n",
    "                                 self.num_anchors * self.cls_out_channels, 1)\n",
    "\n",
    "        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1) \n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        first_conv = F.relu(self.rpn_conv(x))\n",
    "        cls_pred = self.rpn_cls(first_conv)\n",
    "        bbox_pred = self.rpn_reg(first_conv)\n",
    "        return cls_pred, bbox_pred, first_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K3d6_mL0pks"
   },
   "source": [
    "## **RCNN Head**\n",
    "Classically, the RCNN Head is defined as 2 fully connected layers interleaved with ReLU whose output is shared by a regression branch and a classification branch each of which is expressed as a fully connected layer with respectively 4 and #number of classes channels as output. Following these guidelines, complete the network below. As the input of the RCNN Head is a 7x7 x number of feature maps, it needs to be flattened (or the input needs to be averaged) before being fed to the network. In our case we decided to use the flattened version of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaYziMHoZoJY"
   },
   "outputs": [],
   "source": [
    "class BBoxHead(nn.Module):\n",
    "    def __init__(self, num_classes, num_channels, num_feat=88):\n",
    "        super(BBoxHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_channels = num_channels\n",
    "        self.num_feat = num_feat\n",
    "        self.fully_conn_1 = nn.Linear(num_channels, num_feat)\n",
    "        self.fully_conn_2 = nn.Linear(num_feat, num_feat)\n",
    "        self.fc_reg = nn.Linear(num_feat, 4)\n",
    "        self.fc_cls = nn.Linear(num_feat, num_classes)\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        print(x.shape)\n",
    "        fc1 = F.relu(self.fully_conn_1(x))\n",
    "        fc2 = F.relu(self.fully_conn_2(fc1))\n",
    "        cls_pred = self.fc_cls(fc2)\n",
    "        bbox_pred = self.fc_reg(fc2)\n",
    "        return cls_pred, bbox_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks0jqMUU2Llu"
   },
   "source": [
    "## **Definition of loss functions**\n",
    "We have seen multiple loss functions in the lecture. In addition to the weighted soft dice, complete the implementation of the smooth l1 loss and of the focal loss.\n",
    "As a reminder:\n",
    "\n",
    "Smooth L1 (&beta;=1):\n",
    "\n",
    "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUwAAAA0CAYAAADhaAFHAAAgAElEQVR4Ae1dB1gVR9e+xhKJMdZPYwNUjIWYGFM0KBrUzxpLiESjKIjGEiuiqBgNxogVFFEENFQrURQLCEER5ENRigSQDhoLhmtAESmhnP95J3f2XwjlXhRE3X0emN3ZM+3du+/OnDlzRiaTDgkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQmBqhEgoiZxcXFNqpZ6ue+ifZaWlk3Q1pe7JVLtJQQkBOocASJqgEIzMjKcTU1Nyd3dneq8EnVUoK+v7/6ff/6ZHB0daenSpbR161Y7Inq3joqvs2Ly8vI8iEhL1QKJaHdOTs4kVdNJ8hICrxUCycnJ+Q0aNCjt1q0bpaSk/P0qNj4/P/+Hxo0b08OHD43RvgULFvg0b96czMzMer1q7f3Pf/5Dp0+fHqxquzZv3pzbqlUrK1XTSfISAq8NApGRkTfffPNNMjExoeLiYv3abjgROY0ZM8arZcuWwd27d/fKyso6Wl2Zrq6uZ/T19U9GREScTEhIOHngwIGTDRs29CKiDdWl5fevX7/+Iwhz48aNSxGXm5vrJ5PJqHnz5q8cYbZr165GhPnzzz/ntmnTptYJk49q+LNZuHChv4uLizu/lkIJgXqLgL6+PjVp0oTy8vJW1nYl79y580XPnj1p/vz5bNjv5uZW0qBBAwoODl5XVdmbNm2CvPDXqFEjcnBwyKoqTXX3PDw8/tesWTNatmxZ7+pkX7b7NSXMTZs21QlhAs8HDx6Qvb09TZw4Ec+11MbG5uHLhrNU39cMgadPn7qAiLZt28YIrLabv2/fvjutWrUiImqNsoio+aRJk8jc3LzK8rds2ULr16+nXbt20fXr10lPT0+7fC9FlbqXlJTEY9h66NChTCJqrEral0H2RREmfyZE5GdlZRVaVFT0v8rwioqK+lRDQ+NzBweHz9966y3auXOnRJiVgSXF1w8EkpKSosFbly9frpKwnldtJ0+eTPPmzStT1q+//kpt27YFib5XWTlbt26lAQMGTKzsvjieiD7j10TUj09+8Jf50aNHF3V1dTFkfQA5R0fHt7j8qxJWRphE1FncRiIaKr5+lh4mrA7c3d3fDw8Pp65du5K6uvqfhYWFlRKmuNx33nlHIkwxINJ5/USAE2ZYWFgZEqut2jZt2pQUw2uhCLxgIO2UlJQ7QmS5ExDm5cuXHwQFBSW5uLgkEdERiHASVJyn6enpxeNltbOzS8zOzk58//33SwYPHvw0NTX1D4VM6tSpU+nJkyeZuA4NDS3Q0NB4pYfkwIiIhtnY2CR16NDhcb9+/RJLSkqSpk2blv7pp5/S8uXLU/Py8roAD1UJk+MPvfTChQv/gIpjxYoVZGRk9PHdu3fbIE9lDokwlUFJknnhCNQlYRJRPHRVO3fuLEPO8fHxjDAvXrx4qzJAYArUr18/GjJkiOecOXOOoQcVFBQk9EoLCws3fvbZZ3ThwoWY0tJSX7y4n3/+Oe7Tm2++WTpnzhycrtDW1qYWLVoQ0vfu3ZswLP/+++9facIEppcuXWIqDSI6NH36dFJTUyu1s7OjRYsWYdKLTE1Nu0FOVcJ0d3cfdeTIEYJ1xdixY/Nv3rzpX9kzrCpeIsyq0JHu1RsE6pIwCwsLr6BTCD2kGICEhARGmKdPn66UMGNjY+nIkSOmPF1SUhJLc/78eT7DvvzUqVMs35UrV77buXNnmjJlSiTknZ2dqaioiJeJXo/w17Fjx7aenp4Neb6vSlh+SD5z5ky6d+8e60UuX76cPvzwQ3xAGu3fvz/y2LFjVFBQwGw2lSXMrKyseZMmTXqICbuNGzdSRETEFCJqUVP8JMKsKXJSujpFoC4Jk4hgzkM7duzg5MXaCjJEfGBgYKWEKQYFw0BNTc1e6EV26NCBmaKEhIT4cpnIyMgfkF9YWNhNHoeQDx/Fca/quZgwicjezMzsWwUGvbt06UKbN28u8ww4DsoQZmpqKntegwYNopycnFCe9llCiTCfBT0pbZ0hUJeEiUZhNnTVqlVlXtbAwED2AqakpFRImGfPnl3brFmzwqVLl07jwCxevHgghtZGRkboKTXj8QhPnz7NhpniuNftXEyY4rbfuHFjPb4dXbt2nS2O5+fKECZkgfEbb7wBtUfR/fv3mX6Y51GTUCLMmqD2kqYhIpNly5Yd27hxY3dlmxAcHEw+Pj6/Q/5F9nzqmjBhHP/111+XIcx9+/ZRy5YtQXwLFHjA7mgbx3LUqFGw0aTdu3df5nipq6v3gbF9r1693BAnl8s/LioqYrac48ePJ2NjY6EMDPlXrVpV7XARz4GIMFYFqbzUR3nCvHv37i6YT02ePHkFiG7r1q1GaGBSUtLVlStXxvLGKkmYbCltSUnJhvXr18dh4ggfroULF66BypjnpUooEaYqaD1HWSLyXrt2LWEmFOui09PTa32d8JIlS/43bNgwys/PV1elKZ988gk9fvxYeLFVSfu8ZOuaMBXmSyBHD94GvHBmZmYCDpGRkTgnf39/NhPu5eW1HsNIIhJ6RefPn2cyUVFRLB8dHR3S1dX9OzU1dS/SHjhwgOVHROkjR46kkJCQ5ry8qsJffvmFMNQ/f/78oqrk6vs9MWEuX778LiZ2jI2Nm2KSB6udEhMTZxJRN3y8AgICgnl7lCRMLs5CImrj6+vLPnqYVAsICDjj6OjYmIiq1Q3DFMnT07MJ6rdjx46/kOGraBdbBrAXdREUFLRj+vTpruIeGhGNnDZt2o/t27cnbW3tRCJqWpv1S05OPvnxxx+TXC7/QtVyvL29L7Vt27YkOTm5j7gNqubzLPJ1SZho4927dwfOnj2bOnbsGO/k5OQwadKkGMx+X758+VuOQUREBGa2sZLnFNpGRL3OnDlD33zzDc2aNWvX4sWLH8M86ebNmwLJ9u/fH7O0TAaztu+//z6NHj36GD6c169fP60sRgcPHmREbGNjk6hsmvooJybMRYsWPW7YsGGJhYUFrVu3jgYOHEg9e/Y8h8kfhTpEaEJNCJMnJqKFJ0+ezMPKMeg37e3tuSkX65FyOR7+9ddfjrDJxaw9nidfAbZly5ZnWsHF85fCcgi89957DzH8srS0fEN8y9zcvGfr1q2x5A1rhWv1wI/Dy8tLeHFVLWzw4MEwoK5xelXLKy+fmJjIDNfryg6Tl5+QkFCyaNGih8eOHSvicQg5aVYUl5KSkm9hYfHQ2dn5YX5+vo9YHsP5K1euPMzLy5uM+IyMjOORkZGPa+KJ6O+//8bzmF1RXcT1qs/nYsJEPbOyskpv376dw+t8/fr1fCK6gWtxO5+FMHneCMPCwlI6d+78tKCg4Kk4/rU4FwPKG8zjyof8PsKq7nE5ZWTEefF0qampX0LvNWzYMH+eB78XHh7OzFfS09PDeFxthGlpaZ5Yg71///6Py9ehumten9zcXMKQFNfl03CZ2gwvXLgAwizFut7aLEec94top7j86s63b98Ok6Tvq5Orz/fFhCnGW3xeUf2fF2EibyJiOv3qyqyoHi9tXHh4ODy5tI+MjPx99erVWtBXAID58+e3g8GxnZ1dVyJCD699aWkpFL/909PTWxYUFPSQyWRtMWyaOHEiFFD/0u+dOXOGyaSnp6M3qFkepKtXr77z9ddf9wgNDQ2QyWSdwsPDG9++fVu7X79+PaZNmzYbDhhMTEwO4vry5cutePr9+/cTdF64jo+P/zssLOzU48ePUdZzO4CBsbHxaQ0NjRIi6l8+47lz53bIzs7GkOcHImLtjImJ+Wvz5s2Cj0L+Q4L+Jjs7u86/xHhuxsbGiRj+ymQy6JwqHDqVb9urfO3v728BI2wiYmY4L2tbxYSpShueJ2GqUu4rIXv//n1brJaAEn7Tpk1LBwwYgN4Um71csmQJ039Af7dmzZooGCNHRUVR9+7dYeN1b9y4cXT06FEmAyPiGzduxAAU/lJeunQpWkNDg+tQhqIcW1vb/Rw4IvLp3bv3bUtLS+iytsO7zaxZs+4lJiaOhSGygYEBXvLSvXv3MmewGRkZwsymnp4eQYc5d+5cAqnPmDGDzeLxvJ9HeOfOHTXUYerUqenl80tMTLwAfGQymc6XX36JjwH99NNPtHr1amrTBt8bEjzzQDk+dOhQsra2vl8+H/E1ES0holAl/q4SUbU6UTyHgwcPdsazgfOLBw8etBOX97qeBwUFbb548eKAl739MLny8vJS2R/mhg0bcps1a1br7t1ednz/VX+sAujbt+8NkA3s3Pz9/Q3AdyNHjlyDnqSZmVmGmZnZx4gzMjIqJaK3kcnw4cMfIe7EiRPMBEQmk70JBbyRkVEELyQlJQVkQp6eniGII6KWU6dOzevRowd6Wh/K5fIJeJHHjh0r6FyysrJAuHD+yhTJ8FyDWWaeJw+JCL3RUii2iYg/+KlIW1JSYsnlQHijRo3S09TUHNipU6eBffr0GS6TyZTuYXl6eqpB/2hjY1PGdrC4uNikY8eOmJDAyocGXbt2PfX222+jreobNmwgLN8TEyZ6zXBIoa+vXylhYiUKZjORb3V/HTp04B8h3tQKw2vXrml/9913pKmpSU+fPt1YoZAU+dIisHPnztisrKwPVG1ATEzMxR07dsxXNd1rL09EGiNGjGCmAmZmZulpaWkZd+7cYQSFYbaVldVkNzc36tSpEwiAmWxghrpNmzalCxcuZHKK3mRTrNddv349I8ybN2+OaNeuXSleVjHIY8aMOYfe15w5czpv2rQJw1P0Dh25TLdu3VrAVsvS0pLZjL333nvMlx6/z0NbW1svDNV3794tOML94YcfCv7hZfqRy2Ffl0GDBnWEOcqCBQtQl06qEKaNjY0aPgRHjhwpQ5hHjx6dBRtLlJOcnPwmJoXat2+fgevExMRe0dHRPXkdEMKEAraJn3zySaWECbmEhISeQUFBPZT5e/jwYZUOcRMSEu7jg4TlcjExMQJO4npJ5xICEgJKIsCHzfb29ucx1Y/3Gn82NjaMCHg26GGJ7ef8/PwmwzxAXV1dIKacnBykKf3zzz8Z0enp6Y2F4ezgwYOFXg1Io2vXrjcwRH/06FErDP3hfouXg/DIkSPYVoCOHTsWJZfLU3EeFxd3USQDo+NGw4cPv66uro5lcZ/we8OHD2fkwK95SEQfYy1sTEzMJR6nbAjCBDbHjx8vQ5g8vUKvK4M+zNzcvIwXHo4vZNF2fDx0dXWrJEyer7KhuIzyaTw9PS9gxY2NjU1aTk6OUi7Tyuchvsboori4eIo4TtnzJ0+e9Cai8crKS3ISAvUSgUePHhUWFxcz0srIyMj86quvcA4nBhaocF5e3k5cK4yMmW5SV1d3HOKcnJy28kaBDKBTJKK+kZGRWZ6entgvhry9va0ho3ix1UAsmC0mInbfycmpDGF+//33uMbQP2n16tXh0FHOnDmTuY+6ePGiHHkFBga2hAF5ly5dknj5crk8DsX8+eefdPbs2QUFBQWCztHT0zMcM+3GxsYqG7eDMKG/tbOzq5AwUX5WVtZjlH3r1i1mVxYZGdnn/v37JbxuCDEkh62goaFhpYRpYGDQcNiwYQWtW7d+WN1fq1atYHZTpgxxefxcLpcHw2MNPnrcVRq/p2ro4uLSA3pQVdNBvmnTpj95e3tjBCAdLxCBqj6wL7BaL0fRxcXFJ0BO8EHIa7x7924HhZPXMYjbvn37gYYNG1L//v0Fk5qzZ89CHi8giFNWWFh4EPkgPiAgYI21tTXufwiZqKgopl/EBMVvv/1G6OmFhYUtRzotLS2Qm1D2rVu3opDG3Nyc6Ty7dOny+5AhQ3C/oZ+fn4+CzJG0IYjU0tISbsTYMWvWrAdw44UL9IYLCgp2KG7JdHR0rqA3m5iYiOF4maO6HxB0oCAJIyOjNHHCgICAX6GrhDUB9KzQOfL7V69eRW9WuEY8Jn3QA7aysqqUMGFn6u3t7bp3714HZf5SU1OFVTS87IrCuLg4X/Q0hw4dGg/1QUUyysRt27atB3TGyshWIPOTh4dHXgXxzzWKiGYNHTp0bosWLVb06tVrbn5+vmF1BTg6Oi4yNDScFxcXNy8pKWmeo6PjPJlMNo+IoO9+pY4mTZpM9PDwYNtFHD58GJ2henMcP358SUhISP3Vq2LZGgjs119/ZatXioqK4qEzVLjhYkB27NjRCcbhYWFhwziyhoaGNGHCBOHFyczMxKw34j7BcsWJEyf2TU1N1QfxOjg4MDlvb29Glrdu3RLSYVlbnz6Y6KV1T58+LYS+DTPNRDQBZfXv3z/6iy++wHUh9gOxtLRkq3nu37+PPCgzMzOZ12n8+PEPMFN9+/btwjVr1ghlZGZman3wwQdYDRTIZREWFBTs4td//PEHffvtt4UGBgaYSPrXoaWldQh6TCLS4TcVjiUoLS2tGLpUfFTQK7527doZTFKlp6eX0S/OnTu38bvvvku5ubnMoS3Pp67Cbt26/Y7noamp2bKmZYIwYeVQw/Q/HTx4sFYJs7CwUB+jAWyr26dPnybw1AOzs6ioqCpXZ8FtGX5PeI5YTohzLJ2sYTvrXTIi6khEeqjYxYsXb2PZJNpobW3NJ0vrRZ3R6cDEqcLbVb2o078qERgYuA9OEUBee/bsgfv4tRBCzwtrRO3s7O5hHxixy/sTJ06AZIUeHOQvXLjA0k+ePHkVEbVF3KNHj1bp6ekFogerr6/vHxQUtEKUN/LvqKWlZQFTHCzlWrduXZmdAaOiooysrKxQJ5QHO0c2ux0eHm7NiRj54YiJiVkMD9/Qv549exaOYpnsxo0be2BvmT179lyAHM/D1tZWeCEsLCysxo0bd2f8+PEV6ji9vLy81NTU6PDhwx/w9GfPnv3pxx9/xI+OYmNjf3B2dk7CmnYdHZ0j2dnZbHvXf2r2z/+IiIif0cNUxCk9Sy/O41nOfXx8mOF6QUEBr4PK2dV3wly7du0Z6NaJiOm1iWjwRx99ROhRVdVY/G6+/fZbZpKG35uZmRlTR/FnXVXa+n7Pzc3NE+u+3d3dz/G6NmzYELrkekeYu3fvZj5UiegjXlcprGMEtLW1+/zDk3SeF52YmCjsZsjj9PX1r1RGmJCBI1aFP0eeRKUQtpywJVUp0XMU5mvJsbFYTbOtijDhXIFPgCH/Chwr1HoPc8qUKbC+KNM+2PJiFFNVm9EhGDt27H+rkqnv96BqqYjgFyxY4IrH8dFHHwl7+rRv356Z3/EepuLZVbpBnOJ+k/IY4HnjHuIrk+FpxLI8rqKwojZA/w9rl4rkEVdd2ZWlk+JFCED3OGXKlKMw94Fxb4sWLRzU1NQOwsgdGC9evJj1IniS6gjTw8PDUmHzqaqOpcHhw4fNofpITk42qegHwetQmyEnzGdZS16eMHlb9uzZ44itJYyNjQOJyEFHR2eXiYlJga2tLV5WftQ6YWIlFXqIvECE2N4C+nW5XF6pkw2MgMLCws4FBQU5HDx40IGIvkNa3j5xfvXtPC0tzb1x48bAG4tJDjs6OjLM+cfLwcHhNpri5ubmHRYW5pCTkwM3eayHGRgYGDls2DAnfGRgIhgSEsJc6Inb6OTkdADvDNzo6erq2hFRp+zsbA0fHx+n77777gLUX76+vpqYN3B0dATWdt27d3fo2bOng0wmcwgICGBLT48dOxZ348YN9mxmzJjRTENDw7lPnz4Ob7/9tgOcs3h5eTksWLDgqtgaB/roESNG2I8cOfIp1IB9+/b1OHPmjOArlYice/fuvRM25FDlnTp1Ch6sXornJsb4pTyvjjChkoAlAGw5iairso3EYgCshoqMjKx0skfZvJ5FrjYIE/XBHjL4weIclgvwSLRt27YsqHcU1g7HFPWuVcIkonsoT1GuABVWpeEdCg0NFRZUCDcVJyB7WG+MGzcuc/bs2ZnQo8nlctam8rL16drV1fUM2ubh4fG4qKhIPmPGjFJgoHCJJ/P392d6WchAP4tQgQd609idkzBnoaOjkw8dvOJ5MbNAIvoGq++g946KipJHR0dD906Yo/D09OyrqakJixBmv425A7jKw0q2W7duPcRyTXRSoqOjs0tKSg4BM3igwigN50TUKisri9Vn0aJF2WlpaUgDCxi2cg8y2dnZLUGS2traxTk5OXIfHx9506ZN4YkJRAxbZydYvpiYmDzJz8+Xz58/vxjpMzIyWBmQkY5aQmDZsmURmGHv2bMnjPHDKyuGiN6Hx6EVK1YoTZjnzp0DgdRow6jK6lGT+NogzIiIiHmwqSWidnAoO3r06AysQMrIyNCEUT8mTvjWtzKZrFLCfB49udzcXJiY/WuRQ3R0NHsxfXx8KiXMa9eu0b59+2w4rj4+PvAyXnrz5s19PK4+hRyvXr16YYJVLhoW6yiW6kIPuzoiIkLfyMjoIuyhN2zYsDYwMHBSYWGhQZMmTbCIgYYPH34cE6zoNcKyA4RpZWV1DW09fvw4tv0gV1dX/sGTwXcDZKKjoxmpwkwOMqampqecnJy+unLlCq5lBgYGN6BLlslkbE/62NhYF8jhz9nZGRYIsnv37p3E3IJivoHBC7d9sH7BhbGxcUv8lgwNDdlSa8Tt3bs3z8PDg1mGTJs2jVq3bi34ZUAnBpNGWKDCMpP+1Q4C/Mcnzr2iONyvLF6cttx5nU/ulCtfuKwNwjx69KhBVFQUM7fy8/NrBosGLS0twXIBhYswq5QwIQfLB6zBh4lWVX+Y1BORsNA+IqajxsqvMi8Md1J86dKlSglTyERR34EDB36BHpm2tna9MrsR11Ox2Vupi4uLYB8MrLW0tCaCIPmOnTNnznRF7/G///3vv3SYtra23OQPv9O3MLE5Y8YMRpiKSVHCx0Qmk3W7evWqt4uLC3qwsElm7uKwguwfyMhcXLfQ0FC4WywNCAhgz2LQoEHZUBlAVuGHVDZx4sTAwYMHYyJSOGC+xwlz586dLWFtgrZguP3777/DzwVMVdg2JfhQw2IjPz8f+baJj4+/1bt371K0QchQOpEQqCkCtUGYvC7cTymWvNra2mJ1lnAoS5jnzp0b0KhRo8+bN29e5Z+amtrAoqKiIUIBohO8XBhGiqJgRsNe1KSkpAoJ87ffflvxzjvvXDUwMBAcmZiamo5HD2nevHlYXFEvj5MnT7J2/fLLLwJhoqLr16+fDrLnttVVESaf9EG6UaNGdcCwmhMmHP6C4KDigFNnmAWCANetW7cC1iJIA1UMJzgxSLB+gYoDQ3Rvb+8P0JMkIk8M1eFjAbIw/8rPz4caRTjEhAm/CtevX48aPXo0qwfqgiH45s2bmWkaPgKYzMOID7bc+MPHcebMmV8JGUonEgI1RaA2CBO6qJiYGGabCn+f+FE/efKEbYcbHh6+PC8vj70cijpX2cOsabvE6caMGSPoU3k8zM9AIER0AHHwh0BEg/j9IUOGsK0uTpw4kcvjJkyYMBr2mO3bt9/C44ioXnk2gqUHmmNiYpIh3m1g/vz53+DDoa6uvgp154T52Weffc7bUn6WHPHlCROmVsjfzc1N8LjF0/MQhAkS5Nfi0N7eHvGl2Dn0+PHjTEbhgLskJCSEjSLE8jgXE6alpWUjhdMf6DMPbNmyJQF6UawmhCz8VnTp0kV4ZuK8+EcaqgZfX19sWs/MHMUy0rmEQJUI1AZhOjs7s8UGBQUF47CwQGy+A9vecjaftU6Y6AnB0TMRsZVkAAQTUbDz5eCcP38+HxMdwcHBbBjp5OS0F56ciAgvFvPo7+bmhl4MXbhwgbkhtLe3XwMPW1euXPmT5/Oiw/j4+L5wiIO/2NjYD3kv38nJiTmguXfv3nbU0djYmJkVhYeH58IrPRFBr8hmybdu3SqQISfM6dOnwym37Msvv1wKy44RI0ZgMkU4YmNjnYqKitiWItgQDcQl3BSdmJqaTsRHB4tRgoODP8Mt+M7FUBqrzgwNDdlGeIjnBAeLFk7A0GFiMk6UJZ4lX7otww4M6GWePn26kMtgjiE5OfmBIs+bPXr0yIYfCCJiG8FxOSmUEKgWgdogTHgsx+8dhuEwysfL4+fnl7dp06aHM2fOzMfiAf4yVDXpU23llRBAOZcuXRoDQ/WBAweeOHDgwCQLC4sQEMr27dtBhkyf7Orqyl40CwuL3ciWiIxXrlxJLi4uTM/n5eX1BKR7+PBh4WU1NDS0hygWVyhRlToRwUSPtbX1Y5AP9k6SyWSDsQ1I3759schD2Ls9JiYGG8yx2XNMbKK3B3tVtAcTRIaGhmx5s66ubgc4mIbecOnSpaa5ubn9FLt+sokUDPFh8oOZ9aysrN2urq5P4FUM+WA4vGvXLrY6jzfezc3tA6hoRowYkcVdQuLehAkT7gPf0NDQSVwWq8/wG4JKgOd39+7dGTiHTnrChAnz8Tzw0XJxcWGmQ0FBQZkgX5lMVgL1ARaQYPUh3xTv3LlzUKeUuru7V+tvgddDCiUEBARqgzANDAzU4uLi6N69eyDKZhkZGeegM3R3dx/7119/vSMU/s9JrfcwQSIjR45shgkCc3NzpneDn1axEX1wcPB/mjdv3iY6OlrY47ywsLAXNmQDIcKWMCEh4RJ8BPD6o20DBgzowSdSePyLDkFE27Zt63/o0CF8FEjhf7bMvleoIyZGoPOMi4uLTElJuY+9jLp3794OTncUkybYvrhjYWEhVCrokYGIZHfu3Gnt6+trBdtWOMlJT0/PNTc3by6Xy/vDLCgtLe10Zmbm39j2hKfhmBBRPyyxTk9PL2MhAsuDK1euCL1CyMvl8i9hxpWUlLQPXs+gyoEeFO4lsagAw3rM4vv4+EDPzPzxEtFb7u7uXWGNgRV3eOaKDzQbfg8ZMuRvhd208Jx53aRQQqBaBEpLSzEZU/osa6QrM1xH4bwHV0VFap0wK6qHEvWqosr/fys2NtYcEx//H1M/z8q3t/x1RbWuSKaiOKStLL66e+XLrbFyY18AAAEFSURBVCqf8rLKXiNP/FlbW5ugqn5+foI3M2XzkOQkBAQEsCWqYusMWyFShZPyhKlCUojWCWGqWCelxTHcW79+vTQDqzRiL04QnQL4u3hxNZBKfiUQOHr0KPbtht7q95o0CISJfcFrkhaEqZhMqWHyF5csJibmoLu7O4aJSi9YeHG1fb1LFvdcxeevNypS62uMwP79+++CNLHEs6SkZLUqGcXFxWlxw2NV0kF2yZIlP6alpcHRsnRICEgISAi8PAj4+/v7YAYUezXVpNbP8uV+lrQ1qauURkJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBCQEJAQkBC4FVC4P8AZQzWB/YAM20AAAAASUVORK5CYII=)\n",
    "\n",
    "\n",
    "Focal Loss (often reweighted by a factor `\\alpha`\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwvQSS-7YM-Q"
   },
   "outputs": [],
   "source": [
    "def weighted_soft_dice_loss(output, target, weights):\n",
    "    s = (10e-20)\n",
    "    output = output.type(torch.FloatTensor)\n",
    "    target = target.type(torch.FloatTensor)\n",
    "\n",
    "    intersect = torch.sum(output * target * weights)\n",
    "    dice = 2 * intersect / (torch.sum(output*weights) + torch.sum(\n",
    "        target*weights)+s)\n",
    "    return 1 - dice\n",
    "\n",
    "def cross_entropy(pred, label, weight=None, reduction='mean',\n",
    "                  avg_factor=None):\n",
    "    # element-wise losses\n",
    "    # print((pred[:,:]*weight).sum(), label.sum(), pred.shape)\n",
    "    loss = F.cross_entropy(pred, label.type(torch.LongTensor), reduction='none')\n",
    "\n",
    "    # apply weights and do the reduction\n",
    "    if weight is not None:\n",
    "        weight = weight.float()\n",
    "    loss = (loss * weight).sum()/weight.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=1, gamma=2, weights=None):\n",
    "\n",
    "      # Hint: Look at the Retina Net implementation in the slides!\n",
    "        \n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = -1 * alpha * (1-pt)**gamma * BCE_loss\n",
    "        # test = torch.nn.functional.softmax(inputs,1)[:,1,:] > 0.5\n",
    "        # print((targets*weights).sum(),weights.sum(),\n",
    "        #       (test*targets*weights).sum(),\n",
    "        #       (test*weights).sum())\n",
    "\n",
    "        if weights is not None:\n",
    "            if weights.sum() == 0:\n",
    "                return 0\n",
    "            return torch.sum(F_loss*weights)/weights.sum()\n",
    "        else:\n",
    "            return F_loss.mean()\n",
    "\n",
    "\n",
    "def smooth_l1_loss(pred, target, weights=None, beta=1.0):\n",
    "    assert beta > 0\n",
    "    assert pred.size() == target.size() and target.numel() > 0\n",
    "    diff = torch.abs(pred - target)\n",
    "    print(target.shape, pred.shape)\n",
    "    loss = torch.where(diff < beta, 0.5 * diff * diff / beta,\n",
    "                       diff - 0.5 * beta)\n",
    "    if weights is None:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return (loss * weights).sum() / weights.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-SHZa-EtVgf"
   },
   "source": [
    "# **Part 5 - Putting things together**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hOIU6A22yZf"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1tRUG6s7-0oMbqPRQdvghuw1SsPmE1E2G\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Targets for RPN, Non-max suppression and two-stage model creation</b><br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vjiqjo_3c7G"
   },
   "source": [
    "## **Creating targets for RPN**\n",
    "\n",
    "We have all what we need to appropriately train the RPN. We need however to find what are the targets and make sure that the imbalance between negative and positive samples is taken care of.\n",
    "\n",
    "Complete the following `target_anchors_single` function that takes as argument the list of anchors, the boxes and associated labels, a possible mask of where to accept anchors from (in our case, we want to stay in the brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O11Dhyny3bhH"
   },
   "outputs": [],
   "source": [
    "def target_anchors_single(flat_anchors, gt_bboxes, gt_labels,\n",
    "                          img_shape, mask=None,sampling=True):\n",
    "    \"\"\"\n",
    "    Among the choice of anchors across the feature map, decides of the assignment to ground truth boxes \n",
    "    in order to create the necessary tensors for evaluation of the RPN head\n",
    "    Take as arguments:\n",
    "    flat_anchors : the list of anchors across the whole feature map\n",
    "    gt_bboxes : the ground truth boxes\n",
    "    gt_labels: the associated labels\n",
    "    img_shape: the spatial shape of the image\n",
    "    mask: mask over which anchors are allowed\n",
    "    sampling: whether a random sampling scheme is performed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define which anchors are appropriate to use. \n",
    "    inside_flags = anchor_inside_flags(flat_anchors, torch.from_numpy(\n",
    "        np.asarray((1,) * len(flat_anchors))), img_shape, mask=mask)\n",
    "    if not inside_flags.any():\n",
    "        return (None,) * 6\n",
    "    # assign gt and sample anchors\n",
    "    # assign gt and sample anchors with a minimum iou for assignment of 0.3 and a positive assignment for boxes with IoU above 0.7 \n",
    "\n",
    "    anchors = flat_anchors[inside_flags, :]\n",
    "    assign_anchors = MaxIoUAssigner(0.7, 0.3)\n",
    "    assign_results = assign_anchors.assign(anchors, gt_bboxes,\n",
    "                                           gt_labels=gt_labels)\n",
    "    \n",
    "    # Randomly sample to get 256 boxes with a 50/50 ratio of positive vs negative boxes\n",
    "    # Hint: remember the auxiliary sampling functions we defined!\n",
    "    # We use for that the class RandomSampler that has been defined before.\n",
    "    sample_anchors = RandomSampler(256, 0.5, )\n",
    "    sampled_boxes = sample_anchors.sample(assign_results, anchors,\n",
    "                                          gt_bboxes, gt_labels)\n",
    "\n",
    "    # Initialise the target for bounding boxes, the labels and associated weights\n",
    "    num_valid_anchors = anchors.shape[0]\n",
    "    bbox_targets = torch.zeros_like(anchors)\n",
    "    bbox_weights = torch.zeros_like(anchors)\n",
    "    labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)\n",
    "    label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n",
    "\n",
    "    # Identify positive and negative indices from the sampled_boxes\n",
    "    pos_inds = sampled_boxes.pos_inds\n",
    "    neg_inds = sampled_boxes.neg_inds\n",
    "    # We need only to learn boxes targets from positive samples. In this case, the weights should be 1\n",
    "    if len(pos_inds) > 0:\n",
    "        pos_bbox_targets = bbox2delta(sampled_boxes.pos_bboxes,\n",
    "                                      sampled_boxes.pos_gt_bboxes)\n",
    "        bbox_targets[pos_inds, :] = pos_bbox_targets\n",
    "        bbox_weights[pos_inds, :] = 1.0\n",
    "        label_weights[pos_inds] = 1\n",
    "        if gt_labels is None:\n",
    "            # If there are no labels you put 1 only. \n",
    "            labels[pos_inds] = 1\n",
    "        else:\n",
    "            labels[pos_inds] = gt_labels[sampled_boxes.pos_assigned_gt_inds]\n",
    "            \n",
    "    # For negative samples, we need to learn from these examples so we need to set the weight of the labelling to 1\n",
    "    if len(neg_inds) > 0:\n",
    "        label_weights[neg_inds] = 1.0\n",
    "\n",
    "    # map up to original set of anchors\n",
    "    num_total_anchors = flat_anchors.size(0)\n",
    "    labels = unmap(labels, num_total_anchors, inside_flags)\n",
    "    label_weights = unmap(label_weights, num_total_anchors, inside_flags)\n",
    "    bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n",
    "    bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n",
    "\n",
    "    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds,\n",
    "            neg_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzxlwSEk3dYf"
   },
   "source": [
    "## **Creating proposals from output of RPN to input of RCNN**\n",
    "\n",
    "Complete the `create_proposals` function that takes in the scores of the rpn that have already been reshaped to be of size (2, total_num_anchors), the associated targets delta, the anchors, GT boxes and labels as input.\n",
    "Optionally we can a padding around the defined boxes prior to application of ROI align in order to have a bit more context.\n",
    "\n",
    "The aim of this function is to perform the **non-maximum suppression** on the anchors selected by the RPN. Applying the delta over the RPN is optional as indicated by the apply_rpn flag.\n",
    "\n",
    "*Non-Maximum Suppression (NMS) is used in order to overcome the low precision of the proposals done by the RPN network. The RPN is in charge of providing proposals to the RCNN, and thus high recall is prioritized, sometimes in expense of loose constraints that create a lot of false positive samples. Because allowing for many proposals into the RCNN network increases the computational cost, NMS is previously applied to filter the proposals according to their scores.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQROexVtMxl_"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=17xXgo1fCyfPnn0VRLhPu-T90fDF3QTlV\" style=\"width:500px;height:250px;\">\r\n",
    "<caption><center> <b>Non-Max suppression example</b> Source: https://www.kdnuggets.com/2018/09/object-detection-image-classification-yolo.html <br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTgzyo87qicp"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_proposals(rpn_cls, rpn_bbox, anchors, gt_bboxes, gt_labels,\n",
    "                     apply_rpn=False, add_pad=5, shape=[128,128],\n",
    "                     num_nms_post=1000, num_nms_pre=2000):\n",
    "    \"\"\"\n",
    "    From the output of the RPN, this function allows to sample the appropriate number of proposals \n",
    "    to go to the RCNN stage\n",
    "    Take as input rpn_cls, rpn_bbox output of the RPNHead, the anchors, ground_truth boxes and associated labels. \n",
    "    The correction is applied to the boxes if apply_rpn is set to True otherwise original anchors found positive are directly used\n",
    "    num_nms_pre is the number of proposals prior to NMS correction\n",
    "    num_nms_post is the maximum number of proposals after NMS correction\n",
    "    \"\"\"\n",
    "    # Identify the top num_nms_pre boxes according to RPNHead \n",
    "    # Hint: once you softmax the outputs, to select the best boxes, you can use topk\n",
    "    # Ref: https://pytorch.org/docs/stable/generated/torch.topk.html\n",
    "    \n",
    "    softmax_cls = rpn_cls.permute(1, 0).softmax(dim=1)[:, 1] # The \"scores\" given by the RPN are ran through a softmax. \n",
    "    softmax_top, topk_inds = softmax_cls.topk(num_nms_pre)\n",
    "    rpn_box_top = rpn_bbox[topk_inds, :]\n",
    "    anchors_top = anchors[topk_inds, :]\n",
    "    if apply_rpn:\n",
    "        applied_box = delta2bbox(rpn_box_top, anchors_top)\n",
    "    else:\n",
    "        applied_box = anchors_top\n",
    "    \n",
    "    # Perform NMS correction with IoU 0.7 - cf torchvision.ops.nms function\n",
    "    nms_idx = torchvision.ops.nms(applied_box, softmax_top, 0.7) # - redundancy\n",
    "\n",
    "    nms_boxes = applied_box[list(nms_idx.numpy())[0:num_nms_post], :]\n",
    "    # Remove proposals to close to the edge\n",
    "    # Multiply booleans\n",
    "    valid_nms = (nms_boxes[:,0] > add_pad ) * \\\n",
    "                (nms_boxes[:, 1] > add_pad) * \\\n",
    "                (nms_boxes[:,2] < shape[0]-add_pad) * \\\n",
    "                (nms_boxes[:, 3] < shape[1]-add_pad)\n",
    "    nms_boxes = nms_boxes[valid_nms,:]\n",
    "    num_nms_post = np.minimum(num_nms_post, nms_boxes.shape)[0]\n",
    "    # print(num_nms_post, \" is new num_nms_post\")\n",
    "    bbox_targets = torch.zeros(num_nms_post, 4)\n",
    "    bbox_weights = torch.zeros(num_nms_post, 4)\n",
    "    labels = torch.zeros(num_nms_post).type(torch.LongTensor)\n",
    "    label_weights = torch.zeros(num_nms_post)\n",
    "    assign_newboxes = MaxIoUAssigner(0.7, 0.3)\n",
    "    assign_result = assign_newboxes.assign(nms_boxes, gt_bboxes, gt_labels)\n",
    "    length_pos = (assign_result.gt_inds > 0).sum()\n",
    "    \n",
    "    # Define the anchors randomly chosen for next stage \n",
    "    # Similarly to what was done for the RPN, sample appropriately the proposals so as to keep with the ratio 1:2. \n",
    "    # Indicate the maximum length of samples according to length_pos\n",
    "    sample_anchors = RandomSampler(np.minimum(1000,\n",
    "                                              3*length_pos+1), 0.3)\n",
    "    sampled_boxes = sample_anchors.sample(assign_result, nms_boxes,\n",
    "                                          gt_bboxes, gt_labels)\n",
    "\n",
    "\n",
    "    pos_inds = sampled_boxes.pos_inds\n",
    "    neg_inds = sampled_boxes.neg_inds\n",
    "\n",
    "    pos_inds = list(pos_inds.numpy())\n",
    "    neg_inds = list(neg_inds.numpy())\n",
    "\n",
    "    if len(pos_inds) > 0:\n",
    "        pos_bbox_targets = bbox2delta(sampled_boxes.pos_bboxes,\n",
    "                                      sampled_boxes.pos_gt_bboxes)\n",
    "        bbox_targets[pos_inds, :] = pos_bbox_targets\n",
    "        bbox_weights[pos_inds, :] = 1.0\n",
    "        if gt_labels is None:\n",
    "            labels[pos_inds] = 1\n",
    "        else:\n",
    "\n",
    "            labels[pos_inds] = gt_labels[sampled_boxes.pos_assigned_gt_inds]\n",
    "    labels[neg_inds] = 0\n",
    "    label_weights[neg_inds] = 1\n",
    "    label_weights[pos_inds] = 1\n",
    "    \n",
    "    # Add a padding around the boxes using add_pad argument\n",
    "    nms_boxes = nms_boxes + torch.Tensor([[-add_pad],[-add_pad],[add_pad],[add_pad]]).repeat_interleave(nms_boxes.shape[0],1).transpose(1,0)\n",
    "\n",
    "    return labels, label_weights, bbox_targets, bbox_weights, nms_boxes, \\\n",
    "           pos_inds, neg_inds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvuifFMdOXl7"
   },
   "source": [
    "## **Definition of the training process**\n",
    "The following class defines the two stage detector\n",
    "\n",
    "In the first instance, from the definition of the backbone network, complete the extract_feat function so that it returns the layer before the final output as well as the final class\n",
    "\n",
    "Then complete the `forward_train` for the different missing pieces. Don't forget to use the `multi_apply` function each time you have to deal with the fact that you don't have the same boxes defined for each of the images in your batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfhNyeSjUO_a"
   },
   "outputs": [],
   "source": [
    "class TwoStageDetector(nn.Module):\n",
    "    \"\"\"Base class for two-stage detectors.\n",
    "\n",
    "    Two-stage detectors typically consisting of a region proposal network and a\n",
    "    task-specific regression head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 rpn_head=None,\n",
    "                 bbox_roi_extractor=None,\n",
    "                 rcnn_head=None,\n",
    "                #  pretrained='/Users/csudre/Documents/Teaching/AML/AML_lecture_5/Backbone_OD.pt'\n",
    "                pretrained = None):\n",
    "        super(TwoStageDetector, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        if rpn_head is not None:\n",
    "            self.rpn_head = rpn_head\n",
    "\n",
    "        if rcnn_head is not None:\n",
    "            self.bbox_roi_extractor = bbox_roi_extractor\n",
    "            self.rcnn_head = rcnn_head\n",
    "\n",
    "        self.init_weights(pretrained=pretrained)\n",
    "\n",
    "    @property\n",
    "    def with_rpn(self):\n",
    "        return hasattr(self, 'rpn_head') and self.rpn_head is not None\n",
    "\n",
    "    @property\n",
    "    def with_bbox(self):\n",
    "        return hasattr(self, 'rcnn_head') and self.rcnn_head is not None\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        if pretrained is not None:\n",
    "            if 'backbone' in pretrained.keys():\n",
    "                self.backbone.load_state_dict(torch.load(pretrained['backbone'], map_location=torch.device(device)))\n",
    "            if 'rpn_head' in pretrained.keys():\n",
    "                self.rpn_head.load_state_dict(torch.load(pretrained['rpn_head'], map_location=torch.device(device)))\n",
    "            if 'rcnn_head' in pretrained.keys():\n",
    "                self.rcnn_head.load_state_dict(torch.load(pretrained['rcnn_head'], map_location=torch.device(device)))\n",
    "            if 'full' in pretrained.keys():\n",
    "                self.load_state_dict(\n",
    "                torch.load(pretrained, map_location=torch.device(device)))\n",
    "\n",
    "    def extract_feat(self, img):\n",
    "        \"\"\"Directly extract features from the backbone+neck\n",
    "        \"\"\"\n",
    "        list_feat = self.backbone(img)\n",
    "        return list_feat[-1], list_feat[0]\n",
    "\n",
    "    def forward_train(self,\n",
    "                      img,\n",
    "                      gt_bboxes,\n",
    "                      img_meta=None,\n",
    "                      gt_labels=None,\n",
    "                      gt_bboxes_ignore=None,\n",
    "                      label_seg=None,\n",
    "                      training_stage=['rpn_head']\n",
    "                      ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): of shape (N, C, H, W) encoding input images.\n",
    "                Typically these should be mean centered and std scaled.\n",
    "\n",
    "            img_meta (list[dict]): list of image info dict where each dict has:\n",
    "                'img_shape', 'scale_factor', 'flip', and may also contain\n",
    "                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n",
    "                For details on the values of these keys see\n",
    "                `mmdet/datasets/pipelines/formatting.py:Collect`.\n",
    "\n",
    "            gt_bboxes (list[Tensor]): each item are the truth boxes for each\n",
    "                image in [tl_x, tl_y, br_x, br_y] format.\n",
    "\n",
    "            gt_labels (list[Tensor]): class indices corresponding to each box\n",
    "\n",
    "            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n",
    "                boxes can be ignored when computing the loss.\n",
    "\n",
    "            gt_masks (None | Tensor) : true segmentation masks for each box\n",
    "                used if the architecture supports a segmentation task.\n",
    "\n",
    "            proposals : override rpn proposals with custom proposals. Use when\n",
    "                `with_rpn` is False.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, Tensor]: a dictionary of loss components\n",
    "        \"\"\"\n",
    "        losses = dict()\n",
    "        x, full = self.extract_feat(img)\n",
    "        print(\"Features extracted\")\n",
    "        if 'backbone' in training_stage and label_seg is not None:\n",
    "            print(\"Training backbone\")\n",
    "\n",
    "            # soft_dice_loss ? \n",
    "            losses['backbone'] = weighted_soft_dice_loss(torch.sigmoid(full),\n",
    "                                                Variable(label_seg, requires_grad=True))\n",
    "            \n",
    "            test_out = torch.sigmoid(full) > 0.5\n",
    "            print(test_out.sum())\n",
    "            losses['rpn_cls'] = 0\n",
    "            losses['rpn_bbox'] = 0\n",
    "            losses['rcnn_cls'] = 0\n",
    "            losses['rcnn_bbox'] = 0\n",
    "\n",
    "        if 'rpn_head' in training_stage or 'rcnn_head' in training_stage:\n",
    "            print(training_stage)\n",
    "            print(\"running RPN\")\n",
    "            anchors_gen = AnchorGenerator((2, 2), (0.5, 1, 2), (0.5, 1, 2))\n",
    "            anchors = anchors_gen.grid_anchors(img.shape[-2:], stride=1)\n",
    "            print(\"anchors generated\")\n",
    "            anchors_full = [anchors for i in range(0, len(gt_bboxes))]\n",
    "            img_shape_full = [img[i].shape[-2:] for i in\n",
    "                              range(0, len(gt_bboxes))]\n",
    "            valid_flags_full = [(1,) * len(anchors) for i in\n",
    "                                range(0, len(gt_bboxes))]\n",
    "            if gt_labels is None:\n",
    "                gt_labels = [\n",
    "                    torch.from_numpy(np.asarray((1,) * len(gt_bboxes[i])))\n",
    "                    for i in range(0, len(gt_bboxes))]\n",
    "\n",
    "            # RPN forward and loss\n",
    "            mask_full = img>-2\n",
    "            rpn_cls, rpn_bbox, _ = self.rpn_head(x)\n",
    "            print(\"RPN performed\")\n",
    "            (labels, label_weights, bbox_targets, bbox_weights, pos_inds_list,\n",
    "             neg_inds_list) = multi_apply(target_anchors_single, \n",
    "                                          anchors_full,\n",
    "                                          gt_bboxes,\n",
    "                                          gt_labels,\n",
    "                                          img_shape_full,\n",
    "                                          mask_full)\n",
    "            print(\"Assignment RPN done\")\n",
    "\n",
    "            rpn_cls_perm = rpn_cls.permute(0, 2, 3, 1)\n",
    "            print(rpn_cls.shape)\n",
    "            # len(gt_bboxes) batch size bcse it's a list of lists (nested lists)\n",
    "            # -1: 128*128*9\n",
    "            #2 : \"class\" (pos/-)\n",
    "            # DIMENSIONS???????\n",
    "            rpn_cls_res = rpn_cls_perm.reshape([len(gt_bboxes), -1, 2])\n",
    "            rpn_prob = torch.nn.functional.softmax(rpn_cls_res, dim=-1)[:,:,1]\n",
    "\n",
    "            rpn_cls_fin = rpn_cls_res.permute(0, 2, 1)\n",
    "            rpn_bbox_perm = rpn_bbox.permute(0, 2, 3, 1)\n",
    "            rpn_bbox_res = rpn_bbox_perm.reshape([len(gt_bboxes), -1, 4])\n",
    "\n",
    "            print(\"Reshaping done\")\n",
    "            for i in range(len(gt_bboxes)):\n",
    "                if labels[i] is None:\n",
    "                    # labels[i] = torch.zeros_like(anchors[:,0])\n",
    "                    labels[i] = torch.zeros_like(anchors[:, 0]).type(torch.LongTensor)\n",
    "                    label_weights[i] = torch.zeros_like(anchors[:,0])\n",
    "                    bbox_targets[i] = torch.zeros_like(anchors)\n",
    "                    bbox_weights[i] = torch.zeros_like(anchors)\n",
    "\n",
    "            # labels_fin = torch.stack(labels).type(torch.FloatTensor)\n",
    "            labels_fin = torch.stack(labels)\n",
    "            label_weights_fin = torch.stack(label_weights).type(\n",
    "                torch.FloatTensor)\n",
    "            loss_cls = cross_entropy(rpn_cls_fin, torch.stack(labels),\n",
    "                                     torch.stack(label_weights))\n",
    "            loss_cls_focal = focal_loss(rpn_cls_fin,\n",
    "                                        targets=labels_fin.type(torch.LongTensor),\n",
    "                                        weights=Variable(label_weights_fin, requires_grad=True))\n",
    "\n",
    "            loss_cls_dice = weighted_soft_dice_loss(rpn_prob,\n",
    "                                                    Variable(labels_fin.type(torch.FloatTensor), requires_grad=True),\n",
    "                                                    Variable(label_weights_fin, requires_grad=True))\n",
    "            \n",
    "            loss_cls = loss_cls_dice + loss_cls_focal\n",
    "            print(loss_cls_dice, loss_cls_focal)\n",
    "            loss_rpnbb = smooth_l1_loss(rpn_bbox_res, torch.stack(bbox_targets),\n",
    "                                        torch.stack(label_weights).repeat(1, 4).reshape([len(gt_bboxes), -1, 4]))            \n",
    "            losses['rpn_cls'] = loss_cls\n",
    "            losses['rpn_bbox'] = loss_rpnbb\n",
    "            losses['rcnn_cls'] = 0\n",
    "            losses['rcnn_bbox'] = 0\n",
    "            print(\"Loss RPN calculated \\n\")\n",
    "        if 'rcnn_head' in training_stage:\n",
    "            # Create proposals by applying bbox pred\n",
    "            rcnn_lab, rcnn_lab_weights, rcnn_targets, rcnn_weights_bbox, \\\n",
    "            nms_boxes, rcnn_pos_inds, rcnn_neg_inds = multi_apply(create_proposals,\n",
    "                                                                  rpn_cls_fin,\n",
    "                                                                  rpn_bbox_res, \n",
    "                                                                  anchors_full,\n",
    "                                                                  gt_bboxes,\n",
    "                                                                  gt_labels,\n",
    "                                                                  (False,)*len(gt_bboxes))\n",
    "            print(\"NMS performed for proposal \\n\")\n",
    "            # Number of proposals is a batch\n",
    "            rcnn_lab = torch.cat(rcnn_lab)\n",
    "            rcnn_lab_weights = torch.cat(rcnn_lab_weights)\n",
    "            rcnn_weights_box = torch.cat(rcnn_weights_bbox)\n",
    "            rcnn_targets = torch.cat(rcnn_targets)\n",
    "\n",
    "            # Transformation to get all corresponding aligned ROI (bbox extraction) using roi align with output size 7x7\n",
    "\n",
    "            aligned_feat = torchvision.ops.roi_align(x, nms_boxes, 7)\n",
    "            print(\"Alignment performed \\n\", aligned_feat.shape)\n",
    "\n",
    "            # Apply RCNN head to ROIs\n",
    "            \n",
    "            rcnn_cls, rcnn_bbox = self.rcnn_head(aligned_feat)\n",
    "            print(rcnn_cls.shape, rcnn_bbox.shape, rcnn_lab.shape,\n",
    "                  rcnn_targets.shape)\n",
    "            loss_rcnn_cls = cross_entropy(rcnn_cls, rcnn_lab, rcnn_lab_weights)\n",
    "            loss_rcnn_bbox = smooth_l1_loss(rcnn_bbox, rcnn_targets)\n",
    "            losses['loss_rcnn_cls'] = loss_rcnn_cls\n",
    "            losses['losses_rcnn_bbox'] = loss_rcnn_bbox\n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZJyT8sXPbWM"
   },
   "source": [
    "# Part 6 Inference\n",
    "\n",
    "<b> Inference: The process of using the trained model to make a prediction </b>\n",
    "\n",
    "Complete the following function `inference_rpn` to get first the appropriate proposals function - Don't forget the same padding as you had for the training -\n",
    "\n",
    "Complete the function `inference_rcnn` that calls on to `inference_rpn` and provide the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsvH8rGQ2UHC"
   },
   "source": [
    "<img src=\"https://docs.google.com/uc?export=download&id=1qosFHp3SfILtyix7H7MQbuB9QNyvMtAg\" style=\"width:500px;height:250px;\">\n",
    "<caption><center> <b>Final steps: Inference and model improvement</b><br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElxTFHKUW_ju"
   },
   "outputs": [],
   "source": [
    "def inference_rpn(model, img, num_box=200, padding=5,size=(5, 5), scales=(0.5, 1, 2), ratios=(0.5, 1, 2)):\n",
    "        x, final = model.extract_feat(img)\n",
    "        # We create a mask so as to get rid of all the non-brain proposals. We will then discard all the boxes out of \n",
    "        # mask\n",
    "        mask = img>-2\n",
    "        rpn_cls, rpn_bbox,_ = model.rpn_head(x)\n",
    "        anchors_gen = AnchorGenerator(size,scales,ratios)\n",
    "        anchors = anchors_gen.grid_anchors(img.shape[-2:], stride=1)\n",
    "        anchors_full = [anchors for i in range(img.shape[0])] \n",
    "        \n",
    "        # Select the best proposals given the scores from the rpn and the predicted delta. num_box indicates half\n",
    "        # the number of boxes that should be selected from the RPN stage\n",
    "        def select_best_prop(rpn_cls, anchors, rpn_bbox, num_box, img):\n",
    "            #print(\"rpn_cls\", rpn_cls.shape, \"anchors_shape\", anchors.shape, \"rpn bbox shape\", rpn_bbox.shape,  \"shape of the image\", img.shape)\n",
    "            valid = (img > -2).view(-1).repeat_interleave(9)\n",
    "            #print(valid.shape)\n",
    "            rpn_cls = rpn_cls.reshape([18,-1])\n",
    "            rpn_cls = rpn_cls.permute(1,0)\n",
    "            rpn_cls = rpn_cls.reshape([-1, 9, 2])\n",
    "            rpn_cls = rpn_cls.reshape([-1, 2])\n",
    "            scores = rpn_cls.softmax(dim=1)[:,1]\n",
    "            scores = torch.where(valid, scores,torch.ones_like(scores)*-1.0)\n",
    "            _, best_anchors = scores.topk(num_box*2, dim=0)\n",
    "            anchors_selected = anchors[best_anchors, :]\n",
    "            rpn_bbox = rpn_bbox.reshape([36, -1])\n",
    "            rpn_bbox = rpn_bbox.permute(1,0)\n",
    "            rpn_bbox = rpn_bbox.reshape([-1, 9, 4])\n",
    "            rpn_bbox = rpn_bbox.reshape([-1, 4])\n",
    "            rpn_selected = rpn_bbox[best_anchors, :]\n",
    "            anchors_modified = delta2bbox( anchors_selected, rpn_selected)\n",
    "            scores_best = scores[best_anchors]\n",
    "            scores_mod = scores_best\n",
    "            scores_nomod = scores_best\n",
    "            \n",
    "            # Select only anchors valid according to padding possibilities\n",
    "            if padding > 0:\n",
    "                indices_valid_mod = (anchors_modified[:,0]>padding) * (anchors_modified[:,1] > padding) * (anchors_modified[:,2] < img.shape[1] - padding) * (anchors_modified[:,3]<img.shape[2]-padding)  \n",
    "                indices_valid_nomod = (anchors_selected[:,0]>padding) * (anchors_selected[:,1] > padding) * (anchors_selected[:,2] < img.shape[1] - padding) * (anchors_selected[:,3]<img.shape[2]-padding)\n",
    "                anchors_modified = anchors_modified[indices_valid_mod, :]\n",
    "                anchors_selected = anchors_selected[indices_valid_nomod, :]\n",
    "                scores_mod = scores_best[indices_valid_mod]\n",
    "                scores_nomod = scores_best[indices_valid_nomod]\n",
    "                \n",
    "            # Perform NMS with IoU of 0.5\n",
    "            final_idx = torchvision.ops.nms(anchors_modified, scores_mod, 0.5)\n",
    "            final_idx_nomod = torchvision.ops.nms(anchors_selected, scores_nomod, 0.5)\n",
    "            len_final = np.minimum(num_box, final_idx.shape[0])\n",
    "            #print(\"Final number of boxes: \", len_final)\n",
    "            final_anchors = anchors_modified[final_idx[:len_final],:]\n",
    "            final_anchors_nomod = anchors_selected[final_idx_nomod[:np.minimum(num_box,final_idx_nomod.shape[0])],:]\n",
    "            scores_final = scores_mod[:len_final]\n",
    "            scores_final_nomod = scores_nomod[:np.minimum(num_box,final_idx_nomod.shape[0])]\n",
    "            \n",
    "            # Apply padding to be able to perform ROI align afterwards\n",
    "            final_anchors  = final_anchors + torch.Tensor([[-padding],[-padding],[padding],[padding]]).repeat_interleave(final_anchors.shape[0],1).transpose(1,0)\n",
    "            final_anchors_nomod = final_anchors_nomod + torch.Tensor([[-padding],[-padding],[padding],[padding]]).repeat_interleave(final_anchors_nomod.shape[0],1).transpose(1,0)\n",
    "            return final_anchors, scores_final,final_anchors_nomod,scores_final_nomod\n",
    "        # print(rpn_cls.shape, len(anchors_full), rpn_bbox.shape)\n",
    "        (nms_proposals, scores_final, final_nomod, scores_final_nomod) = multi_apply(select_best_prop,\n",
    "                                                                                     rpn_cls, \n",
    "                                                                                     anchors_full, \n",
    "                                                                                     rpn_bbox, \n",
    "                                                                                     (num_box,)*len(anchors_full),img)\n",
    "        # print(len(nms_proposals), \"is length of nms proposals\")\n",
    "\n",
    "        return x, nms_proposals, torch.sigmoid(final), rpn_cls, final_nomod, scores_final, scores_final_nomod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd5qHhB5GsEN"
   },
   "outputs": [],
   "source": [
    "def inference_rcnn(model, img, thresh=0.4, padding=5):\n",
    "        features, proposals, backbone_out, scores_anchors, selected_anchors, scores_final, scores_final_nomod  = inference_rpn(model, img, 500)\n",
    "\n",
    "        averaged_features = torchvision.ops.roi_align(features, \n",
    "                                                      selected_anchors,\n",
    "                                                      output_size=7)\n",
    "        print(\"What goes in the RCNN has shape... \", averaged_features.shape)\n",
    "        rcnn_cls, rcnn_bbox = model.rcnn_head(averaged_features)\n",
    "        \n",
    "        print(\"And the output is... \", rcnn_cls.shape, rcnn_bbox.shape) #no batch \n",
    "        list_cls = []\n",
    "        list_box = []\n",
    "        num_start = 0\n",
    "        \n",
    "        if padding > 0:\n",
    "            proposals_new = []\n",
    "            anchors_new = []\n",
    "            for (p,s) in zip(proposals, selected_anchors):\n",
    "                p_n = p -  torch.Tensor([[-padding],[-padding],[padding],[padding]]).repeat_interleave(p.shape[0],1).transpose(1,0)\n",
    "                s_n = s -  torch.Tensor([[-padding],[-padding],[padding],[padding]]).repeat_interleave(s.shape[0],1).transpose(1,0)\n",
    "                proposals_new.append(p_n)\n",
    "                anchors_new.append(s_n)\n",
    "            proposals = proposals_new\n",
    "            selected_anchors = anchors_new\n",
    "\n",
    "        for i in range(features.shape[0]):\n",
    "            num_proposals = selected_anchors[i].shape[0] # use order to map them back to the images \n",
    "            num_end = num_start + num_proposals\n",
    "            list_cls.append(rcnn_cls[num_start:num_end,:])\n",
    "            list_box.append(rcnn_bbox[num_start:num_end,:])\n",
    "            num_start = num_end\n",
    "           \n",
    "        def select_final(proposal, rcnn_cls, rcnn_bbox, thresh=0.2, padding=5):\n",
    "            rcnn_cls_soft = torch.softmax(rcnn_cls,1)[:,1]\n",
    "            \n",
    "            # We select indices for which probability is above our threshold\n",
    "            indices_choice = (rcnn_cls_soft>thresh).view(-1).squeeze().type(torch.BoolTensor)\n",
    "            temp_proposals = proposal[indices_choice,:]\n",
    "            temp_delta = rcnn_bbox[indices_choice,:]\n",
    "            temp_cls = rcnn_cls[indices_choice,:]\n",
    "            temp_bboxes = delta2bbox(temp_proposals, temp_delta)\n",
    "            width = temp_bboxes[:,2]-temp_bboxes[:,0]+1\n",
    "            height = temp_bboxes[:,3] - temp_bboxes[:,1]+1\n",
    "            \n",
    "            # We remove too big boxes\n",
    "            width_ok = width < img.shape[2]/2\n",
    "            height_ok = height < img.shape[3]/2\n",
    "            temp_ok = width_ok * height_ok\n",
    "            temp_bboxes = temp_bboxes[temp_ok,:]\n",
    "            temp_cls = temp_cls[temp_ok,:]\n",
    "            temp_score = torch.softmax(temp_cls,1)[:,1]\n",
    "            \n",
    "            # We perform a last NMS over the selected boxes\n",
    "            valid_idx = torchvision.ops.nms(temp_bboxes,temp_score,0.4)\n",
    "            \n",
    "            final_bboxes = temp_bboxes[valid_idx,:]\n",
    "            # print(proposal.shape[0],temp_bboxes.shape, final_bboxes.shape)\n",
    "            final_score = temp_score[valid_idx]\n",
    "            final_proposals = temp_proposals[valid_idx,:]\n",
    "            return final_score, final_bboxes, final_proposals\n",
    "        (final_score, final_bboxes, final_proposals) = multi_apply(select_final, \n",
    "                                                                   selected_anchors, \n",
    "                                                                   list_cls, \n",
    "                                                                   list_box, \n",
    "                                                                   (thresh,)*len(selected_anchors))\n",
    "\n",
    "        return final_score, final_bboxes, final_proposals, proposals, backbone_out, scores_anchors, selected_anchors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29Ssd0HfwrR3"
   },
   "outputs": [],
   "source": [
    "def find_containing_boxes(proposals):\n",
    "    # print(proposals)\n",
    "    overlaps = bbox_overlaps(proposals, proposals, 'iof')\n",
    "    # print(overlaps)\n",
    "    fraction_bin = overlaps > 0.9\n",
    "    overlapped_sum = fraction_bin.sum(axis=1)\n",
    "    # print(overlapped_sum)\n",
    "    remaining = overlapped_sum < 1.5\n",
    "    #print(proposals.shape[0], remaining.sum())\n",
    "    return proposals[remaining,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3__La-NwQHoX"
   },
   "source": [
    "## **Trying things out**\n",
    "Complete in the following the definition of the model and the hyper-parameters and see what can happen at inference given the pretrained parts that you have downloaded before.\n",
    "Try changing some of the parameters to see what happens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8650,
     "status": "ok",
     "timestamp": 1614684961524,
     "user": {
      "displayName": "Virginia Fernandez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2-wSJVduwlL_y-ZWh72X1gSpTnf056ew0V1dc=s64",
      "userId": "03836377914834337303"
     },
     "user_tz": 0
    },
    "id": "uIx9SJMhVZ3j",
    "outputId": "ee596862-6c69-458e-d1b2-e2902a9cee4b"
   },
   "outputs": [],
   "source": [
    "def plot_scores_anchors(scores, img, num_anchors=9):\n",
    "    fig2 = plt.figure(figsize=(20,10))\n",
    "    spec2 = GridSpec(scores.shape[0], num_anchors+1, figure=fig2)\n",
    "    scores = scores.permute(0, 2, 3, 1)\n",
    "    #print(scores.shape, img.shape)\n",
    "    scores= scores.reshape([scores.shape[0], img.shape[2], img.shape[3], num_anchors, 2])\n",
    "    \n",
    "    scores = torch.softmax(scores,dim=-1)[...,1]\n",
    "    scores = scores.detach().numpy()\n",
    "    for f in range(img.shape[0]):\n",
    "        aximg = fig2.add_subplot(spec2[round(f), 0])\n",
    "        aximg.imshow(np.squeeze(img[f]).T)\n",
    "        for a in range(num_anchors):\n",
    "            axanch = fig2.add_subplot(spec2[round(f), a+1])\n",
    "            axanch.imshow(scores[f,:,:,a].T)\n",
    "    plt.show()\n",
    "\n",
    "pretrained_dict = {'backbone': './models/Backbone.pt', \n",
    "                   'rpn_head': './models/RPNHead.pt',\n",
    "                   './models/rcnn_head': 'RCNNHead_bis.pt'}\n",
    "    \n",
    "\n",
    "model_full = TwoStageDetector(ADABN(1, 1), \n",
    "                              rpn_head = RPNHead(num_classes = 2, num_feat = 126, num_channels = 64, num_anchors = 9),\n",
    "                              rcnn_head = BBoxHead(num_classes = 2, num_channels = 64 * 7 * 7, num_feat = 88),\n",
    "                              pretrained = pretrained_dict\n",
    "                              )\n",
    "model_full.eval()\n",
    "patch_s = iter(training_loader).next()\n",
    "batch_images = patch_s['flair']['data'][..., 0]\n",
    "batch_labels = patch_s['label']['data'][..., 0]\n",
    "list_gt, gt_bboxes = create_gt_from_labels(batch_labels)\n",
    "                # x, bbox_rpn, seg_backbone, scores, final_nomod = model_full.inference_rpn(batch_images)\n",
    "                # print(len(bbox_rpn))\n",
    "                # plot_scores_anchors(scores, batch_images, num_anchors=9)\n",
    "                \n",
    "final_scores, final_bboxes, final_proposals, proposals, \\\n",
    "backbone_out, scores_anchors, selected_anchors = inference_rcnn(model_full, batch_images, thresh=0.5, padding=5)\n",
    "\n",
    "boxes_toplot = [find_containing_boxes(f.detach().round()).numpy() for f in final_bboxes]\n",
    "proposals_toplot = [find_containing_boxes(f.detach().round()).numpy() for f in final_proposals]\n",
    "scores_tocolor = [s.detach().numpy() for s in final_scores]\n",
    "# print(selected_anchors[2])\n",
    "# print(boxes_toplot[2])\n",
    "plot_boxes_image(batch_images, batch_labels, selected_anchors, backbone_out)\n",
    "plot_boxes_image(batch_images, batch_labels, proposals_toplot)\n",
    "plot_boxes_image(batch_images, batch_labels, boxes_toplot, scores=scores_tocolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15OD967zQlPP"
   },
   "source": [
    "\n",
    "## **Separate training**\n",
    "The results can certainly be improved - Try to train the part that you think is in most need of improving by completing and running an amended version of the following loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSfTBUSbzaAH"
   },
   "outputs": [],
   "source": [
    "# Set the number of epochs here:\n",
    "number_of_epochs = 60\n",
    "\n",
    "# Here is a set of possible optimisers\n",
    "optimizer_backbone = optim.Adam(model_full.backbone.parameters(), lr=1e-4)\n",
    "optimizer_rpn = optim.Adam(model_full.rpn_head.parameters(), lr=1e-4)\n",
    "optimizer_rcnn = optim.Adam(model_full.rcnn_head.parameters(), lr=1e-4)\n",
    "optimizer_mix = optim.Adam(list(model_full.rcnn_head.parameters())+list(model_full.rpn_head.parameters()), lr=1e-4)\n",
    "optimizer_full = optim.Adam(model_full.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    model_full.backbone.eval()\n",
    "    model_full.rpn_head.eval()\n",
    "    model_full.rcnn_head.train()\n",
    "\n",
    "    if epoch % 10 == 1:\n",
    "        name = './models/RCNN_OD_%d.pt' % epoch\n",
    "        torch.save(model_full.state_dict(),name)\n",
    "        with tqdm(total=len(training_loader), file=sys.stdout) as pbar:\n",
    "            start_time = time.time()\n",
    "            running_loss = 0\n",
    "            indb = 0\n",
    "            for patch_s in training_loader:\n",
    "                # Get a batch of source slices\n",
    "                # Get a batch of target slices\n",
    "                batch_images = patch_s['flair']['data'][..., 0]\n",
    "                batch_labels = patch_s['label']['data'][..., 0]\n",
    "                list_gt, gt_bboxes = create_gt_from_labels(batch_labels)\n",
    "                losses = model_full.forward_train(Variable(batch_images, requires_grad=True),\n",
    "                                                  gt_bboxes,\n",
    "                                                  label_seg=batch_labels,\n",
    "                                                  training_stage=['rcnn_head'])\n",
    "                \n",
    "\n",
    "                total_loss = losses['rpn_cls'] +  losses['rpn_bbox']\n",
    "\n",
    "                print(losses['rcnn_cls'], losses['rcnn_bbox'], total_loss)\n",
    "                # total_loss.requires_grad = True\n",
    "                model_full.rpn_head.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer_rpn.step()\n",
    "                # print(model_full.rpn_head.rpn_conv.weight[0])\n",
    "                running_loss += total_loss.item()\n",
    "                indb += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "        end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsdC15cMRcCT"
   },
   "source": [
    "## **Going further**\n",
    "\n",
    "Modify the model to transform it into a Mask RCNN "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Final_Solution_ODetection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
