{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Week-5.Interpretability_part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0q1MX0J_3jF"
      },
      "source": [
        "# Week 5: Interpretability 2: Feature visualisation and TSNE\n",
        "\n",
        "Tutorial by Cher Bass and Emma Robinson \n",
        "\n",
        "In this second tutorial session on interpretability we will look at feature visualisation approaches, specifically layer visualisation through gradient ascent. Then we will look at using these for style transfer using Deep Dream and finally we will look at interpretation of network latent space embedddings using T-SNE\n",
        "\n",
        "First let's mount our Drive and import the libraries we will need. As for part 1, we create examples based on code from the [visualizations repository](https://github.com/utkuozbulak/pytorch-cnn-visualizations). \n",
        "\n",
        "**Note** All the visualizations will be saved to `/generated` folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZCm5dknBIg3"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STUDENTS UPLOAD the Notebooks folder to your drive and specify the path to where you have placed the visualisations package folder\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/AdvancedML/2021/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN1VScAW_3jM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import visualizations\n",
        "from visualizations.src.misc_functions import *\n",
        "from visualizations.src.deep_dream import DeepDream\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkk-SB7h_3jq"
      },
      "source": [
        "## Visualisation\n",
        "\n",
        "The web journal [Distill.pub](https://distill.pub/) is a particularly strong source of information for those interested in network visualisation. In particular [this article](https://distill.pub/2017/feature-visualization/) by Chris Olah has been a strong source of information for this section.\n",
        "\n",
        "The idea of feature visualisation is that we perform backpropagation, similarly to how it is performed when optimising the network. However, for visualisation, we keep the weights ($\\mathbf{w}$) constant and instead optimise an activation $f(\\mathbf{w,x})$ with respect to  input image $\\mathbf{x}$. \n",
        "\n",
        "$$ \\mathbf{x^*} = \\max_{x s.t. || \\mathbf{x}|| = \\rho} f(\\mathbf{w,x}) $$\n",
        "\n",
        "Such that: $\\mathbf{x_{t+1}}= \\mathbf{x_{t}}- \\gamma \\frac{\\delta f(\\mathbf{w,x}}{\\delta \\mathbf{x}}$\n",
        "\n",
        "### **Exercise 1. CNN Layer Visualization** \n",
        "\n",
        "Here we will use the layer visualisation approach of ([Erhan et al 2009](https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network ))\n",
        "\n",
        "The first thing we need to do is load a pre-trained VGG16 model:\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMCPQqBk_3jr"
      },
      "source": [
        "# load vgg model, and extract layers from the features modules only\n",
        "pretrained_model = models.vgg16(pretrained=True).features\n",
        "print(pretrained_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W2nAo3w_3jr"
      },
      "source": [
        "#### **Exercise 1.1 Implementing layer visualisation**\n",
        "\n",
        "Implement the gradient update step for layer visualisation. Here you are updating the image for a specific channel of a specific layer. Thus the function has to make a stepwise forward pass through the network until it reaches the `selected_layer`. The output of these forward pass is then the target activation block. This must then be sliced to return the `selected_channel` to optimise against.\n",
        "\n",
        "**To do**\n",
        "\n",
        "1. Slice the correct channel from the activation returned from `selected_layer` (line 33). The layers activations are returned by using a for loop to perform a forward step by step until the correct layer is reached (lines 27-31)\n",
        "2. Suggest a suitable loss function (line 35). Don't forget, the goal here is to maximise activation strength across the whole channel.\n",
        "\n",
        "**Note** how, in the above function, the optimizer (line 20) is optimising against the `[processed_image]` rather than the network parameters. After tha the training is implemented as for standard network training (with `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` implemented exactly as seen before)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rnDbKbE_3js"
      },
      "source": [
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "def visualise_layer(selected_layer,selected_channel,model,processed_image,lr=0.1,iters=50,l2_reg=1e-4,regularise=False):\n",
        "        \n",
        "        if regularise:\n",
        "          # STUDENT CODE ex 1.3 implement optimizer with weight decay\n",
        "          optimizer = None\n",
        "        else:\n",
        "          # Define optimizer for the image\n",
        "          optimizer = torch.optim.Adam([processed_image], lr=lr)\n",
        "\n",
        "        x = processed_image\n",
        "        for i in range(1, iters):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through network one layer at a time \n",
        "            for index, layer in enumerate(model):         \n",
        "                x = layer(x) #forward pass though current layer\n",
        "                #stop once target layer is reacher\n",
        "                if index == selected_layer:\n",
        "                  break\n",
        "            # STUDENT CODE 1.1 - slice a filter from the layer to get a 2D output\n",
        "            conv_output = None\n",
        "            # STUDENT CODE 1.1 - Implement Loss function - we need to maximise activations across the layer\n",
        "            loss = None\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "                  \n",
        "            # Update image\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Assign processed image to a variable to move forward in the model\n",
        "            x = processed_image\n",
        "            \n",
        "        \n",
        "            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))\n",
        "            if i % 10 == 0:\n",
        "                created_image = recreate_image(processed_image)  \n",
        "                im_path = './generated/ddream_l' + str(selected_layer) + \\\n",
        "                    '_f' + str(selected_channel) + '_iter' + str(i) + '.jpg'\n",
        "                plt.imshow(created_image)\n",
        "                plt.show()\n",
        "        return processed_image\n",
        "            \n",
        "                \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfMzxj7__3js"
      },
      "source": [
        "#### **Exercise 1.2  Train layer visualisation**\n",
        "\n",
        "**To Do** \n",
        "- train with different layers and filters\n",
        "- consider changing the learning rate of number of iterations\n",
        "\n",
        "In this case our starting point is an `image` of random noise. Note, the function expects the image as a numpy integer array. `visualizations.src.misc_functions.preprocess_image` resizes and normalises the image to match the form expected by the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFxfGwOa_3js"
      },
      "source": [
        "cnn_layer =28\n",
        "cnn_filter=2\n",
        "\n",
        "random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n",
        "# Process image and return variable\n",
        "processed_image = preprocess_image(random_image, False)\n",
        "       \n",
        "layer_vis = visualise_layer(cnn_layer,cnn_filter,pretrained_model,processed_image,iters=100, lr=0.5)\n",
        "\n",
        "# plot image\n",
        "\n",
        "# Recreate image - removes intensity normalisation\n",
        "created_image = recreate_image(layer_vis)  \n",
        "plt.imshow(created_image)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sToh0M_9_3js"
      },
      "source": [
        "#### **Exercise 1.3 (optional) regularisation**\n",
        "\n",
        "1. L2 regularisation can be implemented in PyTorch using the `weight_decay` argument of the optimiser. Try adding different levels of L2 regularisation this way\n",
        "2. Consider also implementing activation/gradient clipping or Gaussian blurring?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TonSL8vgaYhQ"
      },
      "source": [
        "# STUDENTS CODE - TRY EDITING THE LAYER VISUALISATION FN (ABOVE) TO IMPLEMENT REGULARISATION\n",
        "\n",
        "# THEN IMPLEMENT FOR THE EXAMPLE AS ABOVE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajvn87B_3js"
      },
      "source": [
        "## **Exercise 2: Deep Dream**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML5VScgm_3jt"
      },
      "source": [
        "In deep dream, rather than optimising for a random image we instead pass a real image. Let's visualise the output from a later layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjzfYKp2_3jt"
      },
      "source": [
        "# THIS OPERATION IS MEMORY HUNGRY! #\n",
        "# Because of the selected image is very large\n",
        "# If it gives out of memory error or locks the computer\n",
        "# Try it with a smaller image\n",
        "cnn_layer = 28\n",
        "filter_pos = 94\n",
        "\n",
        "im_path = './visualizations/input_images/dd_tree.jpg'\n",
        "\n",
        "image=Image.open(im_path).convert('RGB')\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "processed_image = preprocess_image(image, True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTzWGkuZ_3jt"
      },
      "source": [
        "layer_vis = visualise_layer(cnn_layer,filter_pos,pretrained_model,processed_image,lr=0.1,iters=100, regularise=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HlEgRoG_3jt"
      },
      "source": [
        "**To do** Try changing the learning rate or levels of regularisation.\n",
        "\n",
        "Now you can try uploading a photo of yourself and giving it the DeepDream treatment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwFZupKGigZu"
      },
      "source": [
        "## **Exercise 3 T-SNE**\n",
        "\n",
        "Experiment with t-sne using the [scikit-learn implementation](xhttps://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). In its most basic form this can be done in one line\n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1KqHdu9VY9O99QVpMF0JqKWrMLa4QoXiq\"\" alt=\"Drawing\" width=\"600px;\"/>\n",
        "</figure>\n",
        "\n",
        "\n",
        "For this exercise we will again use the data from notebook `1.1-fundamentals-solutions.ipynb` (\"prem_vs_termwrois.pkl\" - available from week 1 section of keats). This represents mean vales of three different types of cortical imaging data: cortical thickness, cortical folding and cortical myelination, all averaged within 100 regions of interest ROIS on the surface (300 features in total). There are 101 babies, 50 terms and 51 preterms. \n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1ZbAn0R_ihQ4DCe1XyKaHIRZSvUQv3puh\" alt=\"Drawing\" width=\"900px;\"/>\n",
        "</figure>\n",
        "\n",
        "**To do**\n",
        "\n",
        "1. implement t-sne using scikit learn. Set `n_components=2`; fit the embedding for the dHCP data\n",
        "2. experiment with changing the perplexity n the range 5 to 50\n",
        "3. experiment with changing the metric to other options available through [`scipy.spatial.distance.pdista](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html) to e.g. correlation\n",
        "4. In each instance plot the embedding with the points color coded by label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q6eATzvijjD"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import manifold\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import copy\n",
        "\n",
        "# STUDENTS CODE HERE - UPDATE THE PATH TO CORRESPOND TO WHERE YOU HAVE UPLOADED prem_vs_termwrois.pkl TO YOUR DRIVE #\n",
        "file_path='/content/drive/My Drive/Colab Notebooks/AdvancedML/2021/01_fundamentals/prem_vs_termwrois.pkl'\n",
        "# Read the data\n",
        "df = pd.read_pickle(file_path)\n",
        "data = df.values[:,:-2]\n",
        "y = df.values[:,-1]\n",
        "\n",
        "# STUDENTS CODE HERE -IMPLEMENT T-SNE FOR THIS DATASET  #\n",
        "# vary parameters (e.g. perplexity) and see effect on embeddding\n",
        "\n",
        "# plot result with different colours for each of the (premature and term baby labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipjPBODnDPl_"
      },
      "source": [
        "# Source references\n",
        "\n",
        "1. [visualizing-convolution-neural-networks-using-pytorch](https://towardsdatascience.com/visualizing-convolution-neural-networks-using-pytorch-3dfa8443e74e)\n",
        "2. [DeepLearning-PadhAI](https://colab.research.google.com/github/Niranjankumar-c/DeepLearning-PadhAI/blob/master/DeepLearning_Materials/6_VisualizationCNN_Pytorch/CNNVisualisation.ipynb#scrollTo=uQI9jHcP6xfP)"
      ]
    }
  ]
}