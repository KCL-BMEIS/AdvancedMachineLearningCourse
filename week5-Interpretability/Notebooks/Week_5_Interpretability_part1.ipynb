{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Week-5.Interpretability_part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0q1MX0J_3jF"
      },
      "source": [
        "# Week 5: Interpretability 1: filters, activations and saliency mapping\n",
        "\n",
        "Tutorial by Cher Bass and Emma Robinson \n",
        "\n",
        "This week we will go through several popular visualization techniques that help interpret deep learning networks.\n",
        "\n",
        "We will cover:\n",
        "1. Filter visualization\n",
        "2. Feature/ activation visualization with PyTorch hooks\n",
        "3. CNN Layer Visualization\n",
        "4. Gradient visualization with Guided backpropagation\n",
        "5. Gradient Class Activation Maps (grad-CAM)\n",
        "\n",
        "In this first notebook we will look at simple network independent solutions for feature and activation visualisation, as well as occlusion-based saliency techniques\n",
        "\n",
        "## Importing\n",
        "\n",
        "The first thing we need to do is import a package called `visualizations`. This was uploaded to Keats the the Notebooks folder. Please upload that to your drive and then specify the path to where you put in the cell below. This will allow you to import it into this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZCm5dknBIg3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STUDENTS UPLOAD the Notebooks folder to your drive and specify the path to where you have placed the visualisations package folder\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/AdvancedML/2021/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4cbbYYZC2pZ"
      },
      "source": [
        "Now import the modules you need from torch and the visualization package by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN1VScAW_3jM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import visualizations\n",
        "from visualizations.src.misc_functions import *\n",
        "from visualizations.src.guided_backprop import GuidedBackprop\n",
        "#from visualizations.src.cnn_layer_visualization import CNNLayerVisualization\n",
        "from visualizations.src.gradcam import GradCam\n",
        "from visualizations.src.deep_dream import DeepDream\n",
        "%pylab inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JODrRIKp_3jN"
      },
      "source": [
        "## Exercise 1. Weights Visualization\n",
        "\n",
        "One of the first things you can visualize in your network is your networks weight tensors. These weights kernels reflect the lernt convolutional kernesl which are optimised during training> They can be visualized by calling the weight data inside your network.\n",
        "\n",
        "Let's first load and print a pretrained network using pytorch.models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "v8WRwFDW_3jN"
      },
      "source": [
        "# first load pretrained alxenet model\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "print(alexnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5dLZB6S_3jO"
      },
      "source": [
        "We can see from the print output (above) that the network is made up of two sequential containers: 1) `features` (the convolutional layers) and 2) `classifier` (the linear layers).\n",
        "\n",
        "### **Example:** \n",
        "\n",
        "To visualize the convolutional weights for some layer of a specific seqential container (e.g. `container`) of a network (e.g. `net`) : we must use the following notation:\n",
        "\n",
        "```python \n",
        "# to return layer\n",
        "layer=net.container[layer_num]\n",
        " # to return weights tensor of a layer\n",
        "weight_tensor = net.container[layer_num].weight.data\n",
        "```\n",
        "\n",
        "**Note that layer id `layer_num` should correspond to a *convolutional* layer (e.g. 0, 3, 6, 10) otherwise there are no weights to be visualized.**\n",
        "\n",
        "We will now define a few functions to help with plotting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o31HQ8fY_3jP"
      },
      "source": [
        "def plot_filters_single_channel(t,channels_to_plot,kernels_to_plot):\n",
        "    \n",
        "\n",
        "    print('printing {} kernels for each of the first {} filters '.format(kernels_to_plot,channels_to_plot))\n",
        "    #total kernels depth * number of kernels\n",
        "    nplots = channels_to_plot*kernels_to_plot\n",
        "    ncols = 12\n",
        "    \n",
        "    nrows = 1 + nplots//ncols\n",
        "    #convert tensor to numpy image\n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    \n",
        "    count = 0\n",
        "    fig = plt.figure(figsize=(ncols, nrows))\n",
        "    \n",
        "    #looping through all the kernels in each channel\n",
        "    for i in range(kernels_to_plot):\n",
        "        for j in range(channels_to_plot):\n",
        "            count += 1\n",
        "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
        "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
        "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "            ax1.imshow(npimg)\n",
        "            ax1.set_title(str(i) + ',' + str(j))\n",
        "            ax1.axis('off')\n",
        "            ax1.set_xticklabels([])\n",
        "            ax1.set_yticklabels([])\n",
        "   \n",
        "    plt.tight_layout()\n",
        "    plt.show()    \n",
        "\n",
        "    \n",
        "def plot_filters_multi_channel(t,kernels_to_plot=60):\n",
        "    \n",
        "    print('printing the first {} 3D filters in RGB '.format(kernels_to_plot))\n",
        "    #get the number of kernals\n",
        "    num_kernels = t.shape[0]    \n",
        "    \n",
        "    #define number of columns for subplots\n",
        "    num_cols = 12\n",
        "    #rows = num of kernels\n",
        "    num_rows = kernels_to_plot // num_cols+1\n",
        "    \n",
        "    #set the figure size\n",
        "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    \n",
        "    #looping through all the kernels\n",
        "    for i in range(kernels_to_plot):\n",
        "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        \n",
        "        #for each kernel, we convert the tensor to numpy \n",
        "        npimg = np.array(t[i].numpy(), np.float32)\n",
        "        #standardize the numpy image\n",
        "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "        npimg = npimg.transpose((1, 2, 0))\n",
        "        ax1.imshow(npimg)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(str(i))\n",
        "        ax1.set_xticklabels([])\n",
        "        ax1.set_yticklabels([])\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6KLRa1x_3jR"
      },
      "source": [
        "We next define our plot weights function, which first extracts the weights of a convolutional filter, and then passes into an appropriate image plotting function.\n",
        "\n",
        "**Ex 1.1** using the generic example given above,  edit the function `plot_weights` function to return the weights tensor for a given convolutional layere\n",
        "\n",
        "1. Select the convolutional `layer` from alexnets `features` container corresponding to `layer_num`\n",
        "2. Return the `weight_tensor` for this layer\n",
        "3. run this function to return features from  `layer_num`=0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QixBmo6u_3jR"
      },
      "source": [
        "def plot_weights(container, layer_num,num_filter=10,num_kernels=30):\n",
        "    '''\n",
        "    plot_weights: a function to plot weights tensors from a network\n",
        "                  this will print in RGB if filter depth is 3 (i.e. first layer)\n",
        "                  else will print each kernel separately\n",
        "    inputs:\n",
        "          container: a sequnetial container for which the convolutional layers of the network are defined\n",
        "          layer_num: choice of layer to visualise\n",
        "\n",
        "    '''\n",
        "    #STUDENTS CODE - REPLACE NONES in FUNCTION BELOW\n",
        "    #Task 1.1.1 use variable layer_num to select a specific \n",
        "    layer = None\n",
        "  \n",
        "    #checking whether the layer is convolution layer or not \n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        # Task 1.1.2 return the weights tensor for this 'layer`\n",
        "        weight_tensor = None\n",
        "        print('This layers learns {} filters each with {} kernels'.format(weight_tensor.shape[0],weight_tensor.shape[1]))\n",
        "        # if the weights tensor has 3 channels the it will plot filters in RGB\n",
        "        if weight_tensor.shape[1]==3:\n",
        "             # labelling each by filer id            \n",
        "             plot_filters_multi_channel(weight_tensor,num_filter)    \n",
        "        else:\n",
        "            # else it will print each filter kernel separately\n",
        "            # labelling each by filer.kernel (the filter id and the kernel id)\n",
        "            plot_filters_single_channel(weight_tensor,num_filter,num_kernels)\n",
        "        \n",
        "    else:\n",
        "        print(\"Can only visualize layers which are convolutional\")\n",
        "\n",
        "\n",
        "#Task 1.1  call the function to visualise layer 0 (in the first instance)\n",
        "# to visualize weights for alexnets - first conv layer\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TZqI8VA_3jR"
      },
      "source": [
        "**Ex 1.2** Explain \n",
        "\n",
        "1. What do the weights of your network represent? \n",
        "2. What is the difference between a kernel and a filter (**hint** see lecture 4)?\n",
        "3. Layer 3 (the second convolution) has a weights matrix of size `[192, 64, 5, 5]`. What does each dimension represent? How many filters does it learn, how many kernels?\n",
        "4. Try plotting the weights for different convolutional layers (changing code cell below). \n",
        "   - Try changing the numbers of filters and/or kernels printed per layer\n",
        "   - What if anything can you interpret from each of these layer?\n",
        "   - Do you think this approach is useful for intrepretation of deep networks? Why do you think that?\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CTGTr35Nedk"
      },
      "source": [
        "\n",
        "#### **Answer Questions in this cell**\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n",
        "\n",
        "4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UY0XEN3Q_3jS"
      },
      "source": [
        "#STUDENTS CODE - REPLACE NONES in FUNCTION BELOW\n",
        "# Task 1.2.4 visualize weights for different convolutional layers\n",
        "# what can you interpret from this \n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJVtIff4_3jT"
      },
      "source": [
        "## Exercise 2. Activation visualization with PyTorch Hooks\n",
        "\n",
        "An alternative approach to examine what your network is learning, is to visualise your network's features or activations, i.e. the 'images' which are created following passing your inputs through a given convolutional layer (with relu)\n",
        "\n",
        "Let's use MNIST for this example. Loading train and validation DataLoaders, and generating a similar basic convolutional network to which we used in Lecture 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu-GLnPo_3jT"
      },
      "source": [
        "mnist_train_dataset = datasets.MNIST(root = 'mnist_data/train', download= True, train = True, transform = transforms.ToTensor())\n",
        "mnist_test_dataset = datasets.MNIST(root = 'mnist_data/test', download= True, train = False, transform = transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "       mnist_train_dataset, batch_size= 8, shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "       mnist_test_dataset, batch_size = 8, shuffle = True)\n",
        "\n",
        "classes = ('0', '1', '2', '3',\n",
        "          '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "id": "5e1utuc5_3jT"
      },
      "source": [
        "class MNIST_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Model, self).__init__()\n",
        "    \n",
        "        self.conv1=nn.Conv2d(1, 10, 3)\n",
        "        self.maxpool1=nn.MaxPool2d(2)\n",
        "        self.dropout1=nn.Dropout2d()\n",
        "        \n",
        "        self.conv2=nn.Conv2d(10, 20, 3)\n",
        "        self.maxpool2=nn.MaxPool2d(2)\n",
        "        self.dropout2=nn.Dropout2d()\n",
        "        \n",
        "        self.lin_blocks = nn.Sequential(\n",
        "            nn.Linear(500, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 10),\n",
        "            #nn.Softmax(),\n",
        "        )\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = x.view(x.size(0),-1)\n",
        "        #print(x.shape)\n",
        "        x = self.lin_blocks(x)\n",
        "\n",
        "        return F.log_softmax(x,dim=1)\n",
        "\n",
        "\n",
        "net = MNIST_Model() \n",
        "print(net)\n",
        "net = net.to(device)\n",
        "\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "loss_fun = loss_fun.to(device)\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUy178NQ_3jU"
      },
      "source": [
        "epochs = 1\n",
        "for epoch in range(epochs): \n",
        "\n",
        "    # enumerate can be used to output iteration index i, as well as the data \n",
        "    for i, (data, labels) in enumerate(train_loader, 0):\n",
        "        \n",
        "        # load data and labels to device\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # clear the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #feed the input and acquire the output from network\n",
        "        outputs = net(data)\n",
        "\n",
        "        # calculating the predicted and the expected loss\n",
        "        loss = loss_fun(outputs, labels)\n",
        "\n",
        "        #compute the gradient\n",
        "        loss.backward()\n",
        "\n",
        "        #update the parameters\n",
        "        optimizer.step()\n",
        "        # \n",
        "\n",
        "        # print statistics\n",
        "        ce_loss = loss.item()\n",
        "        if i % 10 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch + 1, i + 1, ce_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6uMUTqi_3jU"
      },
      "source": [
        "### Understanding PyTorch Hooks\n",
        "\n",
        "Hooks are functions with which you can modify or return different sub-components of your network. Returning the outputs of different layers (i.e. the activations - the goal of this section) is thus an excellent example. In general the point is that they are simply functions that are run whenever the `forward` or `backward` function of a `torch.Autograd.Function` object is called i.e. the `grad_fn` of a tensor (discussed in Lecture 2). \n",
        "\n",
        "You can register a function on a `Module` or a `Tensor` and are defined a priori as forward hooks or a backward hooks. Depending on which they are they will either be executed when a forward call is executed (forward hook) or a backward pass in run (backward hook). \n",
        "\n",
        "Let's look at using a forward and backward hook just for debugging (and thus printing) the output of a function (example taken from [the official PyTorch tutorials](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html))\n",
        "\n",
        "A forward hook has the general form:\n",
        "\n",
        "```python\n",
        "model.conv_name.register_forward_hook(forward_hook_function)\n",
        "```\n",
        "\n",
        "With `forward_hook_function` being a user defined function of form:\n",
        "\n",
        "```\n",
        "def forward_hook_function(module, input,output):\n",
        "```\n",
        "\n",
        "And a backward hook has a general form\n",
        "\n",
        "```python\n",
        "model.conv_name.register_backward_hook(backward_hook_function)\n",
        "```\n",
        "With `backward_hook_function` being a user defined function of form:\n",
        "\n",
        "```\n",
        "def backward_hook_function(module,grad_input,grad_output):\n",
        "```\n",
        "\n",
        "Say we are interested in layer $l$. A forward hook function can look at inputs and outputs the layer during the forward pass i.e. the input activation from layer $l-1$ and the output activation for layer $l$. The backward pass can look at the inputs and outputs to the backward pass i.e. the incoming (input) gradient with respect to the parameters from the layer above $l+1$ and the outgoing (output) gradient with respect to the parameters of the current layer $l$.\n",
        "\n",
        "Thus if we wish to print information about the input and output activations we can define a forward hook function to do this as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baw3as-J_3jU"
      },
      "source": [
        "def printnorm(self, input, output):\n",
        "    # input is a tuple of packed inputs\n",
        "    # output is a Tensor. output.data is the Tensor we are interested\n",
        "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
        "    print('')\n",
        "    print('input type: ', type(input))\n",
        "    print('output type: ', type(output))\n",
        "    print('')\n",
        "    print('input size:', input[0].size())\n",
        "    print('output size:', output.data.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6WeiOszaeGj"
      },
      "source": [
        "We then register the forward hook as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQBNQu5-aeiL"
      },
      "source": [
        "\n",
        "print_forward_handle=net.conv1.register_forward_hook(printnorm)\n",
        "\n",
        "#--------------------- ---- ----------------------------#\n",
        "# performing forward pass\n",
        "out = net(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJlcwaZW_3jV"
      },
      "source": [
        "Note if you run this more than once it will print as many times as you run the cell (registering a new hook every time).\n",
        "\n",
        "You may therefore want to consider removing as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTNxX50GarjW"
      },
      "source": [
        "print_forward_handle.remove()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKk3U9wcarYf"
      },
      "source": [
        " On the other hand and example of a backward hook function to return information about the layers gradients might be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS_ureLO_3jV"
      },
      "source": [
        "def printgradnorm(self, grad_input, grad_output):\n",
        "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
        "    print('Inside class:' + self.__class__.__name__)\n",
        "    print('')\n",
        "    print('grad_input type: ', type(grad_input))\n",
        "    print('grad_output type: ', type(grad_output))\n",
        "    print('')\n",
        "    print('grad_input[0] size:', grad_input[0].size())\n",
        "    print('grad_output[0] size:', grad_output[0].size())\n",
        "\n",
        "\n",
        "print_backward_handle=net.conv2.register_backward_hook(printgradnorm)\n",
        "\n",
        "# first need to run forward function and estimate loss\n",
        "out = net(data)\n",
        "err = loss_fun(out,labels)\n",
        "\n",
        "print('loss:', err)\n",
        "# then by running backward function the hook will be called\n",
        "err.backward()\n",
        "\n",
        "print_backward_handle.remove()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzGqf4MH_3jW"
      },
      "source": [
        "**Exercise 2.1. Register hook to get network activations for a given input**\n",
        "\n",
        "As hooks are functions that can be applied to the inputs and outputs of our forwards and backwards layers we can use them to return the activations of each layer.\n",
        "\n",
        "**To do 2.1.1** Use hooks to return the activations of different layers of your network.\n",
        "\n",
        "- Create a hook function to return the activations of the first convolutional layer.\n",
        "  - should it be a forward hook or a backwards hook?\n",
        "  - do you need to return the input or the outputs to the layer?\n",
        "\n",
        "Note, while it would be sufficient to create a stand alone hook function ([see also this tutorial](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) for an example), one other option may be to take a more generic approach: [saving the outputs of hooks to attributes of a bespoke class](https://www.kaggle.com/sironghuang/understanding-pytorch-hooks)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atOkjdDylCsw"
      },
      "source": [
        "#STUDENTS CODE -  WRITE A HOOK FUNCTION (OR CLASS) which will return the activations of a given convolutional layer\n",
        "# the important point here is to recognise that, unlike the print function above\n",
        "# you will need to define a way to store the activations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P4CbQd5lDu-"
      },
      "source": [
        "**Exercise  2.2 visualise acivations from hook** Use this class to return the activations of different layers of your network.\n",
        "1. Create object `activation` (as class,list,dictionary) which will store activation tensor for different convolutional layer\n",
        "2. Register a hook\n",
        "3. Make a forwards or backwards pass (depending on which type of hook you think is appropriate) \n",
        "4. Plot each activation for this layer; how many would you expect?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MryVJrL2f8TH"
      },
      "source": [
        "#--------------------- Task 2.2.1 ----------------------------#\n",
        "# 2.2.1 create object for storing activation returned from network pass\n",
        "activation=None\n",
        "#2.2.1  register hook on first convolutional layer (1-2 lines)\n",
        "None\n",
        "#--------------------- ---- ----------------------------#\n",
        "\n",
        "# take a single example from the dataset\n",
        "data, _ = mnist_train_dataset[0]\n",
        "data = data.to(device) # push to device\n",
        "data.unsqueeze_(0) # ignore batch axis\n",
        "\n",
        "# Task 2.2.2 Run forward pass on single example\n",
        "# this will apply the hook and to save the relevant activations\n",
        "output = None\n",
        "\n",
        "# Task 2.2.4 Plot all activations for this layer\n",
        "# storing the resulting activation tensor to the act variable\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4caebn3g_3jW"
      },
      "source": [
        "**Ex 2.2 (continued)** \n",
        "5. Try changing the convolutional layer\n",
        "6. How can you interpret the activations of different layers? What does that mean with respect to the relative importance of different parts of your image?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivz9ARlWqb6s"
      },
      "source": [
        "**Task 2.2.6 answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRMi6SZB_3jW"
      },
      "source": [
        "#--------------------- Task 2.2.1 ----------------------------#\n",
        "# Create a hook on the first convolutional layer\n",
        "activation.run_hook_for_layer(net.conv2,'conv2')\n",
        "#--------------------- ---- ----------------------------#\n",
        "\n",
        "# take a single example from the dataset\n",
        "data, _ = mnist_train_dataset[0]\n",
        "data = data.to(device) # push to device\n",
        "data.unsqueeze_(0) # ignore batch axis\n",
        "\n",
        "# Task 2.2.2 Run forward pass on single example\n",
        "# this will apply the hook and to save the relevant activations\n",
        "output = net(data)\n",
        "\n",
        "# task 2.2.3 store the resulting activation tensor to the act variable\n",
        "act = activation.features['conv2'].squeeze()\n",
        "\n",
        "# Task 2.2.4 how many activation maps should we expect?\n",
        "num_maps=10\n",
        "fig, axarr = plt.subplots(1,num_maps,figsize=(20,20))\n",
        "for idx in np.arange(num_maps):\n",
        "    axarr[idx].imshow(act.cpu()[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnOnQpB9_3jY"
      },
      "source": [
        "## Feature Attribution (saliency mapping)\n",
        "\n",
        "Saliency mapping techniques seek to generate heatmaps which highlight which parts of an image are important for activation.\n",
        "\n",
        "### Exercise 3: Saliency by occlusion\n",
        "\n",
        "The most simple approach to saliency mapping is to perform occlusion. This works by greying out (setting to 0.5) patches of pixels for an image to see what impact this has on classification. Let's return a batch of images from our validation DataLoader and visualise them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOtUHFHA_3jY"
      },
      "source": [
        "def plot_MNIST(images,labels):\n",
        "    rows = 2\n",
        "    columns = 4\n",
        "    classes = ('0', '1', '2', '3',\n",
        "          '4', '5', '6', '7', '8', '9')\n",
        "    # plot y_score - true label (t) vs predicted label (p)\n",
        "    fig2 = plt.figure()\n",
        "    for i in range(8):\n",
        "        fig2.add_subplot(rows, columns, i+1)\n",
        "        plt.title('t: ' + classes[labels[i].cpu()])\n",
        "        img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
        "        img = torchvision.transforms.ToPILImage()(img.cpu())\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "im_batch, lab_batch=next(iter(test_loader)) # view one batch\n",
        "im_batch = im_batch.to(device)\n",
        "plot_MNIST(im_batch,lab_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ6XsPeJ_3jZ"
      },
      "source": [
        "The first thing we need to do is run inference on the images without occlusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2znVVgO_3jZ"
      },
      "source": [
        "#running inference on the images without occlusion\n",
        "\n",
        "# pretrained model\n",
        "outputs = net(im_batch)\n",
        "\n",
        "# #passing the outputs through softmax to interpret them as probability\n",
        "outputs = nn.functional.softmax(outputs, dim = 1)\n",
        "\n",
        "#assigning the predicted label from the maximum softmax output\n",
        "prob_no_occ, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "#get the first item\n",
        "prob_no_occ = prob_no_occ[0].item()\n",
        "\n",
        "print('Predictions = ', pred,'max prob',prob_no_occ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7PNKBCe_3jZ"
      },
      "source": [
        "Now, let's look at the effect of zeroing (or greying out) blocks of pixels in our image.\n",
        "\n",
        "Similar to the patch based selection that we saw in lecture 4, we are constrained to select only patches which fit in our image (which means that number of points on which we can start each patch is constrained by the width and height of our patch). This means that the output size will be smaller than the original image size by a factor of: (h-p)/s; where, h is the height (or width), p is the patch size and s is the chosen stride.\n",
        "\n",
        "Let's start create an image with an occluded patch in the centre. \n",
        "\n",
        "#### **To do 3.1 Create a function which occludes a patch from an image**\n",
        "\n",
        "1. set all pixels of patch (starting at height: `heigh_start` and width: `width_start`, and of size: `patch_size` to 0.5\n",
        "  - **note** if the patch_size exceeds the dimensions on any axis - just set the available space to zero (otherwise the results of Ex 3.2 will have shape less thana the original image). This is equivalent to padding the operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CXF-qxT_3jZ"
      },
      "source": [
        "#STUDENTS CODE -  Write a function which occludes a patch from an image\n",
        "\n",
        "def occlude_image(image, model, height_centre,width_centre,patch_size,label):\n",
        "    ''' \n",
        "    Creates a copy of the image and occludes a patch \n",
        "    input:\n",
        "    image (Pytorch tensor): image to be occluded\n",
        "    model: Pytorch network model \n",
        "    height_centre=centre of patch on height dimension\n",
        "    width_centre= centre of patch on width dimension\n",
        "    patch_size: size of patch\n",
        "    \n",
        "    output: \n",
        "    probability\n",
        "    '''\n",
        "    #STUDENTS TO COMPLETE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27MO5Kk5uoyu"
      },
      "source": [
        "2. Pass the full image through the network and estimate the probability of the network predicting the correct label (return softmax output corresponding to the correct label)\n",
        "3. Now occlude the image and make another forward pass; how does occlusion impact the prediction accuracy?\n",
        "\n",
        "\n",
        "**Note** \n",
        "- forward pass expects an input tensor of shape $B\\times C\\times H\\times W$ where $B$ represents the batch size, $C$ the number of channels and $H$ and $W$ the height and width (**hint** perhaps use [unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html))\n",
        "-you will need to use `nn.functional.softmax()` on the output from your network. This will return a probability for each label (summing to one over all classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV8EWK25upB7"
      },
      "source": [
        "true_label=lab_batch[0].numpy()\n",
        "patch_size=20\n",
        "height_start=8\n",
        "width_start=8\n",
        "\n",
        "# Task 3.1.2 pass the image through a forward pass  and estimate the probability of prediction of the True label (2 lines)\n",
        "# remember the network will be expect an input of shape BxCxHxW where B is batch size and C is number of channels\n",
        "# use nn.functional.softmax to return a probability\n",
        "output_full=None\n",
        "full_prob=None\n",
        "\n",
        "# Task 3.1.2 pass the image through the occlusion function and make forward pass (2 lines)\n",
        "occluded_image=None\n",
        "output_occluded=None\n",
        "# Task 3.1.3 return label probabilities for this prediction using softmax\n",
        "occluded_prob=None\n",
        "print('True label: {} original probability: {} occluded probability: {}'.format(true_label, full_prob,occluded_prob)) \n",
        "\n",
        "# plot\n",
        "fig2 = plt.figure(figsize=(15,5))\n",
        "fig2.add_subplot(1, 2, 1)\n",
        "#displaying the image using seaborn heatmap and also setting the maximum value of gradient to probability\n",
        "img = torchvision.transforms.ToPILImage()(occluded_image[0].cpu())\n",
        "plt.imshow(img)\n",
        "fig2.add_subplot(1, 2, 2)\n",
        "img = torchvision.transforms.ToPILImage()(im_batch[0].cpu())\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JerU1b39_3ja"
      },
      "source": [
        "You will see that, not unexpectedly grey out the part of the image which contains the number will dramatically reduce the probability of selecting the correct label.\n",
        "\n",
        "Thus, occlusion presents us a network indepedent way of generating a map of regional saliencies. All we need to do is **occlude patches centered at all feasible locations** in the image; then we can generate a heatmap of label probabilities estimated for each of these patches/locations. \n",
        "\n",
        "\n",
        "#### **Exercise 3.2 Create occlusion based saliency map**\n",
        "\n",
        "Create a function to iterate across an image to generate patch predictions at each feasible location.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biwnG-Fd_3jb"
      },
      "source": [
        "#STUDENTS CODE -  Write a custom function to conduct occlusion experiments\n",
        "\n",
        "def occlusion(model, image, label, occ_size = 50,  occ_pixel = 0.5):\n",
        "    '''\n",
        "       function to iterate occlusion mapping for all locations in an image\n",
        "       in order to return a heatmap\n",
        "       input:\n",
        "            model - the trained model\n",
        "            image - the input image to occlude\n",
        "            label - the true image label\n",
        "            occ_size - the patch size\n",
        "            occ_pixel - the value to fill the patch with\n",
        "      \n",
        "      output:\n",
        "           heatmap (torch array) - same shape as image,\n",
        "                                 - value at each grid location =  label probabilities corresponding to result of occluding a patch centred at the corresponding image location\n",
        "\n",
        "    '''\n",
        "    #create a zero image with shape equal to the image\n",
        "    heatmap = torch.zeros((image.shape[1],image.shape[2]))\n",
        "    \n",
        "    #STUDENTS CODE Task 3.2 complete occlusion function\n",
        "\n",
        "\n",
        "\n",
        "    return heatmap\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD1WnoWWBgsN"
      },
      "source": [
        "**To do** now run for the test image - you may need to change the patch size to get an interpretable result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoJKn21J_3jb"
      },
      "source": [
        "print(pred[0],pred[0].type)\n",
        "image=im_batch[0]\n",
        "heatmap = occlusion(net, image, lab_batch[0].item(),20, 1)\n",
        "print(torch.max(heatmap))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEt7IAnY_3jb"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig2 = plt.figure(figsize=(15,5))\n",
        "fig2.add_subplot(1, 2, 1)\n",
        "#displaying the image using seaborn heatmap and also setting the maximum value of gradient to probability\n",
        "imgplot = sns.heatmap(heatmap.detach().numpy(), xticklabels=False, yticklabels=False, vmax=prob_no_occ)\n",
        "figure = imgplot.get_figure()    \n",
        "fig2.add_subplot(1, 2, 2)\n",
        "img = torchvision.transforms.ToPILImage()(image[0].cpu())\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuV8USNP_3jb"
      },
      "source": [
        "### **Exercise 4. Gradient visualization with Guided backpropagation**\n",
        "\n",
        "So occlusion can be an effective method for returning saliency maps but for large images it can be computationally costly compute. Therefore several alternative methods have been proposed which instead seek to backpropagate gradients from output neurons to the locations in the image which contribute most significantly to the prediction. \n",
        "\n",
        "The first method we will look at will be Guided Backpropation, which is an extension of DeconvNet to generalise to all convolutional networks (which downsample through strided convolutions). The Deconvnet visualises activations pertaining to specific classes, by pushing gradient activations backwards from through the network through a series of unpooling, relus (to zero negative gradients) and inverse convolutions (implemented by applying transposed filters)\n",
        "\n",
        "For [Guided Backpropagation](https://arxiv.org/pdf/1412.6806.pdf), the outputs of relu operations at each layer are pushed backwards, guided by masks which summarise the forwards and backwards relu. Thus these zero contributions from any locations with negative activations or negative gradients to return locations in the image that contribute positively to the prediction.\n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=14kCGfNgojzkwDa5k44RyiqsBukoS7Ls1\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "You can find the implementation of guided backpropagation in the `vizualizations` package folder that you uploaded to run this notebook `/visualizations/src/guided_backprop.py`.\n",
        "\n",
        "#### **Exercise 4.1 first comment each line in `GuidedBackprop.update_relus(self)`**\n",
        "\n",
        "To cement your understanding of the Guided Backpropagation method, comment each line of the function `GuidedBackprop.update_relus(self)`. This uses hooks to extract the activations and gradients of each relu layer. These are used to mask the relu gradients and push them back towards the input pixel space.\n",
        "\n",
        "**To do** Copy and paste below the line which combines the forward and backwards masking of negative gradients below\n",
        "\n",
        "**Answer 4.1:**\n",
        "\n",
        "```\n",
        "# copy here the line of code which combines forwards and backwards masking of gradients is\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNLisyMh_3ji"
      },
      "source": [
        "\n",
        "\n",
        "**Now, run guided backpropagation for a given input image.**\n",
        "\n",
        "The `visualizations` package is specifically designed to visualise the layers of PyTorch pretrained networks. Thus the function `misc_functions.get_example_params()` function loads a pretrained AlexNet and returns a preprocessed image `prep_img`,as well as it's target label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9L0JqUi_3ji"
      },
      "source": [
        "target_example = 0  \n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "\n",
        "im_array=prep_img[0].detach().numpy()\n",
        "print(prep_img.shape,np.moveaxis(im_array, 0, -1).shape)\n",
        "plt.imshow(np.moveaxis(im_array, 0, -1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbbMgCik_3jj"
      },
      "source": [
        "An instance of the guidedbackprop class can be created using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwAVsIHF_3jj"
      },
      "source": [
        "#--------------------------------------------------------- Guided backprop---------------------------------------------------\n",
        "# Guided backprop\n",
        "GBP = GuidedBackprop(pretrained_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdxfNmac_3jj"
      },
      "source": [
        "Gradients specific to a particular input image, and class may be generated using the method `GP.generate_gradients()`, which has the following steps:\n",
        " 1. go through a forward pass with the image input, and generate an output\n",
        " 2. backprop through the output\n",
        " 3. get the gradients from the backprop\n",
        " \n",
        "**Run:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ym-UsdA_3jj"
      },
      "source": [
        "# Get gradients\n",
        "guided_grads = GBP.generate_gradients(prep_img, target_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTg8UTE4_3jj"
      },
      "source": [
        "We can then plot the outputs. Note for RGB matplot lib expects channels on the 3rd axis so we need to reshape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtT5jETU_3jj"
      },
      "source": [
        "# Convert to grayscale\n",
        "grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
        "# Positive and negative saliency maps\n",
        "pos_sal, neg_sal = get_positive_negative_saliency(guided_grads)\n",
        "\n",
        "### return normalise gradients and plot\n",
        "guided_grads=normalise_gradient(guided_grads)\n",
        "grayscale_guided_grads=normalise_gradient(grayscale_guided_grads)\n",
        "pos_sal=normalise_gradient(pos_sal)\n",
        "neg_sal=normalise_gradient(neg_sal)\n",
        "\n",
        "fig2 = plt.figure(figsize=(20,10))\n",
        "fig2.add_subplot(1, 4,1)\n",
        "plt.imshow(np.moveaxis(guided_grads, 0, -1))\n",
        "fig2.add_subplot(1, 4,2)\n",
        "plt.imshow(grayscale_guided_grads[0],cmap='gray')\n",
        "fig2.add_subplot(1, 4,3)\n",
        "plt.imshow(np.moveaxis(pos_sal, 0, -1))\n",
        "fig2.add_subplot(1, 4,4)\n",
        "plt.imshow(np.moveaxis(neg_sal, 0, -1))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfE-hYms_3jl"
      },
      "source": [
        "#### **Exercise 4.2 Run guided backprop for different inputs**\n",
        "You can do this by changing `target_example=` (there's 3 inputs available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNTKkmE0_3jl"
      },
      "source": [
        "# --------------------------------------------- Task 4 --------------------------------------------------------\n",
        "# write code here\n",
        "\n",
        "target_example = None\n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "\n",
        "im_array=prep_img[0].detach().numpy()\n",
        "print(prep_img.shape,np.moveaxis(im_array, 0, -1).shape)\n",
        "plt.imshow(np.moveaxis(im_array, 0, -1))\n",
        "plt.show()\n",
        "\n",
        "# Get gradients \n",
        "guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
        "\n",
        "# Convert to grayscale\n",
        "grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
        "# Positive and negative saliency maps\n",
        "pos_sal, neg_sal = get_positive_negative_saliency(guided_grads)\n",
        "\n",
        "### return normalise gradients and plot\n",
        "guided_grads=normalise_gradient(guided_grads)\n",
        "grayscale_guided_grads=normalise_gradient(grayscale_guided_grads)\n",
        "pos_sal=normalise_gradient(pos_sal)\n",
        "neg_sal=normalise_gradient(neg_sal)\n",
        "\n",
        "fig2 = plt.figure(figsize=(20,10))\n",
        "fig2.add_subplot(1, 4,1)\n",
        "plt.imshow(np.moveaxis(guided_grads, 0, -1))\n",
        "fig2.add_subplot(1, 4,2)\n",
        "plt.imshow(grayscale_guided_grads[0],cmap='gray')\n",
        "fig2.add_subplot(1, 4,3)\n",
        "plt.imshow(np.moveaxis(pos_sal, 0, -1))\n",
        "fig2.add_subplot(1, 4,4)\n",
        "plt.imshow(np.moveaxis(neg_sal, 0, -1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbinlavM_3jn"
      },
      "source": [
        "## **Exercise 5 Gradient Class Activation Mapping (grad-CAM)**\n",
        "\n",
        "[Grad-cam](https://arxiv.org/abs/1610.02391) allows extraction of occlusion-like saliency maps in a single pass, by creating a neuron importance score for a given target class. \n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1POWiMnNv9t-WhLwV6qdFkE5KFGuPoVEQ\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "Specifically, it makes a forward pass, estimates the gradient of output activation ($A^k$), before the softmax and with respect to the target class ($y^c$), clamps this to one and then sets the gradient for all other classes to zero. It then estimates a **neuron importance score** by global average pooling over the spatial dimensions ($i,j$) of the channel:\n",
        "\n",
        "$$\\alpha_k^c=\\frac{1}{Z} \\sum_i \\sum_j \\dfrac{\\partial y^c}{\\partial A^k_{ij}}$$\n",
        "\n",
        "This summarises the importance of a particular featuremap or channel to the prediction.\n",
        "\n",
        "The final $\\mathbb{R}^{u \\times v}$ visualisation is then estimated from a weighted average over all activation map (where weights are given by the neuron importance scores). This is followed by a ReLU to clamp the visualisation to return only positive contributions. \n",
        "\n",
        "$$L^C_{Grad\\_cam}=RELU(\\sum_k \\alpha_k^c A^k)$$\n",
        "\n",
        "\n",
        "### **Exercise 5.1 Apply the code**\n",
        "\n",
        "Run grad cam on the same examples used for Guided Backprop. What key differences do you observe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iU3w2h9_3jq"
      },
      "source": [
        "# STUDENTS CODE - run grad cam for visualizations example\n",
        "\n",
        "#Get params\n",
        "target_example = None\n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "\n",
        "# Grad cam\n",
        "grad_cam = None\n",
        "# Generate cam mask\n",
        "cam = None\n",
        "\n",
        "# Create Grayscale activation map\n",
        "heatmap, heatmap_on_image = apply_colormap_on_image(original_image, cam, 'hsv')\n",
        "\n",
        "# plot\n",
        "fig_cam = plt.figure(figsize=(20,10))\n",
        "fig_cam.add_subplot(1, 3,1)\n",
        "plt.imshow(original_image)\n",
        "fig_cam.add_subplot(1, 3,2)\n",
        "plt.imshow(cam)\n",
        "fig_cam.add_subplot(1, 3,3)\n",
        "plt.imshow(heatmap_on_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPDLmpl_3jq"
      },
      "source": [
        "**Note** `target_example` is a picture of a dog and a cat but the `get_example_params` is hard coded to dog. If you would like to try different pictures the imagenet class label list can be found [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)\n",
        "\n",
        "#### **Exercise Task 5.3 try changing the class of the image to 'tabby cat'**\n",
        "\n",
        "You might also want to chose other images, or vary the target layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUD3wH1M_3jq"
      },
      "source": [
        "# STUDENTS CODE - run grad cam for but change class of image to tabby cat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_d7B12K-F3"
      },
      "source": [
        "### **Exercise 6 (optional) Grad-CAM for MNIST**\n",
        "\n",
        "Go back and change your MNIST network to make work with the visualization library. Remember it is set up to work with pretrained PyTorch networks which expect a very specific modular structure with the convolutional layers all defined in a `nn.Sequential` container called `features` and the linear layers defined in an `nn.Sequential` container called `classfier`. See the PyTorch [AlexNet](https://pytorch.org/vision/0.8/_modules/torchvision/models/alexnet.html#alexnet) for reference.\n",
        "\n",
        "**Note**\n",
        "1. The `visualizations` library assumes the networks have been trained on CPU so you either need to edit the grad_cam function to work for GPU, or (probably simpler in this case) just change runtime to CPU for this experiment.\n",
        "2. when plotting you can make uses of the `apply_colormap_on_image` function shown above provided you first convert your input array to PIL image format, as shown below:\n",
        "\n",
        "```\n",
        "# plotting - first convert MNIST to a PIL image\n",
        "org_im=Image.fromarray(np.uint8(im_batch[0,0,:,:].numpy()*255))\n",
        "heatmap, heatmap_on_image = apply_colormap_on_image(org_im, cam, 'hsv')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9b4hmb1_oL8"
      },
      "source": [
        "# STUDENTS CODE BELOW\n",
        "# Define network\n",
        "# initialise network, optimiser and loss function\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dvxq5CO__PC"
      },
      "source": [
        "# STUDENT CODE TRAIN NETWORK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7GaNWYpAYsx"
      },
      "source": [
        "# STUDENT CODE - PLOTTING GRAD CAM FOR MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olqBGyZSIRw8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipjPBODnDPl_"
      },
      "source": [
        "# Source references\n",
        "\n",
        "1. [visualizing-convolution-neural-networks-using-pytorch](https://towardsdatascience.com/visualizing-convolution-neural-networks-using-pytorch-3dfa8443e74e)\n",
        "2. [DeepLearning-PadhAI](https://colab.research.google.com/github/Niranjankumar-c/DeepLearning-PadhAI/blob/master/DeepLearning_Materials/6_VisualizationCNN_Pytorch/CNNVisualisation.ipynb#scrollTo=uQI9jHcP6xfP)"
      ]
    }
  ]
}