{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Naive LSTM\n",
    "\n",
    "Here we present an example of 1 dimension LSTM to predict stock market. Note this script was modified from the original code presented in https://www.kaggle.com/taronzakaryan/stock-prediction-lstm-using-pytorch/data. \n",
    "\n",
    "First we load all important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import Optional, List, Tuple\n",
    "from enum import IntEnum\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's code an LSTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2\n",
    " \n",
    "class NaiveLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        # input gate\n",
    "        self.W_ii = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hi = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        # forget gate\n",
    "        self.W_if = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hf = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        # input info\n",
    "        self.W_ig = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_hg = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        # output gate\n",
    "        self.W_io = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.W_ho = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "         \n",
    "        self.init_weights()\n",
    "     \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "         \n",
    "    def forward(self, x: torch.Tensor, \n",
    "                init_states: Optional[Tuple[torch.Tensor]]=None\n",
    "               ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = torch.zeros(self.hidden_size).to(x.device), torch.zeros(self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        for t in range(seq_sz): # iterate over the time steps\n",
    "            x_t = x[:, t, :]\n",
    "            i_t = torch.sigmoid(x_t @ self.W_ii + h_t @ self.W_hi + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.W_if + h_t @ self.W_hf + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.W_ig + h_t @ self.W_hg + self.b_g)\n",
    "            o_t = torch.sigmoid(x_t @ self.W_io + h_t @ self.W_ho + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            hidden_seq.append(h_t.unsqueeze(Dim.batch))\n",
    "            \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the network and generate a random input (time series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 125\n",
    "input_size = 50\n",
    "sequence_len = 150\n",
    "high = 1000000 \n",
    "\n",
    "## Generate random input with \n",
    "test_idx = torch.randint(high=high, size=(1, sequence_len)).to(device)\n",
    "embeddings = nn.Embedding(high, input_size).to(device)\n",
    "test_embeddings = embeddings(test_idx).to(device)\n",
    "test_idx_np = test_idx.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build our LSTM model and define a step by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = NaiveLSTM(input_size,hidden_size)\n",
    "\n",
    "def lstm_step(x_t, h_t, c_t, W_ii, W_hi, b_i, W_if, W_hf, b_f,\n",
    "              W_ig, W_hg, b_g, W_io, W_ho, b_o, use_forget_gate=False):\n",
    "    \n",
    "    i_t = torch.sigmoid(x_t @ W_ii + h_t @ W_hi + b_i)\n",
    "    if use_forget_gate:\n",
    "        f_t = torch.sigmoid(x_t @ W_if + h_t @ W_hf + b_f)\n",
    "    g_t = torch.tanh(x_t @ W_ig + h_t @ W_hg + b_g)\n",
    "    o_t = torch.sigmoid(x_t @ W_io + h_t @ W_ho + b_o)\n",
    "    if use_forget_gate:\n",
    "        c_t = f_t * c_t + i_t * g_t\n",
    "    else:\n",
    "        c_t = c_t + i_t * g_t\n",
    "    h_t = o_t * torch.tanh(c_t)\n",
    "    return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1, we set the use_forget_gate == False. In the below plot, one could see instead of decaying, the gradient keeps on accumulating! The reason the gradient behaves this way is because of the update rule\n",
    "c_t = c_{t-1} + i_t * g_t. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1 \n",
    "h_0, c_0 = (torch.zeros(hidden_size, requires_grad=True), \n",
    "            torch.zeros(hidden_size, requires_grad=True))\n",
    "grads = []\n",
    "h_t, c_t = h_0, c_0\n",
    "\n",
    "for t in range(sequence_len):\n",
    "    h_t, c_t = lstm_step(\n",
    "        test_embeddings[:, t, :], h_t, c_t,\n",
    "        lstm.W_ii, lstm.W_hi, lstm.b_i,\n",
    "        lstm.W_if, lstm.W_hf, lstm.b_f,\n",
    "        lstm.W_ig, lstm.W_hg, lstm.b_g,\n",
    "        lstm.W_io, lstm.W_ho, lstm.b_o,\n",
    "        use_forget_gate=False,\n",
    "    ) \n",
    "    loss = h_t.abs().sum()\n",
    "    loss.backward(retain_graph=True)\n",
    "    grads.append(torch.norm(h_0.grad).item())\n",
    "    h_0.grad.zero_()\n",
    "    lstm.zero_grad()\n",
    "    \n",
    "grads_np = np.array(grads)\n",
    "plt.plot(grads_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2, we set the use_forget_gate == True. In this case, one we turn the forget gate, the gradient will vanish through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_02, c_02 = (torch.zeros(hidden_size, requires_grad=True), \n",
    "            torch.zeros(hidden_size, requires_grad=True))\n",
    "grads2 = []\n",
    "h_t2, c_t2 = h_02, c_02\n",
    "\n",
    "for t in range(sequence_len):\n",
    "    h_t2, c_t2 = lstm_step(\n",
    "        test_embeddings[:, t, :], h_t2, c_t2,\n",
    "        lstm.W_ii, lstm.W_hi, lstm.b_i,\n",
    "        lstm.W_if, lstm.W_hf, lstm.b_f,\n",
    "        lstm.W_ig, lstm.W_hg, lstm.b_g,\n",
    "        lstm.W_io, lstm.W_ho, lstm.b_o,\n",
    "        use_forget_gate=True,\n",
    "    ) \n",
    "    # use_forget_gate=True,\n",
    "    loss2 = h_t2.abs().sum()\n",
    "    loss2.backward(retain_graph=True)\n",
    "    grads2.append(torch.norm(h_02.grad).item())\n",
    "    h_02.grad.zero_()\n",
    "    lstm.zero_grad()\n",
    "    \n",
    "grads_np2 = np.array(grads2)\n",
    "plt.plot(grads_np2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of LSTM\n",
    "\n",
    "Here we present an example of 1 dimension LSTM to predict some time series (such as the stock market). Note this script was modified from the original code presented in https://www.kaggle.com/taronzakaryan/stock-prediction-lstm-using-pytorch/data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import data and plot using plt.plot the stock prince of three companies (GOOGLE, IBM, and APPLE) between 2-Jan-2015 to 31-Dec-2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_download_link = \"https://github.com/KCL-BMEIS/AdvancedMachineLearningCourse/blob/main/Week10_Sequences_and_Geometry/Data/Week10_data.zip?raw=true\"\n",
    "!wget -O Week10_data.zip --no-check-certificate \"$file_download_link\"\n",
    "!unzip Week10_data.zip\n",
    "\n",
    "for dirname, _, filenames in os.walk('/content/Stocks'):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        if i<5:\n",
    "            print(os.path.join(dirname,filename))\n",
    "            \n",
    "def stocks_data(symbols, dates):\n",
    "    df = pd.DataFrame(index=dates)\n",
    "    for symbol in symbols:\n",
    "        df_temp = pd.read_csv(\"/content/Stocks/{}.us.txt\".format(symbol), index_col='Date',\n",
    "                parse_dates=True, usecols=['Date', 'Close'], na_values=['nan'])\n",
    "        df_temp = df_temp.rename(columns={'Close': symbol})\n",
    "        df = df.join(df_temp)\n",
    "    return df\n",
    "\n",
    "dates = pd.date_range('2015-01-02','2016-12-31',freq='B')\n",
    "symbols = ['goog','ibm','aapl']\n",
    "df = stocks_data(symbols, dates)\n",
    "df.fillna(method='pad')\n",
    "df.interpolate().plot()\n",
    "plt.show()\n",
    "df.head()\n",
    "# read header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we pick to plot only the price of IBM between 2-Jan-2010 to 11-Oct-2017, this is the data that we will analyse ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2010-01-02','2017-10-11',freq='B')\n",
    "df1=pd.DataFrame(index=dates)\n",
    "df_ibm=pd.read_csv(\"input/Data/Stocks/ibm.us.txt\", parse_dates=True, index_col=0)\n",
    "df_ibm=df1.join(df_ibm)\n",
    "df_ibm[['Close']].plot()\n",
    "plt.ylabel(\"stock_price\")\n",
    "plt.title(\"IBM Stock\")\n",
    "plt.show()\n",
    "\n",
    "df_ibm=df_ibm[['Close']]\n",
    "df_ibm.info()\n",
    "\n",
    "df_ibm=df_ibm.fillna(method='ffill')\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_ibm['Close'] = scaler.fit_transform(df_ibm['Close'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will divide the data (IBM-stock price as shown in the above figure) into training and testing. At the below code, we decided to take 1606 time points to train the model and the rest (402) to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into train and test\n",
    "# function to create train, test data given stock data and sequence length\n",
    "def load_data(stock, look_back):\n",
    "    data_raw = stock.as_matrix() # convert to numpy array\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - look_back): \n",
    "        data.append(data_raw[index: index + look_back])\n",
    "    \n",
    "    data = np.array(data);\n",
    "    test_set_size = int(np.round(0.2*data.shape[0]));\n",
    "    train_set_size = data.shape[0] - (test_set_size);\n",
    "    \n",
    "    x_train = data[:train_set_size,:-1,:]\n",
    "    y_train = data[:train_set_size,-1,:]\n",
    "    \n",
    "    x_test = data[train_set_size:,:-1]\n",
    "    y_test = data[train_set_size:,-1,:]\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "look_back = 20 # choose sequence length\n",
    "x_train, y_train, x_test, y_test = load_data(df_ibm, look_back)\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ',y_train.shape)\n",
    "print('x_test.shape = ',x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)\n",
    "\n",
    "# make training and testing in torch\n",
    "# make training and test sets in torch\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "#train_X = train_X.view([-1, x_train.shape[0], 1])\n",
    "#test_X = test_X.view([-1, x_test.shape[0], 1])\n",
    "#train_Y = train_Y.view([y_train.shape[0], 1])\n",
    "\n",
    "n_steps = look_back-1\n",
    "batch_size = 1606\n",
    "#n_iters = 3000\n",
    "num_epochs = 100 #n_iters / (len(train_X) / batch_size)\n",
    "#num_epochs = int(num_epochs)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(x_train,y_train)\n",
    "test = torch.utils.data.TensorDataset(x_test,y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to build the model by defining the LSTM parameters (e.g. input_dim, hidden_dim, num_layers and output_dim). Since we would like to use the history of stock price to predict its future values (both the input and output dimension are one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build model\n",
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 2 \n",
    "output_dim = 1\n",
    "\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement the model, define loss function, and start learning using torch.optim.Adam with learning rate of 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim =look_back-1  \n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    #model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_train_pred = model(x_train)\n",
    "\n",
    "    loss = loss_fn(y_train_pred, y_train)\n",
    "    if t % 10 == 0 and t !=0:\n",
    "        print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the results we have. Please consider changing the above parameter(s) and see how it would affect the predicting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Make prediction\n",
    "y_test_pred = model(x_test)\n",
    "\n",
    "# invert predictions\n",
    "y_train_pred_scal = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
    "y_train_scal = scaler.inverse_transform(y_train.detach().numpy())\n",
    "y_test_pred_scal = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
    "y_test_scal = scaler.inverse_transform(y_test.detach().numpy())\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(y_train_scal[:,0], y_train_pred_scal[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(y_test_scal[:,0], y_test_pred_scal[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(df_ibm)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(y_train_pred_scal)+look_back, :] = y_train_pred_scal\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(df_ibm)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(y_train_pred_scal)+look_back-1:len(df_ibm)-1, :] = y_test_pred_scal\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(scaler.inverse_transform(df_ibm)) # in blue\n",
    "plt.plot(trainPredictPlot) # in orange\n",
    "plt.plot(testPredictPlot) # in green\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
