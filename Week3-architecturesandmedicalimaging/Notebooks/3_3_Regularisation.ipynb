{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2hJNQhQn9sg"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQYAhwYunzXX"
   },
   "source": [
    "# Lecture 3: Regularisation\n",
    "\n",
    "We will now re-run the simple MLP exercise from lecture 1 but this time with regularisation. Let's import all the functions we need and reload the data (**don't forget to mount the drive and add the path to the dataframe relative to your own drive**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koSj6k-wnzXd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# STUDENTS CODE HERE - UPDATE THE PATH TO CORRESPOND TO WHERE YOU HAVE UPLOADED prem_vs_termwrois.pkl TO YOUR DRIVE #\n",
    "file_path='/content/drive/My Drive/Colab Notebooks/AdvancedML/2021/01_fundamentals/prem_vs_termwrois.pkl'\n",
    "# Read the data\n",
    "df = pd.read_pickle(file_path)\n",
    "data = df.values[:,:-2]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "# create a test and train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create feature matrix\n",
    "X_train = X_train.T\n",
    "X_test=X_test.T\n",
    "bias_row=np.ones((1,X_train.shape[1]))\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "print(bias_row.shape)\n",
    "X_train = np.concatenate((np.ones((1,X_train.shape[1])),X_train))\n",
    "X_test = np.concatenate((np.ones((1,X_test.shape[1])),X_test))\n",
    "\n",
    "\n",
    "\n",
    "# set variables for numbers of feature and examples to improve readabiity of code\n",
    "n_features=X_train.shape[0]-1\n",
    "n_examples=X_train.shape[1]\n",
    "\n",
    "print('Dimension of X is ', X_train.shape,data.shape)\n",
    "print('Dimension of y is ', y.shape)\n",
    "\n",
    "print('Number of features', n_features)\n",
    "print('Number of examples', n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGM3C0kMnzXf"
   },
   "source": [
    "## The multi-layer perceptron (MLP)\n",
    "\n",
    "Our MLP for binary classification contains a single hidden layer (with five units and ReLu activation) and a sigmoid classification output layer, such that:\n",
    "\n",
    "$$ \\hat{y} = f_2 \\left( \\mathbf{W_2} f_1 \\left(\\mathbf{W_1}\\mathbf{X}\\right) \\right) $$\n",
    "\n",
    "with:\n",
    "\n",
    "$$ \\text{Relu}(x) = \\text{max}(0,x)$$\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=1I-1ALBHTlOzmVNSIBiruW3vhS51kejtf\" alt=\"Drawing\" width=\"500px;\"/>\n",
    "</figure>\n",
    "\n",
    "This is implemented with forward pass:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_1 = \\mathbf{W}_1 \\mathbf{X} \\\\\n",
    "\\mathbf{F}_1 = \\text{max}(0,\\mathbf{Z_1}) \\\\\n",
    "\\mathbf{Z}_2 = \\mathbf{W}_2 \\mathbf{F}_1 \\\\\n",
    "\\mathbf{F}_2 = \\dfrac{1}{1+e^{- \\mathbf{Z_2}}} \\\\\n",
    "\\mathbf{L}  =  - \\mathbf{Y} \\ln(\\mathbf{F_2}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F_2})\n",
    "$$\n",
    "\n",
    "And backwards pass:\n",
    "\n",
    "$$\\frac{\\delta L}{\\delta \\mathbf{F}_2}=\\frac{\\mathbf{F}_2-\\mathbf{Y}}{\\mathbf{F}_2(1-\\mathbf{F}_2)} \\\\\n",
    "\\frac{\\delta  \\mathbf{F}_2}{\\delta  \\mathbf{Z}_2}=\\mathbf{F}_2(1-\\mathbf{F}_2) \\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{W}_2}=\\mathbf{F_1} \\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{F}_1}=\\mathbf{W_2^T}\\\\\n",
    "\\frac{\\delta  \\mathbf{F}_1}{\\delta  \\mathbf{Z}_1}=1(\\mathbf{Z}_1 >0)\\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_1}{\\delta  \\mathbf{W}_1}=\\mathbf{X}\\\\ $$\n",
    "\n",
    "Let us redefine the functions from notebook 01-fundamentals.ipynb and re-run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a6KVEcfnzXf"
   },
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    return 1 / (1+ np.exp(-z))\n",
    "\n",
    "def relu(x):\n",
    "    # Answer\n",
    "    return x * (x>=0)\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    epsilon = 1e-5\n",
    "    # note the negative sign so that the loss decreases as our predictions get better\n",
    "    # we must add a small penaty term to prevent calculation of log(0)\n",
    "    L = - y * np.log(y_pred+epsilon) - (1-y) * np.log(1-y_pred+epsilon) \n",
    "    J = np.mean(L)\n",
    "    return J\n",
    "\n",
    "def accuracy(y, y_pred, threshold = 0.5):\n",
    "    y_pred_thresholded = y_pred > threshold\n",
    "    correct_predictions = np.sum(y==y_pred_thresholded)  \n",
    "    total_predictions = np.shape(y)\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouhs-4PWnzXg"
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "# normalise X\n",
    "X_norm = np.ones_like(X_train)\n",
    "X_norm[1:] = (X_train[1:] -X_train[1:].min(axis=1,keepdims=True) )/ (X_train[1:].max(axis=1,keepdims=True)-X_train[1:].min(axis=1,keepdims=True)+epsilon)\n",
    "\n",
    "# initialise w1, w2\n",
    "W1 = np.random.randn(5,X_train.shape[0])\n",
    "W2 = np.random.randn(1,5)\n",
    "\n",
    "# we'll store the loss and accuracy in these lists during training\n",
    "loss_record_mlp = []\n",
    "accuracy_record_mlp = []\n",
    "\n",
    "num_iterations = 2000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # forward pass - get predictions\n",
    "    \n",
    "    #### STUDENT CODE HERE####\n",
    "    # please keep the output names of each stage so they work later with backprop\n",
    "    Z1 = np.matmul(W1,X_norm)\n",
    "    F1 = relu(Z1)\n",
    "    Z2 = np.matmul(W2,F1)\n",
    "    F2 = f(Z2) # recall f is the sigmoid function\n",
    "    l = loss(y_train,F2) \n",
    "\n",
    "    # store the loss/ accuracy at this iteration\n",
    "    loss_record_mlp.append(l)\n",
    "    accuracy_record_mlp.append(accuracy(y_train,F2))\n",
    "\n",
    "    \n",
    "    #backwards pass to get gradients\n",
    "    dL_dW2=np.matmul(F2-y_train,F1.T) \n",
    "    dL_df1=np.matmul((F2-y_train).T,W2)  \n",
    "    df1_dZ1  = 1.0 *(Z1> 0)\n",
    "    \n",
    "    dL_dZ1=np.multiply(dL_df1.T,df1_dZ1)\n",
    "    dL_dW1 = np.matmul(dL_dZ1,X_norm.T)\n",
    "    dJ_dW2=(1/W2.shape[0])*dL_dW2 \n",
    "    dJ_dW1=(1/W1.shape[0])*dL_dW1 \n",
    "\n",
    "    # update the weights\n",
    "    W2 = W2 - learning_rate * dJ_dW2    \n",
    "    W1 = W1 - learning_rate * dJ_dW1\n",
    "    \n",
    "# plot loss and accuracy    \n",
    "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
    "ax[0].plot(loss_record_mlp)\n",
    "ax[1].plot(accuracy_record_mlp)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy');\n",
    "\n",
    "print(accuracy(y_train,F2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOJW-qM9nzXh"
   },
   "source": [
    "Testing on left out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nd0l2jqnzXh"
   },
   "outputs": [],
   "source": [
    "# norm X\n",
    "X_test_norm= np.ones_like(X_test)\n",
    "X_test_norm[1:] = (X_test[1:] -X_test[1:].mean(axis=1,keepdims=True)) / (X_test[1:].max(axis=1,keepdims=True) - X_test[1:].min(axis=1,keepdims=True) +epsilon)\n",
    "\n",
    "Z1_test = np.matmul(W1,X_test_norm)\n",
    "F1_test = relu(Z1_test)\n",
    "Z2_test = np.matmul(W2,F1_test)\n",
    "F2_test = f(Z2_test) \n",
    "l = loss(y_test,F2_test) \n",
    "\n",
    "print(l,accuracy(y_test,F2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QodiwbVMnzXh"
   },
   "source": [
    "## Adding Regularisation\n",
    "\n",
    "One problem with neural networks is that they can involve the training of very high numbers of parameters (defined by the total number of elements in all our weights matrices). The more parameters we can chose from the greater the chance of overfitting \n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=1ZKuwPsBML5vU41hQI0jM8YGwWFwdOP6v\" alt=\"Drawing\" width=\"800px;\"/>\n",
    "</figure>\n",
    "\n",
    "There are several ways of controlling the capacity of Neural Networks to prevent overfitting. These include\n",
    "\n",
    "1. L1 and L2 regularisation â€“ penalise the network through addition of a penalty term i.e. \n",
    "\n",
    "$$ J =\\frac{1}{n} (\\sum_i L_i + \\lambda <\\textrm{penalty term}>)$$\n",
    "\n",
    "2. Dropout - during training keep only a subset of neurons active (with probability ð‘); set to zero otherwise.\n",
    "\n",
    "Dropout will be considered in more detail in lecture 4. Here, we will consider the inclusion of a penalty term. Of these L2 is the most common.  This requires a penalty of $\\lambda/2 â€–\\mathbf{W}â€–^2$ (where the 1/2  term is used to make gradient $\\lambda â€–\\mathbf{W}â€– $ rather than $2 \\lambda â€–\\mathbf{W}â€– $). L2 regularisation encourages the network to learn diffuse weights (small weights spread across all units). On the other hand, L1 has a penalty $\\lambda â€–\\mathbf{W}â€– $ and this encourages the learning of sparse weights, where many individual weights are set to zero.\n",
    "\n",
    "**Task 1** lets try adding L2 regularisation to our MLP network. First write a new loss function which estimates a L2 regularised loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZ6ff3qynzXi"
   },
   "outputs": [],
   "source": [
    "def regularised_loss(y, y_pred, W1,W2, lambda_term,epsilon):\n",
    "    '''\n",
    "        Estimate loss with L2 regularisation\n",
    "        \n",
    "    input:\n",
    "        y: True label\n",
    "        y_pred: estimate label\n",
    "        W1: weights matrix for hidden layer\n",
    "        W2: weights matrix for output layer\n",
    "        lambda_term: equals weighting for penalty term\n",
    "        epsilon: small term to prevent log(0)\n",
    "    returns:\n",
    "        J: L2 regularised cross entropy loss\n",
    "        \n",
    "    '''\n",
    "    # STUDENTS IMPLEMENT REGULARISATION TERM\n",
    "    reg=None\n",
    "    L = - y * np.log(y_pred+epsilon) - (1-y) * np.log(1-y_pred+epsilon)\n",
    "    J = np.mean(L) + reg\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNZzZfAEnzXi"
   },
   "source": [
    "Now making a copy of the code from above but changing the call to the loss function \n",
    "\n",
    "**Task 2**\n",
    "-  implement the new loss and\n",
    "-  update the gradient computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVRtK-UjnzXj"
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "# define lambda - penalty term weighting \n",
    "#(can't use variable name lambda as it is a python operation)\n",
    "lambda_term=5e-4 \n",
    "# centre X\n",
    "X_centred = np.ones_like(X_train)\n",
    "X_centred[1:] = (X_train[1:] -X_train[1:].mean(axis=1,keepdims=True)) / (X_train[1:].std(axis=1,keepdims=True)+epsilon)\n",
    "\n",
    "# initialise w1, w2\n",
    "W1 = np.random.randn(5,X_train.shape[0])\n",
    "W2 = np.random.randn(1,5)\n",
    "\n",
    "# we'll store the loss and accuracy in these lists during training\n",
    "loss_record_mlp = []\n",
    "accuracy_record_mlp = []\n",
    "\n",
    "num_iterations = 1000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # forward pass - get predictions\n",
    "    \n",
    "    Z1 = np.matmul(W1,X_norm)\n",
    "    F1 = relu(Z1)\n",
    "    Z2 = np.matmul(W2,F1)\n",
    "    F2 = f(Z2) # recall f is the sigmoid function\n",
    "    \n",
    "    # TASK 2A Calculate the new regularised loss\n",
    "    l = regularised_loss(y_train,F2,W1,W2,lambda_term,epsilon)\n",
    "\n",
    "    # store the loss/ accuracy at this iteration\n",
    "    loss_record_mlp.append(l)\n",
    "    accuracy_record_mlp.append(accuracy(y_train,F2))\n",
    "\n",
    "    #backwards pass to get gradients\n",
    "    dL_dW2=np.matmul(F2-y_train,F1.T) \n",
    "    dL_df1=np.matmul((F2-y_train).T,W2)  \n",
    "    df1_dZ1  = 1.0 *(Z1> 0)\n",
    "    \n",
    "    dL_dZ1=np.multiply(dL_df1.T,df1_dZ1)\n",
    "    dL_dW1 = np.matmul(dL_dZ1,X_norm.T)\n",
    "    dJ_dW2=(1/W2.shape[0])*dL_dW2 \n",
    "    dJ_dW1=(1/W1.shape[0])*dL_dW1 \n",
    "    \n",
    "    #TASK 2 B update the loss to add the gradient of the penalty term\n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    # update the weights\n",
    "    W2 = W2 - learning_rate * dJ_dW2    \n",
    "    W1 = W1 - learning_rate * dJ_dW1\n",
    "    \n",
    "# plot loss and accuracy    \n",
    "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
    "ax[0].plot(loss_record_mlp)\n",
    "ax[1].plot(accuracy_record_mlp)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy');\n",
    "\n",
    "print(accuracy(y_train,F2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CwN02ARnzXj"
   },
   "source": [
    "Testing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax8YaH9dnzXk"
   },
   "outputs": [],
   "source": [
    "\n",
    "Z1_test = np.matmul(W1,X_test_norm)\n",
    "F1_test = relu(Z1_test)\n",
    "Z2_test = np.matmul(W2,F1_test)\n",
    "F2_test = f(Z2_test) \n",
    "l = loss(y_test,F2_test) \n",
    "\n",
    "print(l,accuracy(y_test,F2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK9RhuNJnzXk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3.3.Regularisation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
