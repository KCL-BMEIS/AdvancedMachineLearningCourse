{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.2.Dealing_with_Medical_Data_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POLfwBkOvRNL"
      },
      "source": [
        "# Dealing with Medical Data\n",
        "\n",
        "Tutorial by Pedro Borges\n",
        "\n",
        "The goal of this notebook is to take you through some common strategies used to pre-process medical imaging data prior to training with convolutional neural networks.\n",
        "\n",
        "Medical image datasets present specific challenges for convolutional deep learning as they are typically small in number, while large in image dimensions. The images themselves are complex and incredibly difficult to read for non clinical experts and presentations of disease may be highly heterogenous.\n",
        "\n",
        "Let's start by pip installing the `SimpleITK` python package for medical image processing; then then defining some plotting functions which will use for some of our analyses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp55oQ8g5T9A"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip3 install SimpleITK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw7UwwEl5T9D"
      },
      "source": [
        "# Setting up various plot functions to be used throughout the notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def plot_images(images, title=None, figsize=(15,15)):\n",
        "    f, axes = plt.subplots(1, len(images), figsize=figsize)\n",
        "    for image_id, image in enumerate(images):\n",
        "        axes[image_id].imshow(image, cmap='gray')\n",
        "        axes[image_id].axis('off')\n",
        "        if not title:\n",
        "            axes[image_id].set_title('Image {}'.format(image_id), fontsize=20)\n",
        "        elif len(title) == 1:\n",
        "            axes[image_id].set_title('Image {}: {}'.format(image_id, title[0]), fontsize=20)\n",
        "        else:\n",
        "            axes[image_id].set_title('Image {}: {}'.format(image_id, title[image_id]), fontsize=20)\n",
        "    f.tight_layout()\n",
        "\n",
        "\n",
        "def plot_grids(grids, figsize=(15,15)):\n",
        "    f, axes = plt.subplots(1, len(grids), figsize=figsize)\n",
        "    for grid_id, grid in enumerate(grids):\n",
        "        grid_array = sitk.GetArrayViewFromImage(grid)\n",
        "        axes[grid_id].imshow(np.flip(grid_array, axis=0),\n",
        "               interpolation='hamming',\n",
        "               cmap='gray',\n",
        "               origin='lower')\n",
        "        axes[grid_id].axis('off')\n",
        "        axes[grid_id].set_title('Grid {}'.format(grid_id), fontsize=20)\n",
        "    f.tight_layout()\n",
        "  \n",
        "\n",
        "def plot_grids_and_images(grids, images, figsize=(15,15)):\n",
        "    f, axes = plt.subplots(1, len(grids), figsize=figsize)\n",
        "    for grid_id, grid in enumerate(grids):\n",
        "        grid_array = sitk.GetArrayViewFromImage(grid)\n",
        "        axes[grid_id].imshow(np.flip(grid_array/np.max(grid_array)*images[grid_id]/np.max(images[grid_id]), axis=0),\n",
        "               interpolation=None,\n",
        "               cmap='gray',\n",
        "               origin='lower')\n",
        "        axes[grid_id].axis('off')\n",
        "        axes[grid_id].set_title('Grid overlay {}'.format(grid_id), fontsize=20)\n",
        "    f.tight_layout()\n",
        "\n",
        "\n",
        "def plot_histograms(images, figsize=(15,7.5)):\n",
        "    f, axes = plt.subplots(1, len(images), figsize=figsize)\n",
        "    for image_id, image in enumerate(images):\n",
        "        histogram, bins = np.histogram(image, bins=40)\n",
        "        axes[image_id].set_xlim([0, 120])\n",
        "        axes[image_id].hist(histogram, bins)\n",
        "        axes[image_id].set_title('Image {} Histogram'.format(image_id), fontsize=20)\n",
        "    f.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8elymp-OFCmX"
      },
      "source": [
        "Next we will download and unpack an example hippocampus MRI dataset, which we will use for our experiments. The hippocampus is a brain structure involved in processing and storing memories. It is known to be one of the first brain areas artophed by Alzheimer's disease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_Sx0scq5T9F"
      },
      "source": [
        "file_download_link = \"https://github.com/pedrob37/AML_Lecture4/blob/master/Task04_Hippocampus.tar?raw=true\"\n",
        "!wget -O Task04_Hippocampus.tar --no-check-certificate \"$file_download_link\"\n",
        "!tar -xf Task04_Hippocampus.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRCDPT40BKZa"
      },
      "source": [
        "If the above cell did not download the data properly (i.e.: You do not see a FOLDER named Task04_Hippocampus.tar) then it's likely download quoatas have been exceeded. If that's the case then please download the data from: https://drive.google.com/drive/folders/1HqEgzS8BV2c7xYNrZdEAnrHk7osJJ--2 (You want the folder named Task04_Hippocampus.tar). Just upload it to this notebook and run:\n",
        "> !tar -xf Task04_Hippocampus.tar  \n",
        "\n",
        " and you should be good to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAPvsmdbEUds"
      },
      "source": [
        "## Exercise 1 Histogram Normalisation\n",
        "\n",
        "Some types of medical scan are non-quamtitative. In other words the intensity distribution represent relative differences between different types of tissue rather than a quantitative measure of, for example, radioisotopes uptake, as seen in PET. This presents problems as the distriubutions may change from scanner to scanner or sequence to sequence, making it difficult to train on datasets acquired from multiple sites.\n",
        "\n",
        "Magnetic Resonance Image (MRI) is one example of (generally) non-quantitative scan which nonetheless has many strengths as a non invasive imaging modality, capable of distinguishing across many different types of tissue at many different scales.\n",
        "\n",
        "One approach to standardising MRI scans, to make images acquired across different scanners (but common protocols) make images more consistent is to perform histogram matching, so as to rescale image intensity distributions to match that of a standardised target distribution. In this notebook, We will go through the step by step process of histogram normalisation for a hippocampus dataset. \n",
        "  \n",
        "While histogram normalisation can be done on a case-by-case basis, given a dataset spanning a number of subjects we would like to normalise them all to a common space. This can be accomplished according to the details in [this](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.204.102&rep=rep1&type=pdf) paper, the main details of which we'll cover here.  \n",
        "\n",
        "The process is as follows: \n",
        "\n",
        "1. First chose a range of percentiles to map between, e.g.: 10, 20, 30, 40, ..., 90 \n",
        "2. Calculate intensity values at each percentiles for each of the \"train\" images, we will call these the landmarks\n",
        "3. Choose additional percentiles to act as the [min, max] of the range (e.g. here 5 and 95 (we assume below 5 and above 95 it represents noise). Scale the intensities corresponding to this range to the target (\"standard\") scale (e.g.: [0, 100]) using interpolation.\n",
        "4. Next apply the same scaling operation to each landmark\n",
        "5. Estimate landmark intensities for all images and average to get a target set of average landmark intensities (one for each percentile in our original list)\n",
        "6. Finally, take each image and scale its intensities to match the target (landmark) percentile scale \n",
        "\n",
        "You will want to read how these two functions work to complete the exercise:\n",
        "1. [np.percentile](https://docs.scipy.org/doc/numpy/reference/generated/numpy.percentile.html)\n",
        "2. [interp1d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html)\n",
        "\n",
        "Let's first begin by just loading in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rXw7xmw5T9I"
      },
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "\n",
        "def read_file(filename):\n",
        "    img = nib.load(filename)\n",
        "    data = img.get_fdata()\n",
        "    aff = img.affine\n",
        "    return data, aff\n",
        "\n",
        "# Build the 2D dataset: Let's take 20 images\n",
        "dataset = []\n",
        "data_dir = 'Task04_Hippocampus/imagesTr'\n",
        "images = os.listdir(data_dir)\n",
        "for entry in range(20):\n",
        "    image, _ = read_file(data_dir + '/' + images[entry])\n",
        "    # Let's take the middle slice and normalise\n",
        "    dataset.append(100 * image[:, :, 17] / np.max(image[:, :, 17]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdYYBoI35T9K"
      },
      "source": [
        "**To do 1.1** Using the step by step instructions above; starting from 1.2 \"Calculate intensity values at each percentiles for each of the \"train\" images\", implement histogram matching.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph79IkI25T9L"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# function for estimation of of landmark (intensities) to match a list of percentiles\n",
        "def calc_landmarks_from_percentiles(image, percentile_list):\n",
        "    '''\n",
        "    function for estimation of of landmark (intensities) to match a list of percentiles\n",
        "    input:\n",
        "      image - image array for which we wish to estimate intensity (landmarks)\n",
        "      percentile_list - list of percentiles e.g. [10,20,30....90] for which we want corresponding intensities\n",
        "\n",
        "      output: array containing the value of the intensity distribution for each percentile in the list\n",
        "    '''\n",
        "    landmarks = np.percentile(image, percentile_list)\n",
        "    return landmarks\n",
        "\n",
        "def create_trained_mapping(images, percentile_list, scale_min, scale_max, p_min, p_max):\n",
        "    '''\n",
        "    function for estimating average landmarks for target population\n",
        "    input:\n",
        "      images - list of all images in the target population\n",
        "      percentile_list - list of percentiles e.g. [10,20,30....90] for which we want corresponding intensities\n",
        "      scale_min - the minimum of the target range (where we wish to rescale intensities corresponding to the p_min and p_max percentile of the input to)\n",
        "      scale_max -maximum of the target range\n",
        "      p_min - the lowest percentile of the input that we consider true signal (and wish to map to a standard scale)\n",
        "      p_max  the maximumpercentile of the input that we consider true signal \n",
        "\n",
        "      output: array containing the value of the intensity distribution for each percentile in the list\n",
        "    '''\n",
        "    average_mapped_landmarks = np.zeros(len(percentile_list),)\n",
        "    for image in images:\n",
        "        # 1.2. Calculate the landmarks for each image\n",
        "        landmarks = #YOUR CODE HERE\n",
        "        # 1.3.  Calculate the image intensities corresponding to the o_min and p_max percentile\n",
        "        # of the disribution. These will anchor the mapping\n",
        "        intensity_min = #YOUR CODE HERE\n",
        "        intensity_max = #YOUR CODE HERE\n",
        "        # 1.4. Create mapping by interpolating between the image's intensity_min + intensity_min_max values\n",
        "        # to those of the standard scale [scale_min, _scale max]\n",
        "        # **hint** using interp1d to rescale from current range to target range\n",
        "        mapping = #YOUR CODE HERE\n",
        "        # 1.5. Map the image landmarks to these values (apply mapping to landmarks)\n",
        "        mapped_landmarks = #YOUR CODE HERE\n",
        "        # Sum the mapped landmarks iteratively\n",
        "        average_mapped_landmarks += mapped_landmarks\n",
        "    # Average the summed landmarks\n",
        "    average_mapped_landmarks = average_mapped_landmarks / len(images)\n",
        "    return average_mapped_landmarks\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KZnszq6uJMa"
      },
      "source": [
        "**To do 1.6** Now that we've coded the mapping training function, tcode a function that applies the learned mapping to a new image, and test the method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCRa0ONTuJsy"
      },
      "source": [
        "# 1.6. Code function to apply mapped landmarks to a new image\n",
        "def standardise_image(image, percentile_list, average_mapped_landmarks):\n",
        "    # 1.6.1 Calculate the landmarks corresponding to the *same* percentile list as above\n",
        "    # But now for the image you wish to standardise to the target distribution\n",
        "    # *Hint* will therefore need to apply same function as before\n",
        "    landmarks = #YOUR CODE HERE\n",
        "    # Calculate mapping between the landmarks of this image and the averaged mapped landmarks\n",
        "    # using intep1d (as before)\n",
        "    mapping = #YOUR CODE HERE\n",
        "    # Apply this mapping to the new image (as before)\n",
        "    new_image = #YOUR CODE HERE\n",
        "    return new_image\n",
        "\n",
        "\n",
        "# Test the method\n",
        "percentile_list = [10, 20, 30, 40, 50, 60, 70, 80, 90] # 1.1 - setting the target list of percentiles\n",
        "# estimating the average landamrks across the population\n",
        "# here using all but the first three images in the list; target range [0,100] p_min=5; p_max=95\n",
        "trained_mapping = create_trained_mapping(images=dataset[3:],\n",
        "                                         percentile_list = percentile_list,\n",
        "                                         scale_min=0, scale_max=100,\n",
        "                                         p_min=5, p_max=95)\n",
        "\n",
        "normalised_images = []\n",
        "for image in dataset[0:3]:\n",
        "    normalised_images.append(standardise_image(image, percentile_list, trained_mapping))\n",
        "\n",
        "# Before\n",
        "plot_images(dataset[0:3], figsize=(11, 11))\n",
        "plot_histograms(dataset[0:3], figsize=(11, 4))\n",
        "\n",
        "# making a rough approximation that the distributions are Gaussian to compare statistics before and after normalisation\n",
        "print('Means of original images:', np.mean(dataset[0]),np.mean(dataset[1]),np.mean(dataset[2]),'Variances:',np.var(dataset[0]),np.var(dataset[1]),np.var(dataset[2]))\n",
        "\n",
        "# After\n",
        "plot_images(normalised_images, figsize=(11, 11))\n",
        "plot_histograms(normalised_images, figsize=(11, 4))\n",
        "print('Means of normalised images:', np.mean(normalised_images[0]),np.mean(normalised_images[1]),np.mean(normalised_images[2]),'Variances:',np.var(normalised_images[0]),np.var(normalised_images[1]),np.var(normalised_images[2]),)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvWX8B4K5T9N"
      },
      "source": [
        "# Exercise 2. Augmentations\n",
        "Deep learning is very much data driven, and augmentations allow us to increase the size of our dataset aritifically by modifying the appearance of existing images, increasing the robustness of trained networks. We will go through and implement some of the most widely used augmentations in medical imaging:\n",
        "1. Flipping\n",
        "2. Rotations\n",
        "3. Bias field \n",
        "4. Noise\n",
        "5. Elastic field deformations  \n",
        "  \n",
        "### 2.1 Flipping and rotations\n",
        "These augmentations are self-explanatory, new images can be generated via horizontal/ vertical flipping and/ or rotating them by some number of degrees. Let's begin by looking at some un-augmented images from our brain dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzulNFZw5T9O"
      },
      "source": [
        "**To do** Let's implement a flip, a rotation, and both a flip and rotation on these images. Code a function for each and run on the sample images\n",
        "\n",
        "- 2.1.1 For the flip augmentation:\n",
        "  -  create a variable, *flip_prob*, that determines the probability that any axis in the image will be flipped.\n",
        "  - If a random generated number is above the threshold for thax axis flip it **hint** [Numpy flip](https://www.google.com/search?q=numpy+flip&oq=numpy+flip&aqs=chrome.0.69i59l2j0l5j69i60.1504j1j7&sourceid=chrome&ie=UTF-8)  \n",
        "- 2.1.1 For the rotation:\n",
        "  -  a *max_rot_angle* is passed which stipulates the maximum rotation in either direction (2D).\n",
        "  - implement rotation and resampling to the regular grid using [Scipy image rotate](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.rotate.html) (note the import has already been included at the top of the cell.)\n",
        "\n",
        "\n",
        "**Note (for future use)**: Interpolation has to be considered carefully when carrying out rotations. *scipy.ndimage.rotate* uses spline interpolation, using [piecewise](https://en.wikipedia.org/wiki/Piecewise) polynomials to interpolate between points. The *order* parameter sets the order of the polynomials that are used. By default this parameter is set to three (Corresponding to cubic splines), BUT this is not suitable when dealing with, for example, segmentation labels: In this case we will use an order zero spline, or *nearest neighbour* because we don't want any new values not found in the original to be introduced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHaeDA5h5T9O"
      },
      "source": [
        "import scipy.ndimage\n",
        "\n",
        "\n",
        "def flip_augmentation(image, flip_prob=0.5):\n",
        "    # 2.1.1 YOUR CODE HERE\n",
        "    # e.g. Calculate the dimensionality of the image (number of axes)\n",
        "    # Flip each axis according to probability flip_prob\n",
        "    # create an array with flip probabilities for each axis \n",
        "    #YOUR CODE HERE\n",
        "    return flipped_image\n",
        "\n",
        "\n",
        "def rotation_augmentation(image, max_rot_angle=180):\n",
        "    # Randomly pick a rotation angle from the range (-max, max)\n",
        "    # choose an angle in range +/- max_roatation angle a\n",
        "    # and rotate image by it using scipy.ndimage.rotate\n",
        "    # YOUR CODE HERE\n",
        "    return rotated_image\n",
        "\n",
        "sampled_images = random.sample(dataset, 3)\n",
        "flipped_example = flip_augmentation(sampled_images[0], flip_prob=1)\n",
        "rotated_example = rotation_augmentation(sampled_images[1], max_rot_angle=180)\n",
        "rotated_flipped_example = rotation_augmentation(flip_augmentation(sampled_images[2], flip_prob=1),\n",
        "                                                max_rot_angle=180)\n",
        "\n",
        "plot_images(sampled_images)\n",
        "plot_images([flipped_example, rotated_example, rotated_flipped_example],\n",
        "            title=['Flipped', 'Rotated', 'Rotated and Flipped'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61bIv7Az5T9T"
      },
      "source": [
        "### 2.2 Elastic field deformation augmentation\n",
        "Rotations and flips can only do so much to increase robustness, they do little to help the network generalise to images of structures with differing shapes. This is where elastic deformations can provide a performance boost. Deforming an image in this way will provide the network with a greater range of anatomical shapes which can be crucial when data is scarce.  \n",
        "\n",
        "The elastic field deformations in this notebook will be carried out using [b-spline](https://en.wikipedia.org/wiki/B-spline) transforms. A b-spline is a piecewise cubic polynomial function defined across a set of points. Think of an image as existing in a space defined by a square mesh. A b-spline transform transforms the mesh, changing the position of the points, altering the topology while ensuring that the lines connecting those points are smooth and fully differentiable.\n",
        "\n",
        "We will be using the [SimpleITK](http://www.simpleitk.org/) library to carry out this augmentation. It is an excellent open-source image analysis tool with many functions that will prove useful for this task. This code is based on [NiftyNet's](https://github.com/NifTK/NiftyNet) implementation.\n",
        "\n",
        "**To do 2.2 A** run below function and check carefully you know what each line does\n",
        "- Comment each line of create_elastic_deformation (**hint** it will help to reference the sitk documentation)\n",
        "- what does the resampler do (initialised line 47 `apply_elastic_deformation`)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW0R0JVa5T9U"
      },
      "source": [
        "import SimpleITK as sitk\n",
        "\n",
        "def create_elastic_deformation(image, num_controlpoints, sigma):\n",
        "    \"\"\"\n",
        "    We need to parameterise our b-spline transform\n",
        "    The transform will depend on such variables as image size and sigma\n",
        "    Sigma modulates the strength of the transformation\n",
        "    The number of control points controls the granularity of our transform\n",
        "    \"\"\"\n",
        "    #YOUR COMMENT HERE\n",
        "    itkimg = sitk.GetImageFromArray(np.zeros(image.shape))\n",
        "    #YOUR COMMENT HERE\n",
        "    trans_from_domain_mesh_size = [num_controlpoints] * itkimg.GetDimension()\n",
        "    #YOUR COMMENT HERE\n",
        "    bspline_transformation = sitk.BSplineTransformInitializer(itkimg, trans_from_domain_mesh_size)\n",
        "    #YOUR COMMENT HERE\n",
        "    params = np.asarray(bspline_transformation.GetParameters(), dtype=float)   \n",
        "    #YOUR COMMENT HERE\n",
        "    params = params + np.random.randn(params.shape[0]) * sigma\n",
        "    bspline_transformation.SetParameters(tuple(params))\n",
        "    return bspline_transformation\n",
        "\n",
        "\n",
        "def myshow(grid, fig_size=(10, 10)):\n",
        "    grid_array = sitk.GetArrayViewFromImage(grid)\n",
        "        \n",
        "    plt.figure()\n",
        "    plt.imshow(np.flip(grid_array, axis=0),\n",
        "               interpolation='hamming',\n",
        "               cmap='gray',\n",
        "               origin='lower')\n",
        "\n",
        "    \n",
        "def create_grid(image):\n",
        "    \"\"\"\n",
        "    Creates a grid to better visualise transform\n",
        "    \"\"\"\n",
        "    grid = sitk.GridSource(outputPixelType=sitk.sitkUInt16,\n",
        "    size=(image.shape[1], image.shape[0]),\n",
        "    sigma=(0.0001, 0.0001),\n",
        "    gridSpacing=(int(image.shape[1]/4), int(image.shape[0]/4)))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def apply_elastic_deformation(image, num_controlpoints=5, sigma=1):\n",
        "    # We need to choose an interpolation method for our transformed image, let's just go with b-spline\n",
        "    resampler = sitk.ResampleImageFilter()\n",
        "    resampler.SetInterpolator(sitk.sitkBSpline)\n",
        "    # Let's convert our image to an sitk image\n",
        "    sitk_image = sitk.GetImageFromArray(image)\n",
        "    sitk_grid = create_grid(image)\n",
        "    # Specify the image to be transformed: This is the reference image\n",
        "    resampler.SetReferenceImage(sitk_image)\n",
        "    resampler.SetDefaultPixelValue(0)\n",
        "    # Initialise the transform\n",
        "    bspline_transform = create_elastic_deformation(image, num_controlpoints, sigma)\n",
        "    # Set the transform in the initialiser\n",
        "    resampler.SetTransform(bspline_transform)\n",
        "    # Carry out the resampling according to the transform and the resampling method\n",
        "    out_img_sitk = resampler.Execute(sitk_image)\n",
        "    out_grid_sitk = resampler.Execute(sitk_grid)\n",
        "    # Convert the image back into a python array\n",
        "    out_img = sitk.GetArrayFromImage(out_img_sitk)\n",
        "    return out_img.reshape(image.shape), out_grid_sitk\n",
        "\n",
        "\n",
        "transformed_images = []\n",
        "transformed_grids = []\n",
        "for image_id in range(2):\n",
        "    trans_img, trans_grid = apply_elastic_deformation(dataset[image_id],\n",
        "                                                      num_controlpoints=3,\n",
        "                                                      sigma=2)\n",
        "    transformed_images.append(trans_img)\n",
        "    transformed_grids.append(trans_grid)\n",
        "plot_images(dataset[:2], figsize=(10,10))\n",
        "plot_images(transformed_images, title=['Deformed'], figsize=(10,10))\n",
        "plot_grids_and_images(transformed_grids, transformed_images, figsize=(10,10))\n",
        "plot_grids(transformed_grids, figsize=(10, 10))\n",
        "#myshow(transformed_grids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex0cGsCK5T9W"
      },
      "source": [
        "**To DO 2.2B: parameterising an elastic deformation**\n",
        "\n",
        "I would like you to investigate what range of sigma is reasonable for augmenting our dataset. Remember, augmentations serve to enrich the dataset with different but *realistic* versions of existing data. Consider plotting many transformed images at once, each with a different choice of sigma.\n",
        "\n",
        "Try modifying the number of control points: What do you observe? Is there a particular choice of number of control points that seems best suited for our images? Can you think of an example where a different choice for this variable might work better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giA1OPLX5T9Q"
      },
      "source": [
        "### Exercise 2.3 Noise augmentation\n",
        "Augmenting your data with noise will make a trained network more robust to noisy unseen images. It is important to tailor your noise augmentation to the imaging modality at hand. For example, MR images exhibit [Rician distributed noise](https://en.wikipedia.org/wiki/Rice_distribution) while CT images exhibit Gaussian distributed noise. You would therefore have different forms of noise augmentation depending on if your dataset consists of MR or CT images. \n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=12znggEB9vi2GpocLLhelPpFZgNUByN7F\" alt=\"Drawing\" width=\"300px;\"/>\n",
        "</figure>\n",
        "\n",
        "> __Figure__: If a point of distance V from the origin is selected, and points are generated indepdently following a Gaussian distribution then these points, if parameterised with respect to their distance to the origin, are Rician distributed. [Source](https://en.wikipedia.org/wiki/Rice_distribution) \n",
        "  \n",
        "  \n",
        "Let's augment our MR images with some Rician noise. A Rician distribution corresponds to a fourier transformed gaussian distribution. In fact, this is why MR images exhibit Rician noise, the MR signal accumulated in k-space boasts Gaussian noise, which manifests itself as Rician once the signal is converted into real space. The easiest way to implement it is to fourier transform our images, add Gaussian noise in k-space, then reverse Fourier transform the images back into real space.\n",
        "\n",
        "It is important to note that the MR signal is composed of a *real* and *imaginary* part (See: [Real Vs Imaginary](http://mriquestions.com/real-v-imaginary.html)), a combination of two channels during the acquisition process. The noise in each channel is independant, and our code should reflect that. \n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1K1M2YiMMg0WclvVHuINNOMLG_ir7rPxz\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "> __Figure__: Examples of *real*, *imaginary*, and *magnitude* images. [Source](http://mriquestions.com/real-v-imaginary.html) \n",
        "\n",
        "\n",
        "**To do** Complete coding of the noise augmentation (the `mr_noise_augmentation` function). This should:\n",
        "1. Given an input `image`  Fourier transform it (see guidance on `np.fft` below)\n",
        "2. Then, pass it to the *add_complex_noise* function (which returns a noisy k-space image), \n",
        "3. transform it back into real space through inverse fourier transform\n",
        "4. Return a magnitude image. Note for complex number $z=a +ib$, where $a$ is the real component and $b$ is the imaginary component, the magnitude of $|z|=\\sqrt{a^2 +b^2}$ just as for any other 2D vector space.\n",
        "\n",
        "[The fft routine of numpy](https://docs.scipy.org/doc/numpy/reference/routines.fft.html) should prove very useful for this task (You certainly do not want to be coding a Fourier transform from scratch!).\n",
        "\n",
        "**Note**: In reference to `add_complex_noise` (line 2). It is common to work in decibels when dealing with noise. Decibels are measured on a logarithmic scale. This makes it the unit of choice because on a linear scale noise can vary by many orders of magnitude. You can read more about the decibel scale [here](https://pulsarinstruments.com/en/post/understanding-decibels-decibel-scale-and-noise-measurement-units) if you'd like a few more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwPQZv1q5T9R"
      },
      "source": [
        "def add_complex_noise(inverse_image, noise_level):\n",
        "    # Convert the noise from decibels to a linear scale: See: http://www.mogami.com/e/cad/db.html\n",
        "    noise_level_linear = 10 ** (noise_level / 10)\n",
        "    # Real component of the noise: The noise \"map\" should span the entire image, hence the multiplication\n",
        "    real_noise = np.sqrt(noise_level_linear / 2) * np.random.randn(inverse_image.shape[0],\n",
        "                                                                   inverse_image.shape[1])\n",
        "    # Imaginary component of the noise: Note the 1j term\n",
        "    imaginary_noise = np.sqrt(noise_level_linear / 2) * 1j * np.random.randn(inverse_image.shape[0],\n",
        "                                                                             inverse_image.shape[1])\n",
        "    noisy_inverse_image = inverse_image + real_noise + imaginary_noise\n",
        "    return noisy_inverse_image\n",
        "\n",
        "\n",
        "def mr_noise_augmentation(image, noise_level):\n",
        "    # 2.3.1 Fourier transform the input image\n",
        "    inverse_image = # YOUR CODE HERE\n",
        "    # 2.3.2 Add complex noise to the image in k-space\n",
        "    inverse_image_noisy = # YOUR CODE HERE\n",
        "    # 2.3.3 Reverse Fourier transform the image back into real space\n",
        "    complex_image_noisy = # YOUR CODE HERE\n",
        "    # 2.3.4 Calculate the magnitude of the image to get something entirely real\n",
        "    magnitude_image_noisy = # YOUR CODE HERE\n",
        "    return magnitude_image_noisy\n",
        "\n",
        "\n",
        "noise_augmented_images = []\n",
        "for image_id in range(3):\n",
        "    noise_augmented_images.append(mr_noise_augmentation(dataset[image_id], noise_level=60))\n",
        "plot_images(dataset[:3])\n",
        "plot_images(noise_augmented_images, title=['Noisy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbofzZs65T9T"
      },
      "source": [
        "Select different noise levels and choose different images. Does noise affect regions differently? What is the maximum tolerable noise level, qualitatively?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5fm6mqd5T9X"
      },
      "source": [
        "# Coding space (If you need it)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzlcEwKd5T9Z"
      },
      "source": [
        "## Exercise 2.3 Bias field augmentation\n",
        "\n",
        "MRI images will sometimes exhibit a form of corruption termed *bias* or *intensity inhomogeneities*. They arise as a result of imperfections in the imaging pipeline, and cause low-frequency intensity changes across the image as shown in the figure below:\n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1ZlpXmBgSKE6U8qVsmtVtsQs_OcyCiX3Z\" alt=\"Drawing\" width=\"500px;\"/>\n",
        "</figure>\n",
        "\n",
        "> __Figure__: Coronal brain slice corrupted by a bias field (Left). Corresponding bias field map (Right).  \n",
        ">[A review of methods for correction of intensity inhomogeneity in MRI.](https://www.ncbi.nlm.nih.gov/pubmed/17354645)  \n",
        "  \n",
        "As such, the robustness of a network can be boosted by augmenting images with bias. A bias field can be modelled by the summation of polynomials up to order $N$, where $N$ is the dimensionality of the image (e.g.: $N=2$ for a $2D$ image). For $2D$, the combinations afforded to you (assuming parameterisation by x and y) are:  \n",
        "  <center> $A$, $Bx$, $Cy$, $Dxy$, $Ex^2$, $Fy^2$ </center>\n",
        "\n",
        "Where $A$, $B$, $C$, $D$, $E$, and $F$ denote random coefficients (The strength of the bias field is modulated by the magnitude of these coefficients).  \n",
        "\n",
        "Note that the polynomials are capped at order 2 for a 2D image but the bias can be lower dimensional (i.e.: The bias field may have components in only x or only y, not necessarily both). \n",
        "\n",
        "**To do** \n",
        "\n",
        "- Go through this code line by line and check you understand what it is doing.\n",
        "Experiment with the bias field strength. \n",
        "- Why would a bias field pose big challenges for medical imaging tasks (e.g.: Segmentation)? what about registration?. Answer in text box below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIREDPCacGYR"
      },
      "source": [
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeoPmQX-5T9Z"
      },
      "source": [
        "# Bias field augmentation\n",
        "# We're dealing with 2D images here\n",
        "\n",
        "def apply_bias_field(image, BF_strength):\n",
        "    # First of all, Calculate the dimensionality of the image\n",
        "    max_order = len(image.shape)\n",
        "    # Initialise the bias map as an array of zeros\n",
        "    bias_map = np.zeros(image.shape)\n",
        "\n",
        "    # we now need to create a meshed grid for x and y that span the image size\n",
        "    # These are essentially just grids that denote coordinates of the image:\n",
        "        # The xmesh has values increasing along x, repeated over y\n",
        "        # The ymesh has values increasing along y, repeated over x\n",
        "\n",
        "    # Array containing all values of x needed to span image in x\n",
        "    x_range = np.arange(-image.shape[0]/2, image.shape[0]/2)\n",
        "    # Array containing all values of y needed to span image in y\n",
        "    y_range = np.arange(-image.shape[1]/2, image.shape[1]/2)\n",
        "    # Create then normalise the meshes (Otherwise the bias will be too large)\n",
        "    x_mesh, y_mesh = np.meshgrid(x_range, y_range)\n",
        "    x_mesh = x_mesh / np.max(x_mesh)\n",
        "    y_mesh = y_mesh / np.max(y_mesh)\n",
        "    \n",
        "    # Loop through the polynomial orders for each dimension up to max_order\n",
        "    for x_order in range(max_order + 1):\n",
        "        # Note: The combined polynomials can't exceed max_order, therefore ensure that y is constrained by x_order\n",
        "        for y_order in range(max_order + 1 - x_order):\n",
        "            # Randomly sample a coefficient for this polynomial term\n",
        "            bf_coeff = np.random.uniform(-BF_strength, BF_strength)\n",
        "            # Create the component of the bias produced by this polynomial\n",
        "            component_bias = bf_coeff * np.power(x_mesh, x_order) * np.power(y_mesh, y_order)\n",
        "            # Add the component bias to the bias_map\n",
        "            bias_map += component_bias.T\n",
        "    # Finally, multiply the image with the exponentiated bias map\n",
        "    # Exponentiation ensures that if the BF_strength is zero then the function returns the original image\n",
        "    bias_image = image * np.exp(bias_map)\n",
        "    return bias_image, bias_map\n",
        "\n",
        "\n",
        "# Visualisation example\n",
        "image_id = np.random.randint(len(dataset))\n",
        "image = dataset[image_id]\n",
        "bias_image, bias_map = apply_bias_field(image, 0.75)\n",
        "plot_images([image, bias_image, bias_map], title=['Original', 'Biased', 'Bias map'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fHxiv45T9c"
      },
      "source": [
        "# 3 Dealing with 3D data\n",
        "One of the unique challenges posed by medical images is their dimensionality; oftentimes we have datasets comprised of 3D volumes that we want to train networks on to extract useful information, but it is impossible to fit the entirety of the image into memory. We'll go over some methods of circumventing this limitation in this section.\n",
        "\n",
        "### 2.5D data\n",
        "An in-between compromise between 2D and 3D data is to resort to train your networks with \"2.5D\" data. There are two different ways of setting up 2.5D data:\n",
        "1. Stacking multiple 2D slices of a 3D image\n",
        "2. Taking 2D slices of three perpendicular planes of a 3D image\n",
        "\n",
        "In the first instance you might still be restricted to a fairly small number of slices since there are increased resource requirements, though you would be able to employ 3D kernels to your data, thus gaining greater context awareness. For the second form you could train separate networks, each with one of the planar slice types, and combine the outputs, affording you a better prediction.  \n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1j-BWu3SRERirdQNvo_YCtTEc4M1tt78t\" alt=\"Drawing\" width=\"700px;\"/>\n",
        "</figure>\n",
        "\n",
        "> __Figure__: Example illustrating extraction and use case of 2.5D images for a segmentation task  \n",
        ">[Deep Feature Learning for Knee Cartilage\n",
        "Segmentation Using a Triplanar Convolutional\n",
        "Neural Network](https://link.springer.com/chapter/10.1007/978-3-642-40763-5_31)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzMLe57R5T9c"
      },
      "source": [
        "### **Exercise 3.1 Tackling the problems associated with 3D data: Patches**\n",
        "While 2.5D methods do have their uses, they are no substitute for training in full 3D. As such, it is worthwhile devising techniques that allow us to make full use of the additional dimension. In a manner not entirely dissimilar to the first form of 2.5D, we can sample sections from 3D volumes to obtain a \"patch\". Patches are typically isotropic and it is typically beneficial for them to be as large as the hardware will allow to maximise contextual information. However, the larger the patch the smaller the batch size has to be to account for the increased memory requirements. As such, it is often appropriate to compromise on the patch size if that allows for a significant relative increase in the size of the batch (e.g.: While you might be able to fit a patch of size 64 with batch size one, it might be worth it to decrease the patch to 56 if that meant that you can therefore double the size of the batch).\n",
        "\n",
        "**To do** Design a simple sampler. It should take in an image and return a patch sampled from that image of size (20, 20, 20) through randomly sampling the initial coordinates in an appropriate range. \n",
        "\n",
        "**Note** You'll need to calculate constraints based on the size of each axis and the size of the patch - your patch must fit fully within your image. \n",
        "\n",
        "Complete the function below by:\n",
        "1. Define the max range of coordinates - the maximum value for each axis in which a patch can start\n",
        "2. Randomly selecting the starting position of the patch for each of the coordinate axis. \n",
        "3. Define the endpoints of the patch \n",
        "\n",
        "Your function should output the 3D patch, as well as a full image with the patch area highlighted. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXbPyc7s5T9d"
      },
      "source": [
        "# Load in the data in 3D\n",
        "dataset_3D = []\n",
        "data_dir = '../Task04_Hippocampus/imagesTr'\n",
        "labels = os.listdir(data_dir)\n",
        "for entry in range(20):\n",
        "    image, _ = read_file(data_dir + '/' + labels[entry])\n",
        "    dataset_3D.append(100 * image / np.max(image))\n",
        "\n",
        "# Begin coding below\n",
        "def sample_3d_patch(image, patch_size=20):\n",
        "    import copy\n",
        "    shape = image.shape      \n",
        "    # 3.1.1 define the max range of coordinates for sampling from\n",
        "    # you need to calculate constraints based on the size of each axis and the size of the patch - your patch must fit fully within your image.\n",
        "    max_coords = # YOUR CODE HERE\n",
        "    starting_patch_coords = np.zeros(len(shape), dtype=np.int32)\n",
        "    # 3.1.2 Randomly pick a starting point for the patch, \n",
        "    #by filling in starting_patch_coords for each dimension\n",
        "    #(estimated 3 lines)\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # 3.1.3 Define the end_patch_coords given your starting coords\n",
        "    end_patch_coords = # YOUR CODE HERE\n",
        "    image_patch = image[starting_patch_coords[0]:end_patch_coords[0],\n",
        "                        starting_patch_coords[1]:end_patch_coords[1],\n",
        "                        starting_patch_coords[2]:end_patch_coords[2]]\n",
        "    highlighted_image = copy.deepcopy(image)\n",
        "    highlighted_image[starting_patch_coords[0]:end_patch_coords[0],\n",
        "                      starting_patch_coords[1]:end_patch_coords[1],\n",
        "                      starting_patch_coords[2]:end_patch_coords[2]] *= 7.5\n",
        "    return image_patch, highlighted_image, starting_patch_coords\n",
        "\n",
        "# Extract and visualise three patches for three images in the dataset\n",
        "for image_id in range(1):\n",
        "    image = dataset_3D[image_id]\n",
        "    image_patch, highlighted_image, starting_patch_coords = sample_3d_patch(image)\n",
        "    print(image_patch.shape,highlighted_image.shape,highlighted_image[starting_patch_coords[0], :, :].shape)\n",
        "    plot_images([image[starting_patch_coords[0], :, :],\n",
        "                 image[:, starting_patch_coords[1], :],\n",
        "                 image[:, :, starting_patch_coords[2]]])\n",
        "    plot_images([highlighted_image[starting_patch_coords[0], :, :],\n",
        "                 highlighted_image[:, starting_patch_coords[1], :],\n",
        "                 highlighted_image[:, :, starting_patch_coords[2]]])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEdnxPMQ5T9f"
      },
      "source": [
        "### **Exercise 3.2 Sampling patches**\n",
        "We have been sampling our images randomly, not favouring any part of the anatomy. In practice however it might prove beneficial to force your sampler to sample more frequently from certain areas to minimise background and/or to increase the emphasis on regions that are particularly pertinent for the network to focus on. This can be accomplished with a weighted sampler.\n",
        "\n",
        "Weighted samplers often rely on a weight map, an array the same size as the image populated by values denoting the sampling importance of any particular voxel. For a brain image, the corresponding weight map might be filled with ones in the intra-cranial region and with zeros outside of it. You would then design a sampler that takes in the image and the map and outputs patches that favour the non-zero weight regions. \n",
        "\n",
        "Let's create one such sampler for our hippocampus dataset. We have some hippocampus segmentations which we'll load in and use as our weight maps. The hippocampus is a fairly small region, so even in our zoomed in brain images it will take up no more than 10% of the volume. \n",
        "\n",
        "You will be implementing the sampler. There are many approaches to creating a sampler but let's start with something simple. \n",
        "\n",
        "**To do** \n",
        "- Create patches comprised by at least 20% hippocampus (as indicated by the segmentation map) \n",
        "- Assume an isotropic patch size of 20, as before. \n",
        "- one time costly but straightforward approach is to repeatedly sample patches until the criteria has been met.\n",
        "- start from your existing sampler code and adapt it from there. \n",
        "- for qualitative purposes your function should also output the corresponding patch extracted from the label map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMycOTHL5T9g"
      },
      "source": [
        "# Load in the hippocampus labels: The labels should match with the 3D dataset 1:1\n",
        "labels_dataset = []\n",
        "data_dir = '../Task04_Hippocampus/labelsTr'\n",
        "labels = os.listdir(data_dir)\n",
        "for entry in range(20):\n",
        "    label, _ = read_file(data_dir + '/' + labels[entry])\n",
        "    label[label>1] = 1\n",
        "    labels_dataset.append(label)\n",
        "\n",
        "# Begin coding below\n",
        "def sample_weighted_3d_patch(image, weight_map, patch_size=20, acceptance_threshold=0.2):\n",
        "    import copy\n",
        "    patch_rejection = True\n",
        "    while patch_rejection:\n",
        "       # YOUR CODE HERE\n",
        "       \n",
        "# Extract and visualise three patches for three images\n",
        "# Label visualisation is included for qualitative sanity check\n",
        "for image_id in range(1):\n",
        "    image = dataset_3D[image_id]\n",
        "    weight_map = labels_dataset[image_id]\n",
        "    highlighted_image, highlighted_label, starting_patch_coords = sample_weighted_3d_patch(image, weight_map)\n",
        "    plot_images([highlighted_image[starting_patch_coords[0], :, :],\n",
        "                 highlighted_image[:, starting_patch_coords[1], :],\n",
        "                 highlighted_image[:, :, starting_patch_coords[2]]])\n",
        "    plot_images([highlighted_label[starting_patch_coords[0], :, :],\n",
        "                 highlighted_label[:, starting_patch_coords[1], :],\n",
        "                 highlighted_label[:, :, starting_patch_coords[2]]])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmLnUO9r5T9i"
      },
      "source": [
        "In practice this method would be quite inefficient since it relies on discarding patches that do not meet the selection criteria until one is generated that does: Can you think of better methods? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWJQMY0XvMcd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}