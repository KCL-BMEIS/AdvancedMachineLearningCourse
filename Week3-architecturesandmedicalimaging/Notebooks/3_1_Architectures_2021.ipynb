{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "3.1.Architectures_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2rYrsy8ZS6_"
      },
      "source": [
        "#Week 3 Part 1: CNN Architectures\n",
        "\n",
        "Tutorial by Cher Bass\n",
        "(edited by Emma Robinson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZTfHcpjZS7B"
      },
      "source": [
        "Let's start by importing the modules and Data that we need for the notebook. We start by testing on the MNIST dataset as before.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLIo0NQxZS7C"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #contains some useful functions like activation functions & convolution operations you can use\n",
        "\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# This is used to transform the images to Tensor and normalize it\n",
        "transform = transforms.Compose(\n",
        "   [transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "training = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                       download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(training, batch_size=8,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "testing = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                      download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testing, batch_size=8,\n",
        "                                        shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('0', '1', '2', '3',\n",
        "          '4', '5', '6', '7', '8', '9')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3qRbbd-77Lr"
      },
      "source": [
        "**To DO** Now set your device to cuda (if you have access to a GPU) or cpu otherwise \n",
        "\n",
        "**hint** see lecture 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnG7tbGs7_6O"
      },
      "source": [
        "#STUDENTS TO DO \n",
        "device = None \n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK-5lOHjZS7F"
      },
      "source": [
        "## ResNet with pytorch\n",
        "\n",
        "ResNet was first introduced in 2015 as a way to support training of deeper networks through supporting networks in learning identity mappings during training. It does this through implementation of residual blocks\n",
        "\n",
        "An example of a resnet block (from the original [2015 paper](https://arxiv.org/abs/1512.03385)) is illustrated below (see [image source](t)):\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1NQ_sLsu0GsXQsVEQ9Rtm5Mnvm2BuucCZ\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "<figcaption align = \"centre\"> Fig 1. ResNet 2015 residual block  </figcaption>\n",
        "</figure>\n",
        "\n",
        "Here, input data passes down two paths. In one, it is passed through two convolutional (weights learning) layers; in the other it skips these out to be added to the output of these layers. This shortcut operation is the identity mapping. If there are no gains to be made by learning more weights kernels (the nework is already deep enough); then the network can simply learn to pass the input unchanged through the block (an idenity transform) by pushing these weights kernels to zero.\n",
        "\n",
        "### Using PyTorch implementation\n",
        "\n",
        "Torchvision offers some default implementations of popular networks\n",
        "\n",
        "For example the following pretrained resnets models can be loaded in Pytorch:\n",
        "```python\n",
        "import torchvision\n",
        "torchvision.models.resnet18(pretrained=True, **kwargs)\n",
        "```\n",
        "\n",
        "You can also load a model that hasn't been pretrained in the following way:\n",
        "```python\n",
        "torchvision.models.resnet18(pretrained=False, **kwargs)\n",
        "```\n",
        "\n",
        "To see more examples, including networks such as ResNet, Alexnet, VGG, Densenet, see [torchvision models](https://pytorch.org/docs/stable/torchvision/models.html) and, for usage, see the official [tutorial](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
        "\n",
        "However, these pretrained models will not always suit your needs. For example, the resnet models are designed for 3 channel input  (i.e. for RGB natural channels); this means that you can't use them without adjustment on grayscale images, or on 3D medical data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHtb1_mKZS7F"
      },
      "source": [
        "## **Exercise 1 Programming your own ResNet**\n",
        "\n",
        "### **Coding the residual block**\n",
        "\n",
        "The first thing we need to do is implement a `BasicBlock` class, which will implement a single ResNet (2015) block, which includes the following steps (see Fig 1): \n",
        "\n",
        "1. **(strided) Convolution, followed by batchnorm**, followed by relu: with option to downsample through stride=2 and increase the number of output channels\n",
        "2. **Convolution, followed by batchnorm:** stride 1; input and output channels constant\n",
        "3. **shortcut step**, where the input is first transformed through a strided $1 \\times 1$ convolutional operation to match the dimensions of the output of the residual block and then added to the output of the convolutions. \n",
        "4. **relu**\n",
        "\n",
        "Note, **only the first convolution of each block offers the option of upsampling the channel dimension and downsampling the data through striding**. Further, several residual blocks are typically changed together between downsampling steps (see lilac, green and red groups); therefore downsampling is not implemented for all blocks.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1SVmOrg7uxRowWQNtr5jWmz2re9fLID4Q\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "### **Ex 1.1 - Create the Residual  block**\n",
        "\n",
        "The most challenging bit of coding up a residual block is implementing the reshapeing of the shortcut step. So let's start by ignoring it to create the main body of the residual block. This will work **_provided we maintain input dimensions_**. \n",
        "\n",
        "Let us create a `ResidualBlock` and define (parametrise) the required `Conv2d` and `BatchNorm2d` steps in the constructor `__init__(self, channels1,channels2,res_stride=1)`; here `res_stride` is the intended stride, `channels1` are the number of channels of the incoming activations, and `channels2` is the number of output channels. Note, `res_stride`=1 by default and this should only change if this is intended as a downsampling block;\n",
        "\n",
        "Note, biases are set to `False` in the block as they are instead handled by the batchnorm layer [see](https://discuss.pytorch.org/t/why-does-the-resnet-model-given-by-pytorch-omit-biases-from-the-convolutional-layer/10990/2). Also, observe that the Relu layer is implemented in the forward pass function.\n",
        "\n",
        "**To do 1.1 A** Tasks: Edit (`__init__`) to input\n",
        "\n",
        "1. `self.conv1` a 2D convolution with the power to: a) downsample spatial dimensions (with stride `res_stride` ); and b) upsample channel dimensions (to `channels2`). Set arguments `kernel_size=3, stride=res_stride, padding=1, bias=False`\n",
        "2. ` self.bn1` a 2D batchnorm layer to follow the first convolutional layer. What does it expect for the number of input features (`num_features`)?\n",
        "3. `self.conv2` the second convolutional layer. What should its stride, input and output channel dimensions be given **only the first convolution can change output dimensions**? (set `kernel_size=3, padding=1, bias=False` as before)\n",
        "4. ` self.bn2` a 2D batchnorm layer to follow the second convolutional layer. **Note, a different batch normalisation instance is needed each time as each stores learnable parameters**.\n",
        "\n",
        "For PyTorch documentation, see [nn.Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) and [nn.BatchNorm2d](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d)\n",
        "\n",
        "**To do 1.1 B** Edit `forward(self, x)` line 35 to **implement the shortcut**. Here the identity mapping `self.shortcut(x)` must be _added_ to the output of the weights layers. \n",
        "\n",
        "Here, the operations of the identity mapping `self.shortcut(x)`  depends on whether this is a downsampling block or not. In this first instance, we assume `res_stride` $= 1$ and thus the shape of the output tensor is the same as the input tensor. Thus  `self.shortcut=nn.Sequential()`, an empty sequential block which outputs the identity mapping `self.shortcut(x)=x`.\n",
        "\n",
        "**Make sure you understand what all lines of the forward function are doing**. Note, that at line 29 the output of the first operation is assigned to variable `out` in order to preserve the input `x` for the shortcut (identity) mapping.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Six1QUZS7G"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels1,channels2,res_stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.inplanes=channels1\n",
        "        # Exercise 1.1 construct the block (without shortcut -> Ex 1.3)\n",
        "        # implement conv1 (which option for reshaping), conv2 (no reshaping) and 2 batchnorm layers to insert between each\n",
        "        # 4 lines\n",
        "        None\n",
        "\n",
        "        if res_stride != 1 or channels2 != channels1:\n",
        "            # Exercise 1.3 the shortcut (supports resizing input during identity mapping)\n",
        "            # create an nn.Sequential block with one 1x1 conv2D and one batchnorm\n",
        "            # using res_stride to change spatial dimensions and channel 2 to change channel dimensions\n",
        "            # again bias must be set to False\n",
        "            self.shortcut=None\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut=nn.Sequential()\n",
        "            \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # forward pass: Conv2d > BatchNorm2d > ReLU > Conv2D >  BatchNorm2d > ADD > ReLU\n",
        "        out=self.conv1(x)\n",
        "        out=self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        # Exercise 1.1B STUDENT TO Do - IMPLEMENT THE shortcut (1 line)\n",
        "        None\n",
        "\n",
        "        # final ReLu\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwwGqKxtZS7I"
      },
      "source": [
        "\n",
        "\n",
        "### **Ex 1.2 . Perform a test forward pass ( <font color=\"red\"> _keeping input and output dimensions constant_ </font> )**\n",
        "\n",
        "1. Instantiate an instance of class ResidualBlock (create a network called `blk`) with input chananels = 3 and output channels=3; leave `res_stride` as default (1). **We have not implemented a shortcut with downsampling yet** so running with stride will fail\n",
        "2. create a random tensor of size $5 \\times 3 \\times 100 \\times 100$ (which matches expected input dimensions $N,C_{in},H,W$ the expected input channel dimensions of 3\n",
        "3. Pass the input through a forward pass and print input and output shape.\n",
        "\n",
        "**hint** look at how this was done in previous training loops. Remember - you don't need to explicitely call the forward function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0VpRV0_ZS7I"
      },
      "source": [
        "##  Student To do \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzz_Pw49ZS7K"
      },
      "source": [
        "### **Ex 1.3. Implement the shortcut**\n",
        "\n",
        "Next, let's go back and implement a shortcut with downsampling. \n",
        "\n",
        "Specfically, **if this is a reshaping residual block** (`channels2` $\\ne$ `channels1` and `res_stride` $\\neq$ 1) then **we will also need to reshape the input as it is passed through the shortcut**. This is the objective of this task.\n",
        "\n",
        "**To do:** Go back and edit the `ResidualBlock` constructor to complete the shortcut function, which will downsample the input as it is passed through the shortcut. \n",
        "\n",
        "**Change line 14 in `ResidualBlock.__init__()` to implement a nn.Sequential block with two steps**:\n",
        "1. A $1 \\times 1 $ `nn.Conv2d` layer with `stride=res_stride,bias=False`.  This will support changes of spatial dimensions through strided convolutions and changes of feature dimensions through $1 \\times 1 $ convolutions. What should your input and output channels be to make it equivalent to the output of a _reshaping_ residual block?\n",
        "2. batch normalisation. Think carefully about the input dimension. \n",
        "\n",
        "**To Do** Once you have done this, test the network again, but this time **implmnent stride of 2 and change the number of output channels**\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNZthYp9ZS7L"
      },
      "source": [
        "# STUDENTS TO DO\n",
        "# forwards pass with stride 2; change channels from 3 to 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylG1jIvOZS7M"
      },
      "source": [
        "We are now have all the building blocks we need to build a residual network. In what follows we will construct a ResNet with four residual layers. Each layer will contain 2 residual blocks. \n",
        "\n",
        "### **Ex 1.4 : Create a Residual Network class**\n",
        "\n",
        "In the original paper the network starts with a  convolutional layer with a $7 \\times 7 $ kernel, followed by a batchnorm. However, as we intend to test on the MNIST (which is very small) lets change the $7 \\times 7 $ kernel to a $3 \\times 3 $. We will implement 4 residual layers (or blocks), where the residual block class is passed to the network class as the argument `block`, and output channels and strides for each block are parametrised by lists (also past to the constructor) as `num_features` and `num_strides` respectively.\n",
        "\n",
        "**Step 1 To do 1.4 A** \n",
        "- Initialise the network with a 3 x3 convolutional layer, with input channels = `in_channels`, output channel = `num_features[0]`,  stride = `num_strides[0]`, padding=1 and bias false; \n",
        "- implement a batch normalisation layer to follow this.\n",
        "\n",
        "**Step 2 To Do 1.4 B** \n",
        "\n",
        "- Comment the function `_make_layer`. What is each line doing? \n",
        "- make sure you understand how this is used to create residual blocks in the constructor \n",
        "\n",
        "**Step 4 To Do 1.4 C** \n",
        "\n",
        "- The penultimate layer of the network is an average pool (line 40) which averages over spatial dimensions to return a flattened vector of length equal to the number of channels of the tensor passed to it. \n",
        "- the network must output 10 class predictions\n",
        "- Bearing that in mind implement the final linear layer of the network \n",
        "\n",
        "**hint** if you remain unsure you can always print the shape of all the tensors in the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAryRi7PZS7N"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_strides, num_features, in_channels, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = num_features[0] # in_planes stores the number channels output from first convolution\n",
        "        #STUDENT TO DO 1.4A replace None (2 lines)\n",
        "        # step 1. Initialise the network with a 3 x3 conv and batch norm\n",
        "        None\n",
        "\n",
        "        # Step 2: TO DO Using function make_layer() create 4 residual layers\n",
        "        # num_blocks per layer is given by input argument num_blocks (which is an array)\n",
        "        self.layer1 = self._make_layer(block, num_features[1], num_blocks, stride=num_strides[1])\n",
        "        self.layer2 = self._make_layer(block, num_features[2], num_blocks, stride=num_strides[2])\n",
        "        self.layer3 = self._make_layer(block, num_features[3], num_blocks, stride=num_strides[3])\n",
        "        self.layer4 = self._make_layer(block, num_features[4], num_blocks, stride=num_strides[4])\n",
        "        #STUDENT TO DO 1.4 C create Linear layer\n",
        "        self.linear = None\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        layers = []\n",
        "        #STUDENT TO DO 1.4B -comment\n",
        "        layers.append(block(self.in_planes, planes, stride))\n",
        "\n",
        "        for i in np.arange(num_blocks):\n",
        "            layers.append(block(planes, planes))\n",
        "        \n",
        "        self.in_planes = planes \n",
        "              \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # initial convolution and batch norm\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        #residual blocks \n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        \n",
        "        #average pool (flattens spatial dimensions)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)       \n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPPybgE3ZS7T"
      },
      "source": [
        "### **1.5:  Train on MNIST for classification**\n",
        "\n",
        "Below we have created an instance of our resent class which runs four levels of residual blocks, with 2 blocks in each group\n",
        "\n",
        "**To do** \n",
        "- create a suitable loss function for classification\n",
        "- create an SGD optimiser with momentum, assign learning rate as 0.001\n",
        "- complete the training function - **don't forget to set runtime to GPU and to push input data and labels (from each batch) to the device**\n",
        "- test performance of your network by running the validation code in the cells below\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0qKNTKVZS7T"
      },
      "source": [
        "#-----------------------------------------------------task 4 -----------------------------------------------------\n",
        "# Task 4: Train and test ResNet on MNIST dataset for classification\n",
        "# hints: define your resnet network, loss function, optimizer and dataloaders. \n",
        "# Then you can run the same training and testing code as above.\n",
        "# ----------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW8AkHTeZS7V"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# \n",
        "resnet = ResNet(ResidualBlock,2, [1,1,2,2,2], [64,64,128,256,512], in_channels=1)\n",
        "# see how the network is loaded to the device (cpu or GPU)\n",
        "# this allows the optimisation to be run on GPU\n",
        "resnet = resnet.to(device) \n",
        "# Ex 1.5 create suitable loss function\n",
        "loss_fun = None\n",
        "# Ex 1.5 create SGD optimiser with. momentunm\n",
        "optimizer = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ybhLDChZS7X"
      },
      "source": [
        "epochs = 1\n",
        "for epoch in range(epochs): \n",
        "\n",
        "    # enumerate can be used to output iteration index i, as well as the data \n",
        "    for i, (data, labels) in enumerate(train_loader, 0):\n",
        "        # STUDENTS TO DO 1.5 complete training loop\n",
        "        None \n",
        "        \n",
        "        # print statistics of loss tensor\n",
        "        ce_loss = loss.item()\n",
        "        if i % 10 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch + 1, i + 1, ce_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7XU-rO3ZS7Z"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#make an iterator from test_loader\n",
        "#Get a batch of testing images\n",
        "test_iterator = iter(test_loader)\n",
        "images, labels = test_iterator.next()\n",
        "\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "y_score = resnet(images)\n",
        "# get predicted class from the class probabilities\n",
        "_, y_pred = torch.max(y_score, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
        "rows = 2\n",
        "columns = 4\n",
        "# plot y_score - true label (t) vs predicted label (p)\n",
        "fig2 = plt.figure()\n",
        "for i in range(8):\n",
        "    fig2.add_subplot(rows, columns, i+1)\n",
        "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
        "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
        "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZEbMq6mZS7b"
      },
      "source": [
        "y_true = labels.data.cpu().numpy()\n",
        "y_pred = y_pred.data.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6WF5Vk9ZS7d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4g1Y77qZS7f"
      },
      "source": [
        "## **(optional) Exercise 2 - Use ResNet for classification - CIFAR10**\n",
        "\n",
        "Use the torch inbuilt ResNet for RBG images and train for classification on the CIFAR10 dataset.\n",
        "\n",
        "Here are some example images (from [source]( https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/)) from the CIFAR10 datasets - we have 10 classes:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1C_rcRMzjw3V-nnv18WDoHKuvTxKPhLwn\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "You can load the CIFAR10 dataset using torchvision in the following way:\n",
        "```python\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "```\n",
        "You can use [this tutorial]( https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) as a reference for training on CIFAR10 -\n",
        "\n",
        "Remember to define your loss function, optimizer, dataloaders, and your resnet network. \n",
        "Then run the training and testing, same as with MNIST.\n",
        "\n",
        "**To do 2.1 ** First, run the below cell to import the PyTorch ResNet; push it to the device (for GPU training)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkhGLE-6ZS7g"
      },
      "source": [
        "from torchvision import models\n",
        "\n",
        "resnet_cifar = models.resnet18(pretrained=True)\n",
        "resnet_cifar = resnet_cifar.to(device) # note how network is passed to device for GPU training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ1I0DgPZS7i"
      },
      "source": [
        "#-----------------------------------------------------task 5 -----------------------------------------------------\n",
        "# Task 5: Train and test ResNet on CIFAR10 dataset for classification\n",
        "# hints: define your resnet network, loss function, optimizer and dataloaders. \n",
        "# Then you can run the same training and testing code as above.\n",
        "# ----------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZC7Ag1CWK9-"
      },
      "source": [
        "**TO Do: 2.2** Train and test ResNet on CIFAR10 dataset for classification\n",
        "- create a train and test dataset using torchvision.datasets.CIFAR (completed for you)\n",
        "- create separate dataloader for test and train with `batch_size=8`; as usual, set `shuffle=True` for trains and `shuffle=False` for test\n",
        "- define a loss function for classification\n",
        "- set optimizer to SGD with momentum, learning rate=0.001\n",
        "- implement training loop; train on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKADdVPGZS7j"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = None\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = None\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTqh8YfeZS7l"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#resnet_cifar = ResNet18(in_channels=3)\n",
        "#resnet_cifar = resnet_cifar.to(device)\n",
        "\n",
        "loss_fun = None\n",
        "loss_fun = loss_fun.to(device)\n",
        "\n",
        "optimizer = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S9rqhiFZS7m"
      },
      "source": [
        "epochs = 1\n",
        "for epoch in range(epochs): \n",
        "\n",
        "    # enumerate can be used to output iteration index i, as well as the data \n",
        "    for i, (data, labels) in enumerate(train_loader, 0):\n",
        "        # EX 2.2. Students to implement training loops\n",
        "        None\n",
        "\n",
        "        # print statistics\n",
        "        ce_loss = loss.item()\n",
        "        if i % 10 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch + 1, i + 1, ce_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "731RV-9_ZS7o"
      },
      "source": [
        "#make an iterator from test_loader\n",
        "#Get a batch of testing images\n",
        "test_iterator = iter(test_loader)\n",
        "images, labels = test_iterator.next()\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "y_score = resnet_cifar(images)\n",
        "# get predicted class from the class probabilities\n",
        "_, y_pred = torch.max(y_score, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
        "rows = 2\n",
        "columns = 4\n",
        "# plot y_score - true label (t) vs predicted label (p)\n",
        "fig2 = plt.figure()\n",
        "for i in range(8):\n",
        "    fig2.add_subplot(rows, columns, i+1)\n",
        "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
        "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
        "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMjiACQIZS7r"
      },
      "source": [
        "y_true = labels.data.cpu().numpy()\n",
        "y_pred = y_pred.data.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui7a9F2dZS7v"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoLwmEjJZS7w"
      },
      "source": [
        "## **Exercise 3 Image segmentation with pytorch using U-net**\n",
        "\n",
        "U-net was first developed in 2015 by [Ronneberger et al.](https://arxiv.org/abs/1505.04597), as a segmentation network for biomedical image analysis.\n",
        "It has been extremely successful, with 9,000+ citations, and many new methods that have used the U-net architecture since.\n",
        "\n",
        "\n",
        "The architecture of U-net is based on the idea of using skip connections (i.e. concatenating) at different levels of the network to retain high, and low level features.\n",
        "\n",
        "Here is the architecture of a U-net:\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1zUKKrbcB1BZxJ7-hEYpteCVlVFRJ1nRg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLc2QhVnZS7w"
      },
      "source": [
        "### Two-photon microscopy dataset of cortical axons\n",
        "\n",
        "In this tutorial we use a dataset of cortical neurons with their corresponding segmentation binary labels.\n",
        "\n",
        "These images were collected using in-vivo two-photon microscopy from the mouse somatosensory cortex. To generate the 2D images, a max projection was used over the 3D stack. The labels are binary segmentation maps of the axons.\n",
        "\n",
        "Here we will use 100 [64x64] crops during training and validation. \n",
        "\n",
        "These are some example images [256x256] from the original dataset, taken from [Bass et al 2019](http://proceedings.mlr.press/v102/bass19a.html)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1YRJev88nBr4aqyaHRU27KWxFaX4JwUJz\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "\n",
        "**To DO Ex 3.1** \n",
        "- this dataset is too large for Keats so you must access the data from this [Google Drive link](https://drive.google.com/drive/folders/1R5Myca6egnuqLXrDqj6pbw9ei9ufRlDj?usp=sharing). \n",
        "- Once you access the link the folder should be accessible from your Google Drive, you may then need to edit the line 2 of cell 31 to reference the path **relative to your own Drive** and **make sure to save outputs to your own Drive** (note if this is proving difficult you can download the folder and upload it to your own drive)\n",
        "- run the below lines of code to mount the drive and the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWdYOWSdwk2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR5egYifZS7x"
      },
      "source": [
        "# edit this link to correspond to the location of the shred folder on your google drive \n",
        "%cd '/content/drive/My Drive/Colab Notebooks/Colab_Data/week3-Data/'\n",
        "\n",
        "# # List files to make sure we're in the expected directory.\n",
        "# # Your output will look different, showing your own Drive files here.\n",
        "!ls\n",
        "\n",
        "#load modules\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from AxonDataset import AxonDataset\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as vutils\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLvx70rlZS7y"
      },
      "source": [
        "# Setting parameters\n",
        "timestr = time.strftime(\"%d%m%Y-%H%M\")\n",
        "__location__ = os.path.realpath(\n",
        "    os.path.join(os.getcwd(), os.path.dirname('__file__')))\n",
        "\n",
        "print(__location__)\n",
        "path = os.path.join(__location__,'results')\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    \n",
        "# Define your batch_size\n",
        "batch_size = 16\n",
        "print(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z_RoXugZS70"
      },
      "source": [
        "### **Ex 3.2 Creating a dataloader**\n",
        "\n",
        "In this example, a custom dataset was created, and we import it from `AxonDataset.py`. \n",
        "\n",
        "Then rather than create a separate test and train instance, we override the default DataLoader `shuffle` option to instead implement a  `torch.utils.data.sampler.SubsetRandomSampler` to 'samples elements randomly from a given list of indices, without replacement' (see [PyTorch documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler)).\n",
        "\n",
        "To do this:\n",
        "\n",
        "1. data is randomly split into 80% train and 20% validation sets\n",
        "2. these lists are passed to class `SubsetRandomSampler` to create a sampling instance for each group\n",
        "3. train and validation DataLoaders are created from the same dataset by passing a different sampler for each class\n",
        "\n",
        "This is a good way of randomly separating your own data, in instances where PyTorch does not provide custom Datasets\n",
        "\n",
        "**To Do**\n",
        "- Comment the lines of code to verify you understand how the bespoke samplers are implemented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnzaD1iTZS71"
      },
      "source": [
        "# STUDENTS to comment code lines\n",
        "#First we create a custom dataset of two photon microscopy images of axons\n",
        "axon_dataset = AxonDataset(data_name='org64', type='train')\n",
        "\n",
        "indices = list(range(len(axon_dataset)))  \n",
        "\n",
        "split = int(len(indices)*0.2)  \n",
        "\n",
        "validation_idx = np.random.choice(indices, size=split, replace=False)\n",
        "\n",
        "train_idx = list(set(indices) - set(validation_idx))\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "validation_sampler = SubsetRandomSampler(validation_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
        "                                           sampler=train_sampler) \n",
        "val_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
        "                                        sampler=validation_sampler) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14imZFJHZS72"
      },
      "source": [
        "## **Building a U-net**\n",
        "\n",
        "We next build our u-net network.\n",
        "\n",
        "First we define a layer `double_conv` that performs 2 sets of convolution followed by ReLu.This is set up as a `nn.Sequential(` block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnZWq_gwZS73"
      },
      "source": [
        "# define U-net\n",
        "def double_conv(in_channels, out_channels, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVp-bRHWZS74"
      },
      "source": [
        "The original U-net encoder performs downsampling through a $2 \\times 2 $ max pool (however, strided convolutions are equally viable). Thus, in what follows, a single level of encoding can be represented as:\n",
        "\n",
        "```\n",
        " conv1 = self.dconv_down1(x)\n",
        " conv1 = self.dropout(conv1)\n",
        " x = self.maxpool(conv1)\n",
        " ```\n",
        "Here, a dropout layer is inserted between the convolutional layer and the maxpool for regularisation. An alternative approach is to insert a batchnorm between the `nn.Conv2d` and the `nn.ReLU` e.g. [see](https://github.com/milesial/Pytorch-UNet)\n",
        "\n",
        "Next we need to define how we perform an upsample step. This  is performed through use of [`nn.Upsample`](https://pytorch.org/docs/stable/nn.html#torch.nn.Upsample), which interpolates the data to a higher resolution grid. The layer must be created in the consructor (see line 14)  and expects arguments `scale_factor` and (interpolation) `mode`. There are several options for the interpolation mode; we recommend bilinear. In this example we upsample by a `scale_factor` of 2 each time (to match the $2\\times 2$ max pool used during downsampling). \n",
        "        \n",
        "Then, a single level of decoding might may represented as:\n",
        "\n",
        "```\n",
        " deconv4 = self.upsample(conv5)\n",
        " deconv4  = self.dconv_up4(deconv4)\n",
        " deconv4 = self.dropout(deconv4)\n",
        " ```\n",
        " \n",
        "However, we are still missing something vital...\n",
        "\n",
        "### **Skip connections**\n",
        "\n",
        "The U-net is a symmetric network with equal numbers of encoding and decoding layers. These form pairs where the spatial dimensions of each encoder/decoder layer in the pair are consistent.\n",
        "\n",
        "A key feature of the U-net is that to support segmentation of sharp boundaries, with preservation of high resolution features, it is necessary to pass features learnt during encoding across the network. The theory is that the early layers, with their small-receptive fields, learn the high-spatial frequency information (i.e. they act as edge detectors and/or texture filters). As the receptive field increases during encoding spatial specicity is lost, but spatial localisation (where class relevant objects broadly are in the image) is gained. In order to import the high spatial frequency information of the early encoding layers into the final decoding layers the *activations* learnt during encoding are directly concatenated onto the upsampled activations of the paired decoding layer.\n",
        "\n",
        "In other words for the first decoding layer (which for a 5-layer U-Net is the layer that directly follows the bottleneck `conv5`) is:\n",
        "\n",
        "```\n",
        " deconv4 = self.upsample(conv5)\n",
        " deconv4 = torch.cat([deconv4, conv4], dim=1)\n",
        " deconv4  = self.dconv_up4(deconv4)\n",
        " deconv4 = self.dropout(deconv4)\n",
        " ```\n",
        " \n",
        " The activations (output) of convolution layer conv (`conv4`) is directly concatenated to the output of `self.upsample` where concatenation is performed on the channel axis (`axis=1`); Thus putting this all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s42uHIxZS74"
      },
      "source": [
        "### **Exercise 3.3. Creating U-Net class**\n",
        "\n",
        "We then define our U-net network.\n",
        "\n",
        "We first initialise all the different layers in the network in `__init__`:\n",
        "1. `self.dconv_down1` is a double convolutional layer (defined above)\n",
        "2. `self.maxpool` is a max pooling layer that is used to reduce the size of the input, and increase the receptive field\n",
        "3. `self.upsample` is an upsampling layer that is used to increase the size of the input\n",
        "4. `dropout` is a dropout layer that is applied to regularise the training\n",
        "5. `dconv_up4` is also a double convolutional layer- note that it takes in additional channels from previous layers (i.e. the skip connections).\n",
        "\n",
        "\n",
        "### **To do 3.3.1  complete the forward pass**\n",
        "\n",
        "1. Following the example for conv1 complete encoder layers 2,3 and 4. How many features does each layer have?\n",
        "2. Complete layer `conv5`; this is the bottleneck layer (the bottom of the network) and thus **has no maxpool**.\n",
        "2. Using the upsampling and skip connection example above implement the decoder layers `deconv4`,`deconv3`,`deconv2`,`deconv1`.\n",
        "5. We are expecting class labels as output; thus the output requires a sigmoid transformation; check you understand what this does?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5pKMcEkZS75"
      },
      "source": [
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.dconv_down1 = double_conv(1, 32)\n",
        "        self.dconv_down2 = double_conv(32, 64)\n",
        "        self.dconv_down3 = double_conv(64, 128)\n",
        "        self.dconv_down4 = double_conv(128, 256)\n",
        "        self.dconv_down5 = double_conv(256, 512)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.dropout = nn.Dropout2d(0.5)\n",
        "        self.dconv_up4 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up2 = double_conv(128 + 64, 64)\n",
        "        self.dconv_up1 = double_conv(64 + 32, 32)\n",
        "\n",
        "        self.conv_last = nn.Conv2d(32, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        #######   ENCODER ###############\n",
        "        \n",
        "        conv1 = self.dconv_down1(x)\n",
        "        conv1 = self.dropout(conv1)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        # --------------------------------------------------- task 3.3.1 ----------------------------------------------------------\n",
        "        # Ex 3.3.1.1 implement encoder layers conv2, conv3 and conv4\n",
        "        \n",
        "        None\n",
        "\n",
        "        # --------------------------------------------------- task 3.3.2 ----------------------------------------------------------\n",
        "        # Ex 3.3.1.2 implement bottleneck (hint 2 lines)\n",
        "        \n",
        "        conv5=None\n",
        "        # ---------------------------------------------------------------------------------------------------------------------\n",
        "       \n",
        "        #######   DECODER ###############\n",
        "        \n",
        "        # --------------------------------------------------- task 3.3.3 ----------------------------------------------------------\n",
        "        # Ex 3.3.1.3 Implement the decoding layers\n",
        "        \n",
        "        #---------------------------------------------------------------------------------------------------------------------\n",
        "        out = F.sigmoid(self.conv_last(deconv1))\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWyAImXZZS76"
      },
      "source": [
        "## **Saving and loading models**\n",
        "*For practical reasons training this network from scratch will take too long, and require large computational resources* To save time we initialise the network with a previously trained network by loading the weights in the following way.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNE51dT4ZS77"
      },
      "source": [
        "# initialise network - and load weights\n",
        "print(device)\n",
        "net = UNet()\n",
        "#net.load_state_dict(torch.load(path+'/'+'model.pt')) #this function loads a pretrained network\n",
        "net.load_state_dict(torch.load(path+'/'+'model.pt',map_location=torch.device(device)))\n",
        "net=net.to(device)\n",
        "\n",
        "# Example how to save a model - check in your results path\n",
        "torch.save(net.state_dict(), path+'/model_save_test.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd6uu5U-Nnsr"
      },
      "source": [
        "In general [PyTorch documentation](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html#save-the-general-checkpoint), it is advised that you save and load not just network paramters but also the state of the optimiser, current state of the loss and the epoch \n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1hU498xlA_DbstHSSUwqfm9U9fqtGCZw1\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "Here, saving the optimiser state will be particularly important if the optimiser is implementing learning rate annealing, for example. To then load you implement:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1mQllAgFWxZ9ViaXJmPjeStXJViroBQHi\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "More details on options for saving and loading are provided [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html). These provide an example of the model and optimiser `state_dict()` where you can see these are python dictionaries with:\n",
        "\n",
        "- **in the case of the model**: keys which store the current state of weight and bias tensors of the model. In more general terms the model dict will store all parameter tensors required to restart the model\n",
        "- **in the case of the optimiser**: this dict stores the hyper-parameters of the optimiser e.g. learning rate, momentum, weight decay etc as well as the current state of the optimiser object.\n",
        "\n",
        "An example of printing the model and optimiser state is given as:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1Z1rrGkWocKmimoo1KbRRElrgQtVSKed0\" alt=\"Drawing\" width=\"600px;\"  />\n",
        "</figure>\n",
        "\n",
        "Note, **if saving for inference _only_, it is only necessary to save the `model.state_dict()_**\n",
        "\n",
        "A common PyTorch convention is to save models using either a .pt or .pth file extension (see example of `SAVE_PATH`) above.\n",
        "\n",
        "We will try this out at the end of the exercise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XIjmMWgZS79"
      },
      "source": [
        "## Exercise 3.4 Create loss function and optimiser instance\n",
        "\n",
        "We next define our loss function - in this case we use Dice loss, a commonly used loss for image segmentation.\n",
        "\n",
        "The Dice coefficient can be used as a loss function, and is essentially a measure of overlap between two samples.\n",
        "\n",
        "Dice is in the range of 0 to 1, where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n",
        "\n",
        "$Dice = \\dfrac{2|A\\cap B|}{|A| + |B|}$\n",
        "\n",
        "where $|A\\cap B|$ represents the common elements between sets $A$ and $B$, and $|A|$ represents the number of elements in set $A$ (and likewise for set $B$).\n",
        "\n",
        "For the case of evaluating a Dice coefficient on predicted segmentation masks, we can approximate  $|A\\cap B|$ as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix.\n",
        "\n",
        "An **alternative loss** function would be pixel-wise binary cross entropy loss. It would examine each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.\n",
        "\n",
        "**To do 3.4** \n",
        "- Also define a binary cross entropy loss\n",
        "- notice how the Adam optimiser loads the parameters of the pretrained weights as `filter(lambda p: p.requires_grad, net.parameters())` ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmCqq3dxZS7-"
      },
      "source": [
        "# dice loss\n",
        "def dice_coeff(pred, target):\n",
        "    \"\"\"This definition generalize to real valued pred and target vector.\n",
        "    This should be differentiable.\n",
        "    pred: tensor with first dimension as batch\n",
        "    target: tensor with first dimension as batch\n",
        "    \"\"\"\n",
        "\n",
        "    smooth = 1.\n",
        "    epsilon = 10e-8\n",
        "\n",
        "    # have to use contiguous since they may from a torch.view op\n",
        "    iflat = pred.contiguous().view(-1)\n",
        "    tflat = target.contiguous().view(-1)\n",
        "    intersection = (iflat * tflat).sum()\n",
        "\n",
        "    A_sum = torch.sum(iflat * iflat)\n",
        "    B_sum = torch.sum(tflat * tflat)\n",
        "\n",
        "    dice = (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
        "    dice = dice.mean(dim=0)\n",
        "    dice = torch.clamp(dice, 0, 1.0-epsilon)\n",
        "\n",
        "    return  dice\n",
        "\n",
        "# 3.4 define binary cross entropy loss\n",
        "loss_BCE = None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d_kb2T5ZS8A"
      },
      "source": [
        "Here the penalty term `smooth` is added to prevent division by zero.\n",
        "\n",
        "As before, we define the optimiser to train our network - here we use Adam.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtOa1EziZS8A"
      },
      "source": [
        "#define your optimiser\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-05, betas=(0.5, 0.999))\n",
        "optimizer.zero_grad()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zn_eFtSZS8B"
      },
      "source": [
        "## **Ex 3.5 Training and evaluation**\n",
        "\n",
        "Review the stages of training with:\n",
        "- net set as 'net.train()` for training and `net.eval()` for validation\n",
        "- clearing of gradients\n",
        "- loss as `err=1- dice_coeff(pred,target)` where pred is output of forwards pass\n",
        "- backpropagation and update\n",
        "\n",
        "**To do** \n",
        "\n",
        "1. Question, which operations are implemented only in the training loop and not during validation? Answer in the text cell below \n",
        "2. Plot the dice coefficient of the validation set using the plotting fucntion below; **Note down your dice validation scores for each experiment** then \n",
        "3. Change the dice loss to a binary cross entropy loss in the code - is dice loss or cross entropy loss better?\n",
        "4. Add checkpointing using torch.save - i.e. save the model and optimiser state_dict every epoch (line 63)\n",
        "5. Re-load and train from your saved model, \n",
        "  - make sure to load the state dict for the model *and* the optimiser\n",
        "  - first print out the state dict for the optimiser as shown above\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "note that the results are saved to a folder \\results - so please check that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JEFMg0Qlv0i"
      },
      "source": [
        "**Ex 3.5.1 Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU5av-MRZS8C"
      },
      "source": [
        "epochs=10\n",
        "save_every=10\n",
        "all_error = np.zeros(0)\n",
        "all_error_L1 = np.zeros(0)\n",
        "all_error_dice = np.zeros(0)\n",
        "all_dice = np.zeros(0)\n",
        "all_val_dice = np.zeros(1)\n",
        "all_val_error = np.zeros(0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    ##########\n",
        "    # Train\n",
        "    ##########\n",
        "\n",
        "    # set network to train prior to training loop \n",
        "    net.train() # this will ensure that parameters will be updated during training & that dropout will be used\n",
        "    t0 = time.time()\n",
        "    for i, (data, label) in enumerate(train_loader):\n",
        "        ######### STUDENT TO DO  IMPLEMENT TRaining loop *****\n",
        "        data = data.to(device)\n",
        "        label= label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        target_real = torch.ones(data.size()[0])\n",
        "        batch_size = data.size()[0]\n",
        "        pred = net(data)\n",
        "        \n",
        "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
        "        # Task 3.5.3: change loss function here\n",
        "        err = 1- dice_coeff(pred, label) \n",
        "        # -------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        dice_value = dice_coeff(pred, label).item()\n",
        "\n",
        "        err.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        time_elapsed = time.time() - t0\n",
        "        print('[{:d}/{:d}][{:d}/{:d}] Elapsed_time: {:.0f}m{:.0f}s Loss: {:.4f} Dice: {:.4f}'\n",
        "              .format(epoch, epochs, i, len(train_loader), time_elapsed // 60, time_elapsed % 60,\n",
        "                      err.item(), dice_value))\n",
        "\n",
        "        if i % save_every == 0:\n",
        "            # setting your network to eval mode to remove dropout during testing\n",
        "            net.eval()\n",
        "\n",
        "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_train_data.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_train_label.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_train_pred.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "\n",
        "            error = err.item()\n",
        "\n",
        "            all_error = np.append(all_error, error)\n",
        "            all_dice = np.append(all_dice, dice_value)\n",
        "\n",
        "    # # Task 3.5.4 ADD Checkpointing Here!\n",
        "    None\n",
        "\n",
        "    # #############\n",
        "    # # Validation\n",
        "    # #############\n",
        "    mean_error = np.zeros(0)\n",
        "    mean_dice = np.zeros(0)\n",
        "    t0 = time.time()\n",
        "\n",
        "    # set network to eval prior to training loop \n",
        "    net.eval()\n",
        "    for i, (data, label) in enumerate(val_loader):\n",
        "        data=data.to(device)\n",
        "        label=label.to(device)     \n",
        "        batch_size = data.size()[0]\n",
        "\n",
        "        pred = net(data)\n",
        "        \n",
        "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
        "        # Task 3: change loss function here\n",
        "        err = 1-dice_coeff(pred, label)\n",
        "        # -------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        # compare generated image to data-  metric\n",
        "        dice_value = dice_coeff(pred, label).item()\n",
        "\n",
        "        if i == 0:\n",
        "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_val_data.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_val_label.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_val_pred.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "\n",
        "        error = err.item()\n",
        "        mean_error = np.append(mean_error, error)\n",
        "        mean_dice = np.append(mean_dice, dice_value)\n",
        "\n",
        "    all_val_error = np.append(all_val_error, np.mean(mean_error))\n",
        "    all_val_dice = np.append(all_val_dice, np.mean(mean_dice))\n",
        "\n",
        "    time_elapsed = time.time() - t0\n",
        "\n",
        "    print('Elapsed_time: {:.0f}m{:.0f}s Val dice: {:.4f}'\n",
        "          .format(time_elapsed // 60, time_elapsed % 60, mean_dice.mean()))\n",
        "    \n",
        "    \n",
        "    num_it_per_epoch_train = ((train_loader.dataset.x_data.shape[0] * (1 - 0.2)) // (\n",
        "            save_every * batch_size)) + 1\n",
        "    epochs_train = np.arange(1,all_error.size+1) / num_it_per_epoch_train\n",
        "    epochs_val = np.arange(0,all_val_dice.size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPUiuraRB_QG"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "#plt.figure()\n",
        "plt.plot(epochs_val, all_val_dice, label='dice_val')\n",
        "# plt.xlabel('epochs')\n",
        "# plt.legend()\n",
        "# plt.title('Dice score')\n",
        "# plt.savefig(path + '/dice_val.png')\n",
        "# plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABmNEcVOO8DJ"
      },
      "source": [
        "# STUDENT TO DO 3.5.5 RELOAD AND TRAIN FROM *YOUR* SAVED MODEL\n",
        "# BE SURE TO LOAD BOTH THE MODEL AND OPTIMISER STATE DICT\n",
        "# PRINT OUT THE PARAMETERS OF THE OPTIMISER STATE DICT AFTER LOADING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6MWNjLqZS8F"
      },
      "source": [
        "## **Results** \n",
        "the results are saved to a folder \\results - so please check that:\n",
        "\n",
        "The results are saved per epoch for both training and validation, and are saved as the \n",
        "1. real data, \n",
        "2. binary labels, \n",
        "3. predicted labels. \n",
        "\n",
        "In this example since we trained on a small sample of the data (100 crops) the results are far from optimal, and are likely to overfit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX8HFcfyZS8G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}