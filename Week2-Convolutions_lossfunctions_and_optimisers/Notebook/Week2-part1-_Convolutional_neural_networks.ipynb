{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.1.Convolutional_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjVpejWAOH25"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "In this notebook we will motivate the development and design of convolutional neural networks (CNNs), and define what is meant by the  convolutional operation.\n",
        "\n",
        "## 1. Why Convolutions?\n",
        "\n",
        "The design of CNNs arose from a need to overcome the computational constraints met when upscaling deep learning to the processing of high dimensional images, but took significant inspiration from biological vision networks:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1BmITbP1fg3G4ifm3eD988QHfGFf6JMxZ\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "For the mammalian visual system (see above fig and [Van Essen et al.](https://www.sciencedirect.com/science/article/abs/pii/0896627394904553) for reference) visual stimulae first reach the back of the brain from the retina, travelling first to the region known as V1 before being hierarchically processed through a series of regions distributed across a number of different visual pathways.\n",
        "\n",
        "Taking object recorgnition as a specific example, the cells of V1 act as edge detectors, able to detect the high spatial frequency features of an image. More sophisticated patterns are then learnt by passing the information processed by V1 to a series of other brain regions (e.g. V2 and V4), which learn to detect a hierarchy of more complex image textures, before finally reaching the posterior inferior temporal cortex (PIT) where individual cells are known to activate for individual objects such as faces or cars.\n",
        "\n",
        "CNNs are therefore designed to mimic this process. Representations are learnt over many convolutional layers, where early layers can be seen to act as edge detectors and higher layers detect more complex textures or whole objects.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1bXEO3A9tk0M11lAy56Y1Syo1m9a8l6Hh\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "This has strong advantages for image recognition, object localisation and segmentation tasks as it allows images to be compared without any requirement for spatial normalisation or image registration. In other words there is no assumption that corresponding image pixels – at the same relative locations in the image, represent the same content, which is something that can be very difficult to achieve for example of complex natural scenes, which can vary in content dramatically whilst having the same label (for example, imagine all the different architectures of a house or building, how would one to one correspondence be achieved for images in that case?). The same is true in many cases for medical imaging for example brains vary in ways which cannot fully be compared through deformable models.\n",
        "\n",
        "## 2. The convolutional operation\n",
        "\n",
        "So how do convolutional operations support comparisons between images? Lets first look at an example of the convolutional operation applied for a hand engineered filter kernel known as a Sobel filter (designed to detect edges) \n",
        "\n",
        "Here we apply it to a small part of a 2D slice through a brain scan, at a point where we know there is a sharp change in image intensity. The numbers in the central grid reflect the intensity on a scale from 0 to 255.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Sy7tQfKvbrMwRCgIOe9q1Jmcgygbewt3\" alt=\"Drawing\"/>\n",
        "\n",
        "The convolutional operation then results from translating the convolutional kernel across the image and performing elementwise multiplication and sum at each location. The output of the operation is assigned to the pixel location at the centre of the filter:\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1HyefEWs_wUg2xt3gzmbkcqXtaTPO5l5r\" alt=\"Drawing\"/>\n",
        "\n",
        "<br><br>\n",
        "\n",
        "Note, that technically a mathematical convolution first involves flipping the filter; however, CNNs ignore this as, since they are learning the filters themselves, it doesn't matter if the learn the originals or the flipped forms.Further, notice that the result of the full convolutional operation fills a grid of size 2 rows and 2 columns less than the original – corresponding to the number of full times the kernel can be fit into the space this can be corrected for using padding:\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1V_5sqoG8s-fGdjshSXXQfUj5T1yKf2sL\" alt=\"Drawing\"/>\n",
        "\n",
        "<br>This adds as many outer layers of zeros as required to ensure that the center of the filter kernel now fits to the outer layer of the original network (i.e one layer for $3 \\times 3$ kernels but 2 for $5 \\times 5$ kernels.\n",
        "\n",
        "A related concept is strides, which allow the network convolutional operation to skip over locations in the image, with the result that the output shape is downsampled (e.g. for an input dimension h =7, kernel size f =3 and a stride s=2):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Eue4ycUntrmGY9lMB7lH6Osn5v5ocPlw\" alt=\"Drawing\"/>\n",
        "\n",
        "Where the dimensions of output following (strided and/or padded convolutions) can be determined from the following formular $ \\lfloor (h+2p -f)/s \\rfloor +1$.\n",
        "This means that technically not all input dimensions support strides > 1; however, in practice, should  $h+2p -f$ returns an odd number, deep learning frameworks will add zero padding to make it fit\n",
        "\n",
        "The end result of a sobel operation is a map sensitive to the edge structures in an image (it is an edge detector).\n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=1KYKVmx29EfsJzLn9_nJnTr4INhwavWHc\" alt=\"Drawing\"/>\n",
        "\n",
        "IN general, images may be compared using hand engineered feature detectors by comparing the responses of images to these features in transformation and scale invariant ways. More specifically, historically the goal of traditional image detection approaches was to use the detectors to identify specific types of structure in an image (characterised by specific textures, or edge structures) and then compare images in terms of how often different types of features appeared. For example staying with the house analogy the detected patterns might signify a door or a window.\n",
        "\n",
        "The advantage of convolutional neural networks over traditional approaches is, therefore, that rather than hand designing techniques to detect features from images, the CNNs instead learn to detect these through design of bespoke filter kernels. The CNN filters correspond to the weights of the network; these are optimised through minimisation of a loss with respect to a image classification, regression or segmentation task, for example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbUOWAvhaLoJ"
      },
      "source": [
        "## 3. The building blocks of Convolutional Neural Networks (CNNs).\n",
        "\n",
        "The essential components of a CNN are the convolutional layers, downsampling operations (performed through pooling or striding) and the activations (which support learning of non-linear interactions).\n",
        "\n",
        "### 3.1 Convolutional layers\n",
        "\n",
        "In the last section we looked at what is meant by the convolutional operation, but how is this implemented within deep networks and how does it relate to parameter (weights) learning during optimisaton.\n",
        "\n",
        "In contrast to fully connected layers, CNNs do not employ full connectivity between each incoming feature and each neuron in the layer. Rather each neuron in a CNN has a very restricted field of view, constrained to the dimensions of some local filter kernel fit at each location. Let's give this filter dimensions $f \\times f \\times d_0$, where $f$ represents the height and width of the kernel and $d_0$ represents the channel depth.\n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=1NHP6eHiWLq6toj15mZUsH2hSxIvUgV87\" alt=\"Drawing\"/>\n",
        "\n",
        "The receptive field (or scale $f$ of filters) varies (although these are typically odd numbers e.g. $3 \\times 3$, $5 \\times 5$, $7 \\times 7$); however, the depth must always equal the depth of the incoming data array e.g for the input layers this might be 3 channels for a RGB image, or 1 for greyscale.\n",
        "\n",
        "All points in the image are constrained to learn the same filter weights as their neighbours (otherwise known as parameter sharing). This operation therefore reduces to learning a set of convolutional filters, which operate on the image to return activation maps e.g. the edge maps shown in the above figure. Training CNNs in this way both significantly reduces the amount of parameters which need to be learnt (relative to a comparable fully connected, or MLP, network). It also allows CNNs to take advantage of the hierarchical, multi-scale properties of images, in a similar way to biological networks.\n",
        "\n",
        "Accordingly the forward pass of a CNN is implemented in the following way:\n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=1K_6GlJAAuoXcfLXh_ElcwGY4UWJCzI_H\" alt=\"Drawing\"/>\n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=1dXaNcyZHrQdMbcu6BUuKoy0Cph98-fT2\" alt=\"Drawing\"/>\n",
        "\n",
        "Where the top figure shows the filter (weights) kernel fit at the first available location in the image, and the second shows the second location (ignoring channels/depth). At both locations the exact same weights $\\mathbf{W}_0=\\{W_{01},W_{02},....., W_{09}\\}$ are learnt. $Z$ here represents the result of the linear transformation where depth (third axis) records the result of this operation for each weights kernel $\\mathbf{W}_i$ learnt e.g. for the second kernel we instead see it fills index 1 of the 3rd axis:\n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=1OW1swfolAE-swxK9il2fntT02Fg4uGjF\" alt=\"Drawing\"/>\n",
        "\n",
        "This can be implemented fast using numpy vectorisation by reshaping the filters and image patches to support straight matrix multiplication.\n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=14q27bUCWHT1Af1Ln_kCxca3Z22xn9uuE\" alt=\"Drawing\"/>\n",
        "\n",
        "Backpropagation is also simplied by the fact that it may also be implemented as a convolution. Specifically, if we consider that the forward operation is estimated as:\n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=1E2p8sZhS8Uy7FZ-I_-QHZoIyPYVbO9X0\" alt=\"Drawing\"/>\n",
        "\n",
        "\n",
        "If we subsequently look at the partial derivatives of the loss estimated with respect to the parameters we see these take the following form, which itself reduces to a convolution, where the elements of $\\mathbf{X}$ are convolved by the partial derivatives of $L$  with respect to $\\mathbf{Z}$ \n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=1LjuR3TG6vHWxxWCgiJOFy9cwQIdKJaZg\" alt=\"Drawing\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVJ91MGbk1c8"
      },
      "source": [
        "### 3.2 Downsampling\n",
        "\n",
        "An important feature of convolutional networks is that they use downsampling to increase the receptive field of filter kernels so as to learn to recognise objects over a hierarchy of scales.\n",
        "\n",
        "For example imagine a car, early features of the network might recognise sharp edges, mid range features (filters/kernels) might then learn more complex textures and shapes, building towards the deeper layers recognising parts of the object (wheels, mirrors, windows, doors) and finally the whole object.\n",
        "\n",
        "There are two different mechanisms that CNNs use for downsampling: pooling and striding. \n",
        "\n",
        "**Pooling**: works by fitting a pooling kernel to typically non-overlapping patches of an image and then applying simple min, max or averaging operations to aggregate/filter those values.\n",
        "\n",
        "For example, below we show max pool, implemented with a pooling filter of shape $2\\times 2$ and stride of 2:\n",
        "\n",
        "<br> <img src=\"https://drive.google.com/uc?id=1lUZHys3qCJczk9h_mdt2Ve2ftrKH9Gs1\" alt=\"Drawing\"/>\n",
        "\n",
        "Looking first at the top left yellow block, we see the max pool reduces the four numbers to one by picking the largest element (9); the same applies to the green, blue and red blocks. As a result the image is downsampled by a factor of 2.\n",
        "\n",
        "Backpropagation through max pool layers operates similarly as for relu (given that both result from elementwise max operations). However, in order to track back through a max poool a mask must be kept, which saves the locations of the largest elements of each pooling kernel (resulting from the forward operation.\n",
        "\n",
        "**Strides** on the work hand work by applying a convolutional operation whilst skipping out certain locations in the image, for example every alternate kernel centre:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1h75jIooLppf6I9ANsrCZ8igIfEUeUzx6\" alt=\"Drawing\"/>\n",
        "\n",
        "These offers the advantage of effectively learning the downsampling operation, but at the cost of learning more parameters.\n",
        "\n",
        "### 3.3. Activations\n",
        "\n",
        "Importantly as before convolutional networks require activation layers in order to learn non-linear mappings of the data. It is typical to implement activations (within the body of the CNN) as Relu functions. As these operate elementwise, their behaviour is much as before.\n",
        "\n",
        "### 3.4 Optional elements\n",
        "\n",
        "In addition to the key components of convolutional networks, several other operations have been introduced to regularise, speed up training and/or improve efficiency or generalisability of network:\n",
        "\n",
        "#### 3.4.1 $1\\times 1$ convolutions\n",
        "\n",
        "The motivation behind $1\\times 1$ convolutions is to support compression or upsampling of the channel dimension of an activation block. This can be useful when the goal is to perform some parameter heavy operation, after which point the data can be upsampled back to its previous resolution. \n",
        "\n",
        "An example of how $1x1$ comvolutions achieve this can be found in the linked\n",
        "[blog](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\n",
        "), which explains that convolving the blue block (shape $H \\times  W  \\times D$)  with the orange filter (shape $1\\times 1 \\times D$) will return one 2D output shape $H\\times W \\times 1$; when we apply N of these then the dimensions of the output will be $H\\times W \\times N$\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1F9gcqSORiGFpYkbvVx0rwAZVnYb9xddR\" alt=\"Drawing\"/>\n",
        "\n",
        "As always the depth of the kernel must be equal to the depth of the incoming avctivation tensor.\n",
        "\n",
        "#### 3.4.2 Batch Normalisation (batchnorm)\n",
        "\n",
        "As we will see in the session on optimisers, deep networks are typically trained with variants of stochastic gradient descent, this samples batches from the data sets and estimates average loss for each batch rather than estimating it across all examples. This can lead to noisy gradient updates, since the composition of each batch is subject to change. [Batch-norm](https://arxiv.org/abs/1502.03167) seeks to address this by normalising and rescaling the activations of each batch, at every layer throughout the network. In doing so it can considerably speed up training.\n",
        "\n",
        "For more details on batch normalisation, the (Fastai course)[https://github.com/hiromis/notes/blob/master/Lesson6.md] provides an excellent intuitive explanation (at 44mins)\n",
        "\n",
        "#### 3.4.3 Dropout regularisation\n",
        "\n",
        "Finally dropout is a technique which can be used to performed network regularisation. It works by randomly dropping activations during training, by applying a randomised masking operation on the output of each layer. In doing so, the approach stops individual network components form memorising the inputs – something which would lead to overfitting. There is a trade off however, as dropping out too many weights will prevent the network from learning well enough and will cause underfitting.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXFj2s03p0oO"
      },
      "source": [
        "## 4 Implementing Convolutional Neural Networks in PyTorch\n",
        "\n",
        "### 4.1 Convolution Layers\n",
        "\n",
        "A 2D convolution class is defined within the `torch.nn` module as follows:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=12hvQSk-kCsPWTnEE16KKPkA0zo1R3Wzc\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "#### **Exercise 1** implementing a 2D convolution\n",
        "\n",
        "Let's look at implementing implement a 2d Convolution with stride = 1, kernel size = 3x3 and 2 output channels, applied to a random 3D 'image' generated with the same shape, demoed previously for linear layers ($3 \\times 100 \\times 100$ - noting here channels are specified first, following the required PyTorch convention). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEr-zNz6qA5U"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # importing torch.nn \n",
        "\n",
        "# the first dimension has size N where N is the number of images. \n",
        "#here it is simply 1\n",
        "\n",
        "input_image = torch.randint(0, 255, (1, 3,100,100)) # our random image. \n",
        "\n",
        "# building our conv operation. note that we did not need to specify the names of the parameters. \n",
        "#nn.Conv2d(3,2,3) is sufficient\n",
        "operation = nn.Conv2d(in_channels = 3,out_channels = 2, kernel_size = 3) \n",
        "\n",
        "print(operation) #we can see our convolution operation by printing it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmlpS-0rr1Q"
      },
      "source": [
        "First try the following operation - observe the ```RuntimeError```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVIAp99Qrsa8"
      },
      "source": [
        "result = operation(input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPvIaqNDrufw"
      },
      "source": [
        "The operation fails as it cannot work on integer tensors. Let us convert it into a float tensor first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhEi6npcrxii"
      },
      "source": [
        "input_image = input_image.to(torch.float)\n",
        "result = operation(input_image)\n",
        "print(result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBxSs6i4r3fj"
      },
      "source": [
        "\n",
        "Observe the shape of the ```result``` with respect to the shape of the original image. We see we lose a unit around the edge of the 2D image and the output number of features reduce from 3 to 2 as required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-1Av_KQr9Zk"
      },
      "source": [
        "print(result.shape,input_image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUHb7Xvxr-hB"
      },
      "source": [
        "We can correct this using padding, as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWe2Jy2jsD4b"
      },
      "source": [
        "operation = nn.Conv2d(in_channels = 3,out_channels = 2, kernel_size = 3, padding=1)\n",
        "input_image = input_image.to(torch.float)\n",
        "result = operation(input_image)\n",
        "print(result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4UCc3hbsHRR"
      },
      "source": [
        "The result shows the result of a 2d Convolution between our image and some randomly generated kernel. What if we wanted to inspect that kernel? We can use: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w65oZ_IHsJYD"
      },
      "source": [
        "for name, param in operation.named_parameters(): # for each named parameter\n",
        "    print(name, param.data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LANliiOasMlN"
      },
      "source": [
        "Now we can see that our convolution weight tensor is of shape [2,3,3,3] (2  3×3×3  convolutional filters) and has a bias of shape [2].\n",
        "\n",
        "**To do:** \n",
        "- How many parameters is this?\n",
        "- what would be the equivalent size of a linear layer with 2 output neurons (perhaps try it out below and see)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j3_NsPVY_Wb"
      },
      "source": [
        "# answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjC-SJH28x1p"
      },
      "source": [
        "### 4.2 Max pooling\n",
        "\n",
        "The maxpool function in pytorch is [```nn.MaxPool2d```](https://pytorch.org/docs/stable/nn.html?highlight=maxpool#torch.nn.MaxPool2d). As we have seen in our lectures the max pool operation downsamples an image by selecting the maximum intensity of an image patch to represent the whole patch.\n",
        "\n",
        "#### **Exercise 2**: MaxPooling in 2D.\n",
        "\n",
        "Generate a random integer array to represent 5 images which have 3 channels are of size (100 x 100). Perform a 2D maxpool on the images using PyTorch. Your max pooling operations should have:\n",
        "\n",
        "1. filter size 3x3, stride = 1 x 1\n",
        "\n",
        "2. filter size 4 x 2, stride = 2 x 2\n",
        "\n",
        "**Hint** check the docs (linked above)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnpTxMsz8gfA"
      },
      "source": [
        "#STUDENT CODE HERE (replace Nones)\n",
        "\n",
        "random_ims = torch.randint(0, 255, (5,3,100,100)).to(torch.float)\n",
        "\n",
        "#4.2.1 implement max pool with filter size 3x3 stride 1x1\n",
        "maxpoolop = None\n",
        "print(maxpoolop)\n",
        "r = maxpoolop(random_ims)\n",
        "print(r.shape)\n",
        "\n",
        "#4.2.1 implement max pool with filter size 4x2 stride 2x2\n",
        "maxpoolop = None\n",
        "print(maxpoolop)\n",
        "r = maxpoolop(random_ims)\n",
        "print(r.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6U5evMv9jsw"
      },
      "source": [
        "### 4.3 Sequential layers\n",
        "\n",
        "It should be clear that as networks become more and more complicated, the forward function can quickly become long and cluttered. For these reasons PyTorch provides functionality to combine steps by stacking Modules in blocks usin `nn.sequential`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW1faO_daFSd"
      },
      "source": [
        "class Model(nn.Module):\n",
        " def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "    \n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, \n",
        "                      out_channels=64, kernel_size=5),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64,\n",
        "                      out_channels=128, kernel_size=5),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(320, 10)\n",
        "\n",
        "def forward(self, x):\n",
        "       x = self.conv_block1(x) \n",
        "       x = self.conv_block2(x)\n",
        "       x = x.view(-1, 320)\n",
        "       x = F.relu(self.fc1(x))\n",
        "       return F.log_softmax(x)\n",
        "\n",
        "net = Model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YdkylNfaKgH"
      },
      "source": [
        "Sequential blocks are advantageous as they run faster. The one limitation, however, is that it is then not possible to observe the outputs of the intermediate steps stacked inside. If this is required, an alternative approach  can be to use a `ModudeList` or `ModuleDict`. For more functionality on `nn.sequential` `nn.ModuleList` and `nn.ModuleDict,` please read https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok4V1elqaLyG"
      },
      "source": [
        "### 4.4 The convolutional network class\n",
        "\n",
        "Taking the example above (for sequential models) it should be clear that the structure of a CNN network remains similar to what we have seen before (for an MLP); however, this time we implement convolutional blocks from alternating convolutions, batch normalisation and activation layers.\n",
        "\n",
        "In what order should these be placed? It has been common convention to place the batchnorm before the ReLu with the pool (or downsample) at the end.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1MZjYk4PoG5CmBD1JdIH4gY5-HY3DU85M\" alt=\"Drawing\"/>\n",
        "\n",
        "The order of the ReLu and pool is actually unimportant since, as elementwise operations, they commute.\n",
        "\n",
        "More frequently nowadays pooling operations ar ereplaced with strided convolutions (for example as seen in all variants of ther ResNet)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1lB6XogE2drtqm5qs4D4UxycZ5HgMobYg\" alt=\"Drawing\"/>\n",
        "\n",
        "And in certain recent forum posts it has been suggested that greater performance is achieved through sitching the position of the batchnorm and relu:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=18iVfjvek8KAhbGLynOqDyeERAe8CHb5F\" alt=\"Drawing\"/>\n",
        "\n",
        "Dropout for regularisation has largely fallen out of favour as certain [sources](https://arxiv.org/abs/1801.05134) suggest its operations clash with batch normalisation. However, dropout remains commonly used at test time as a simple approx to estimate model uncertainty (see week 10)\n",
        "\n",
        "### 4.5 **Exercise 3** MNIST classification using a simple convolutional network\n",
        "\n",
        "We will next implement a convolutional implementation for MNIST classification and compare it against the MLP that we created in the last lecture\n",
        "\n",
        "#### 4.5.1 First we must download the MNIST dataset from torchvision and generate the DataLoaders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KkUkFkmdqLT"
      },
      "source": [
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# load datasets from torchvision - set test/train - convert to tensors\n",
        "mnist_train_dataset = datasets.MNIST(root = 'mnist_data/train', download= True, train = True, transform = transforms.ToTensor())\n",
        "mnist_test_dataset = datasets.MNIST(root = 'mnist_data/test', download= True, train = False, transform = transforms.ToTensor())\n",
        "\n",
        "# pass these to the DataLoader class to create instances for each of test and train\n",
        "# batch size is now smaller (8)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "       mnist_train_dataset, batch_size= 8, shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "       mnist_test_dataset, batch_size = 8, shuffle = True)\n",
        "\n",
        "# class labels for plotting function\n",
        "classes = ('0', '1', '2', '3',\n",
        "          '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viyj6xDYFMhi"
      },
      "source": [
        "See how we reduce the batchsize. \n",
        "\n",
        "#### 4.5.2 GPU or CPU?\n",
        "\n",
        "Before we can begin, we need to know whether we are using a GPU or a CPU. **If its GPU, the model and all the data should be uploaded into the GPU**. Fortunately, we can define a variable \"device\" that will either be cpu or gpu depending on availability, and load the data automatically on the correct 'device'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAiU2d9-FoQC"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    \n",
        "print(device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm6dTWiVd1gX"
      },
      "source": [
        "We see that if cuda is available our model will be run on a GPU, otherwise the model will need to run on CPU.\n",
        "\n",
        "**To do** It may now be worth changing the Colab runtime to GPU to speed up training\n",
        "\n",
        "#### 4.5.3 Create a CNN network class \n",
        "\n",
        "We wish to create a CNN with 2 convolutional layers, using max pooling to downsample and implementing relu non-linearities. \n",
        "\n",
        "Following the two convolutional layers we implement followed by two fully connected layers in order to compress the features to 10 output neurons (representing each class). Here, the first layer has 50 neurons, each of which will be connected to all activations from the final convolutional layer. The second fully connected layer connects the 50 neurons of the penultimate layer to 10 output neurons followed by a softmax layer to return probabilities of the raining example belonging to each class.\n",
        "\n",
        "**To Do Ex 3.1 edit the number of input and output channels of the convolutional layers**\n",
        "\n",
        "1. MNIST is grayscale; thus how many input channels do you think this will be?\n",
        "2. The first convolutional layer has a kernel size of $5 \\times 5 $ and learns 10 output channels (in other words the neurons learn 10 $5 \\times 5$ image filters. \n",
        "3. The second layer has a kernel size of $5 \\times 5 $ and learns 20 output channels. \n",
        "\n",
        "**To Do Ex 3.2 Define MaxPool2d (line 22)**\n",
        "Using the syntax shown above define a maxpool2D operation with filter size $x \\times 2$ and stride of $2$\n",
        "\n",
        "**To Do Ex 3.3 The first fully connected layer has 50 neurons and the second has 10 neurons; input the number of input and output features for these layers**\n",
        "\n",
        "The dimensions of first linear layer  are trickier to work out. Each neuron in the output must connect to each activation in the second convolutional layer. We see that the network applies $5\\times 5$ kernels with no padding and has downsamples through pooling twice. This results in the following reductions in spatial dimensions\n",
        "- conv1 downsamples from $28 \\times 28$ to $24 \\times 24$\n",
        "- maxpool1 downsamples from $24 \\times 24$ to $12 \\times 12$\n",
        "- conv2 downsamples from $12 \\times 12$ to $8 \\times 8$\n",
        "- maxpool1 downsamples from $8 \\times 8$ to $4 \\times 4$\n",
        "- Thus the activations of the final convolution layer have spatial dimensions $4 \\times 4$; how many output features is this in total?\n",
        "\n",
        "Note, if you are having problems working this out you can always print the shapes of your tensors while debugging!\n",
        "\n",
        "**However first we need to put together the forward function....**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVZNXL2Ve74L"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MNIST_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Model, self).__init__()\n",
        "        # STUDENTS CODE - replace Nones with correct code #\n",
        "        # 3.1 a) edit the number of input and output channels of the convolutional layers\n",
        "        # MNIST is grayscale; thus how many input channels do you think this will be? We want to learn 10 filters, kernel size 5x5\n",
        "        self.conv1=nn.Conv2d(None, None, kernel_size=5)\n",
        "        \n",
        "        # 3.1. b) edit the number of input and output channels of the convolutional layers\n",
        "        # the previous layer learnt 10 kernels this one shall lern 20, kernel size 5x5\n",
        "        self.conv2=nn.Conv2d(None, None, kernel_size=5)\n",
        "        \n",
        "        # 3.3 create the linear laayers with the correct numbers of input and output features of the linear layers\n",
        "        self.fc1=None\n",
        "        self.fc2=None\n",
        "\n",
        "        # 3.2 definition of maxpool\n",
        "        self.maxpool=None\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # STUDENTS CODE 3.4 construct forward function\n",
        "        # 3.4.1 implement first convolutional block conv1 -> maxpool -> relu\n",
        "        # We implement the first conv layer for you\n",
        "        x = self.conv1(x)\n",
        "        x = None\n",
        "        x = None\n",
        "        # 3.4.2 implement second convolutional block conv2 -> maxpool -> relu\n",
        "        x = None\n",
        "        x = None\n",
        "        x = None\n",
        "        # 3.4.3 implement linear layers linear ->  relu  -> linear\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = None\n",
        "        x = None\n",
        "        x = None\n",
        "\n",
        "        return F.log_softmax(x,dim=1)\n",
        "\n",
        "    \n",
        "net = MNIST_Model() \n",
        "print(net)\n",
        "net = net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoRyijVFgsdA"
      },
      "source": [
        "**To Do 3.4** Implement the forward function to contain\n",
        "\n",
        "**Ex 3.4.1 the first convolutionl block** Here the first layer is implemented for you. See it references the layer instantiated in the constructor by name `self.conv1,` the argument is the input data `x`, and the output is also called `x`. Now implement the max pool and relu (using nn.functional form). Don't forget that, each time, the output of each operation of the forward layer (here `x`) becomes the input of the next layer (also `x`). If you get stuck go back to look at the MLP from the last lecture which applies the same basic structure, just with a different combination of layers\n",
        "\n",
        "**Ex 3.4.2 the second convolutionl block** Here repeat the process but for the second convolutional layer\n",
        "\n",
        "**Ex 3.4.3 the linear layers** we implement the flattening for you. PLease implement the 2 lienar layers with *one* relu activation only (between them). The finl activation is the softmax (shown in the return statement)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA9AFY21e_2V"
      },
      "source": [
        "### 4.5.4 Loss function and optimizer\n",
        "\n",
        "We again need to define our loss and optimizers. In this case, since we're doing classification, we use **CrossEntropy Loss**, a commonly used loss function for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu9GBo15fBb4"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "loss_fun = loss_fun.to(device)\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTsEPs5gfGsi"
      },
      "source": [
        "### 4.5.4 Training \n",
        "\n",
        "We will now train our classifer model - this time iterating using enumerate. This which iterate over batches. The outer loop allows for iterating over epochs (with each epoch defining a pass through all the data) in the first instance we will leave this as 1 but if you are training on GPU you might choose to increase it a bit.\n",
        "\n",
        "**To do Ex 3.5 Implement the training interations**\n",
        "\n",
        "Implement the following steps:\n",
        "\n",
        "1. load data and labels to device\n",
        "2. clear the gradient\n",
        "3. feed the input and acquire the output from network\n",
        "4. calculating the predicted and the expected loss\n",
        "5. compute the gradient\n",
        "6. update the parameters\n",
        "\n",
        "**Hint** use examples from regression and MLP tasks\n",
        "\n",
        "**Don't forget! to clear gradients at the beginning of each epoch else they will be acculuated across each mini batch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh1-JkOLfH3o"
      },
      "source": [
        "epochs = 1\n",
        "for epoch in range(epochs): \n",
        "\n",
        "    # enumerate can be used to output iteration index i, as well as the data \n",
        "    for i, (data, labels) in enumerate(train_loader, 0):\n",
        "        # STUDENTS CODE - replace Nones with correct code #\n",
        "\n",
        "        # Task 3.5.1 load data and labels to device\n",
        "        data = None\n",
        "        labels = None\n",
        "        \n",
        "        # --------------------------------------------------task 2 ------------------------------------------------------------\n",
        "        # 3.5.2: implement training iteration here\n",
        "        # clear the gradient\n",
        "        None\n",
        "\n",
        "        #3.5.3 feed the input and acquire the output from network\n",
        "        outputs = None\n",
        "\n",
        "        #3.5.4 calculating the predicted and the expected loss\n",
        "        loss = None\n",
        "\n",
        "        #3.5.5 compute the gradient on the loss tensor\n",
        "        None\n",
        "\n",
        "        #3.5.6 update the parameters (calling the update on the optimiser object)\n",
        "        None\n",
        "        # ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        # print statistics\n",
        "        ce_loss = loss.item()\n",
        "        if i % 100 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch + 1, i + 1, ce_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCaVs4jtfNuM"
      },
      "source": [
        "### 4.5.5 Testing\n",
        "\n",
        "We will now use our trained network to make a prediction on our test set. \n",
        "\n",
        "**To do** run below code to obtain your test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdNjP5dyfQff"
      },
      "source": [
        "#make an iterator from test_loader\n",
        "test_iterator = iter(test_loader)\n",
        "#Get a batch of testing images\n",
        "images, labels = test_iterator.next()\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmmJWnb9zAzE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF1_817ofSqU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_score = net(images)\n",
        "# get predicted class from the class probabilities\n",
        "_, y_pred = torch.max(y_score, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
        "rows = 2\n",
        "columns = 4\n",
        "# plot y_score - true label (t) vs predicted label (p)\n",
        "fig2 = plt.figure()\n",
        "for i in range(8):\n",
        "    fig2.add_subplot(rows, columns, i+1)\n",
        "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
        "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
        "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6nBpZNvfVcd"
      },
      "source": [
        "#### 4.5.5.1 Computing classification scores\n",
        "\n",
        "We will now use the predictions to compute the accuracy, f1 score, precision and recall. These are scores commonly used to evaluate classification, in particular the f1 score is a good measure for datasets with imbalanced classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDzloW28fZSC"
      },
      "source": [
        "# first convert tensors to numpy\n",
        "y_true = labels.data.cpu().numpy()\n",
        "y_pred = y_pred.data.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpZC-gNgfZy1"
      },
      "source": [
        "You can use sklearn classification metrics to calculate the scores - you will \n",
        "need to input the true labels, and predicted classes.\n",
        "\n",
        "See https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNNv9TgffhSf"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk2w-7302IKX"
      },
      "source": [
        "You should see "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahZPNOcfmWd"
      },
      "source": [
        "## Ex 4 Optional Extras \n",
        "\n",
        "1. Try reducing the lines of code in the convolutional neural network class by grouping lines of code into `nn.Sequential` blocks. \n",
        "   - An example of your this can be done for linear layers is:\n",
        "```\n",
        "self.lin_blocks = nn.Sequential(\n",
        "            nn.Linear(320, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 10),\n",
        "            \n",
        "        )\n",
        "```\n",
        "  - try swapping out the linear layers of the network for this sequnetal block\n",
        "  - then create your own sequential convolutional blocks\n",
        "2. Try adding dropout and batchnorm to your convolutional blocks\n",
        "3. Try removing/changing/adding layers to see how it impacts performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0O79PmYpv-c"
      },
      "source": [
        "# STUDENTS CODE HERE\n",
        "# 4.1 copy and paste the network from above but this time swap out the conv->maxpool->relu operations (and linear layers) for sequential blocks\n",
        "\n",
        "class MNIST_Sequential_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Sequential_Model, self).__init__()\n",
        "        # 4.1 insert first sequential conv block\n",
        "        \n",
        "        # 4.1 insert sequential conv block\n",
        "        \n",
        "        \n",
        "        # lineaar sequential block\n",
        "        self.lin_blocks = nn.Sequential(\n",
        "         nn.Linear(320, 50),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(50, 10),\n",
        " \n",
        "       )\n",
        "\n",
        "      # (optional) dropout\n",
        "        self.dropout=nn.Dropout2d() # could also go in sequential blocks of course\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # 4.1 implement  with sequential blocks the foward function reduces to\n",
        "      \n",
        "        return F.log_softmax(x,dim=1)\n",
        "\n",
        "    \n",
        "net_seq = MNIST_Sequential_Model() \n",
        "print(net_seq)\n",
        "net_seq = net_seq.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjM6VOW6sCv7"
      },
      "source": [
        "# STUDENTS CODE HERE\n",
        "# regenerate optimiser for this new network\n",
        "\n",
        "optimizer = None\n",
        "\n",
        "# Now copy and pste training loop and run for the new network \n",
        "# Don't forget to change the call to the correct network object!\n",
        "\n",
        "epochs = 1\n",
        "for epoch in range(epochs): \n",
        "\n",
        "    # enumerate can be used to output iteration index i, as well as the data \n",
        "    for i, (data, labels) in enumerate(train_loader, 0):\n",
        "        # STUDENTS CODE - insert code for training loop #\n",
        "\n",
        "        \n",
        "        # print statistics\n",
        "        ce_loss = loss.item()\n",
        "        if i % 100 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch + 1, i + 1, ce_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJC61pkYzB25"
      },
      "source": [
        "# Test performance \n",
        "\n",
        "# keeping test batch constant for comparison i.e. using image and labels from above\n",
        "\n",
        "#STUDENT CODE - get prediction by implementing forwarada pass through network for test data\n",
        "y_score = None\n",
        "# get predicted class from the class probabilities\n",
        "_, y_pred = torch.max(y_score, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
        "rows = 2\n",
        "columns = 4\n",
        "# plot y_score - true label (t) vs predicted label (p)\n",
        "fig2 = plt.figure()\n",
        "for i in range(8):\n",
        "    fig2.add_subplot(rows, columns, i+1)\n",
        "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
        "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
        "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}