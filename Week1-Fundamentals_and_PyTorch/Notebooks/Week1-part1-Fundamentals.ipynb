{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.1-fundamentals.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ikjLYcvCnACs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9PySGSUUm3K"
      },
      "source": [
        "# Lecture 1: The inner workings of neural networks\n",
        "\n",
        "## The relationship between artifical and biological neurons\n",
        "\n",
        "The basic biological unit of brain computation is a neuronal cell (see figure below). Each cell is essentially an electrical device that receives signals at the dendrites (surrounding the cell body) and, once these exceed a threshold, transmits the signal down the cell axon to synaptic terminals, where it then connects with other cells.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1i3nntiP9pWUCyFNfZGPL-Rvg-8EYXftg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "Artificial neurons are designed to mimic this process, but only in a limited or constrained way. The idea is that the modulatory effect of the different synaptic connections is modelled through linear multiplication  of input signals $x_i$ with weights $w_i$, plus a bias term which shifts the centre of the prediction from zero. If the final sum is high enough, modelled through an nonlinear activation function $f$, the artificial neuron is allowed to â€˜fireâ€™ in response to that pattern of activation. \n",
        "\n",
        "Then, the optimisation process of neural networks involves learning optimal weights and biases to perform a given task. \n",
        "\n",
        "Note, while it can be useful to explain how the design of artifical neurons was motivated by the concept of biological networks, they remain far from a close simulation of the true thing. In truth design of many components of the most common artificial networks used for computer vision today are more inspired by engineering choices than any true goal to replicate human brain function. Biologically inspired neural networks  exist but, to an extent, they are a are a separate and  distinct line of research.\n",
        "\n",
        "For more details, and the source which inspired the neuron diagram, see the explanation from [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/neural-networks-1/)\n",
        "\n",
        "## A Single Neuron Logistic Regression Classifier\n",
        "\n",
        "To understand how neural networks work, it serves to consider a single neuron as a logistic regression classifier:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1bz_hR949l986NLChyme1tlD4gyg1VoF4\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "Here the line $z =0$ defines a separating hyperplane, where the bias term $w_0$ has shifted this from the origin, and all data points with $z >0$ are assigned to the positive class, and all data points with $z < 0$ are assigned the negative class.\n",
        "\n",
        "The vector $\\mathbf{W}$ runs perpendicular to the line $z =0$ and defines the direction in which data classes are maximally separated when projected onto it. \n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1PX-napPbTEAVAhe2zwoukRNbjoAZbk4p\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
        "\n",
        "## Exercise 1: Implementing a single neuron classifier through logistic regression\n",
        "\n",
        "In this section we will train a classifier to predict if a neonate is preterm based on volume measures of 86 brain volumes. We will code up and train a logistic regression classifier from scratch.\n",
        "\n",
        "### Import the data\n",
        "The data are in the file \"prem_vs_termwrois.pkl\". The final column indicates whether each data set was collected from a term or preterm baby (scanned at term equivalent age). The data represent mean vales of three different types of cortical imaging data: cortical thickness, cortical folding and cortical myelination, all averaged within 100 regions of interest ROIS on the surface. This gives 300 features in total. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ZbAn0R_ihQ4DCe1XyKaHIRZSvUQv3puh\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "There are 101 babies, 50 terms and 51 preterms. The code below loads the file and splits the data randomly into a train and test set. The data is transposed such that the rows reflect features and the columns examples (as expected from the lectures notation). A row of ones is added to each dataset to allow model;ling of the bias term.\n",
        "\n",
        "**To Do** upload the data file to your local Google Drive, update the ```file_path``` accordingly and run the below code cell.\n",
        "\n",
        "**Be sure to understand what each line is doing.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j303w0jSUm3M"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# STUDENTS CODE HERE - UPDATE THE PATH TO CORRESPOND TO WHERE YOU HAVE UPLOADED prem_vs_termwrois.pkl TO YOUR DRIVE #\n",
        "file_path='/content/drive/My Drive/Colab Notebooks/AdvancedML/2021/01_fundamentals/prem_vs_termwrois.pkl'\n",
        "# Read the data\n",
        "df = pd.read_pickle(file_path)\n",
        "data = df.values[:,:-2]\n",
        "y = df.values[:,-1]\n",
        "\n",
        "# create a test and train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create feature matrix\n",
        "X_train = X_train.T\n",
        "X_test=X_test.T\n",
        "\n",
        "#reshape y to (1 x n_T) matrix\n",
        "y_train=np.expand_dims(y_train, axis=0)\n",
        "y_test=np.expand_dims(y_test, axis=0)\n",
        "\n",
        "\n",
        "# add a row of ones for multiplication with bias term\n",
        "X_train = np.concatenate((np.ones((1,X_train.shape[1])),X_train))\n",
        "X_test = np.concatenate((np.ones((1,X_test.shape[1])),X_test))\n",
        "\n",
        "# set variables for numbers of feature and examples to improve readabiity of code\n",
        "n_features=X_train.shape[0]-1\n",
        "n_examples=X_train.shape[1]\n",
        "\n",
        "print('Dimension of X is ', X_train.shape,data.shape)\n",
        "print('Dimension of y is ', y_train.shape)\n",
        "\n",
        "print('Number of features', n_features)\n",
        "print('Number of examples', n_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of0GyW9rUm3Q"
      },
      "source": [
        "### Eyeball the data\n",
        "The following code plots histograms of a single feature (brain volume) for preterms vs terms. Run the code for a few different values of 'feature'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ydk_R3yUm3R"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#### STUDENT'S CODE HERE ####\n",
        "# try values between 0 and 300\n",
        "feature = 150\n",
        "\n",
        "plt.hist(X_train[feature+1, y_train[0]==0], bins=30)\n",
        "plt.hist(X_train[feature+1, y_train[0]==1], bins=30)\n",
        "plt.xlabel('Feature number {}/86'.format(feature))\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(['Term','Preterm']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QExCDN2aUm3T"
      },
      "source": [
        "### Model\n",
        "\n",
        "Our predictions for a single input can be written:\n",
        "$$ f= f(z) = \\dfrac{1}{1+e^{-z}} $$\n",
        "\n",
        "Where, for logistic regression, $f$ is the sigmoid function and:\n",
        "\n",
        "$$z=w_0 + w_1x_1 + w_2x_2 +w_3x_3....+w_m x_m$$\n",
        "\n",
        "Here $w_0$ is the bias term, $w_1,w_2....w_m$ are the weights;, $m$ is the number of features and $\\mathbf{x}$ is a single example (i.e. one column) from our training set $X \\in \\mathbb{R}^{m\\times n}$ .\n",
        "\n",
        "### Implementation of the forward pass\n",
        "\n",
        "We could calculate $f$ in one line of code, but it will come in handy when considering backpropagation later to consider the computation in stages, with each stage consisting of a simple module:\n",
        "\n",
        "$$\n",
        "\\begin{align} \n",
        "\\mathbf{Z} &= \\mathbf{W} \\mathbf{X} \\\\\n",
        "\\mathbf{F}=f(\\mathbf{Z}) &= \\dfrac{1}{1+e^{-\\mathbf{Z}}} \n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Implemented using vectorisation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNYIH2_LUm3V"
      },
      "source": [
        "### Task 1.1 Initialise $\\mathbf{W}$\n",
        "\n",
        "**To do** Create a matrix of zeros to initialise $\\mathbf{W}$ (note initialisation by zero is ok for a single neuron). \n",
        "\n",
        "- If $\\mathbf{X}$ has shape $(m_{features} \\times n_{examples})$, and we know that $\\mathbf{Z}$ (and thus $\\mathbf{F}$) should return _one_ scalar prediction _per example_, what shape should $\\mathbf{W}$ be? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBfjieGBUm3V"
      },
      "source": [
        "#### STUDENT'S CODE HERE ####\n",
        "# Answer:\n",
        "print(W.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re2-XSGq7KZZ"
      },
      "source": [
        "### Task 1.2 Estimate $\\mathbf{Z}$: \n",
        "\n",
        "**To do** Write a function $z(w,x)$ that uses vectorisation to linearly transform data matrix $\\mathbf{X}$ using the weights matrix $\\mathbf{W}$.\n",
        "\n",
        "**Hint** implement $\\mathbf{Z} = \\mathbf{W} \\mathbf{X}$; print out the shape - is it what you would expect?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ziYkuJOUm3T"
      },
      "source": [
        "# task 1.2 complete function to calculate Z (can be done in one line in the return statement)\n",
        "def z(w,x):\n",
        "    #### STUDENT'S CODE HERE ####\n",
        "    return \n",
        "\n",
        "output = z(W,X_train)\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JPKqUVuUm3Y"
      },
      "source": [
        "### Task 1.3 Implement Sigmoid function f: \n",
        "\n",
        "**To do** Now write a function to compute $f(\\mathbf{Z})=\\dfrac{1}{1+e^{-\\mathbf{Z}}} $, our logistic regression function:\n",
        "\n",
        "**Hint** don't forget to implement with numpy functions - to support vectorisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqLM6S0gUm3Y"
      },
      "source": [
        "# task 1.3 implement sigmoid function with vectorisation (in one line in the return statement)\n",
        "def f(z):\n",
        "  #### STUDENT'S CODE HERE ####\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXCKUvBPUm3a"
      },
      "source": [
        "**To do** Verify your softmax looks right by running this plotting code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4SHs64oUm3a"
      },
      "source": [
        "inputs = np.linspace(-10,10)\n",
        "outputs = f(inputs)\n",
        "plt.plot(inputs, outputs)\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('sigmoid(z)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WeqRIOUUm3c"
      },
      "source": [
        "We're now in a position to compute some predictions $\\mathbf{\\hat{y}}$ (**run below code cells**):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c2TznnyUm3c"
      },
      "source": [
        "y_pred = f(z(W,X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjQBHdE6Um3e"
      },
      "source": [
        "**To do** Are these predictions any good? Let's take a look at the accuracy (run below cell):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoj4OTlrUm3g"
      },
      "source": [
        "def accuracy(y, y_pred, threshold = 0.5):\n",
        "    y_pred_thresholded = y_pred > threshold\n",
        "    correct_predictions = np.sum(y==y_pred_thresholded) \n",
        "    total_predictions = y.shape[1]\n",
        "    accuracy = 100 * correct_predictions / total_predictions\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43qXm4ofUm3i"
      },
      "source": [
        "y_pred = f(z(W, X_train))\n",
        "print(accuracy(y_train, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DQzn0WRUm3k"
      },
      "source": [
        "Look at the predictions ```y_pred```, what does this initial prediction return and why? **Enter your answer in the box below**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d52gogye6n2O"
      },
      "source": [
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVEpgDgPUm3n"
      },
      "source": [
        "### Task 1.4 Implement Cross Entropy Loss:\n",
        "\n",
        "Accuracy is easy to intepret, but can't be optimised using gradient descent. We need a measure of our prediction quality that can be. A typical loss function used in  classification problems is cross-entropy:\n",
        "\n",
        "$$L(y_i,f(z_i)) = - y_i \\ln(f(z_i)) - (1-y_i) \\ln(1-f(z_i))$$\n",
        "\n",
        "This may be implemented using vectorisation as:\n",
        "\n",
        "$$L(\\mathbf{Y},\\mathbf{F}) = - \\mathbf{Y} \\ln(\\mathbf{F} + \\epsilon) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F} + \\epsilon)$$\n",
        "\n",
        "This returns a vector of losses $(L_1,L_2....L_n)$ estimated for all training examples n. The $\\epsilon$ is added for numerical stability. We require the total cost estimated as:\n",
        "\n",
        "$$ J(\\mathbf{W})= \\frac{1}{n} \\sum_i L_i(y_i,f(z_i)) $$\n",
        "\n",
        "**To do Implement the Cross-Entropy loss and return the total cost**\n",
        "\n",
        "**hint** using numpy functions for vectorisation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjKFxwASUm3o"
      },
      "source": [
        "# task 1.4 implement loss function to calculate cross-entropy loss for all examples and average to return total cost\n",
        "# note the negative sign so that the loss decreases as our predictions get better\n",
        "def loss(y, y_pred):\n",
        "    epsilon = 1e-5\n",
        "    #### STUDENT'S CODE HERE - replace Nones - ####\n",
        "    # note, we must add a small penalty term (epsilon) to prevent calculation of log(0)\n",
        "    L = None\n",
        "    J = None \n",
        "    return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftVHqyS2Um3r"
      },
      "source": [
        "total_loss= loss(y_train,y_pred)\n",
        "print(total_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbo7wTt_Um3u"
      },
      "source": [
        "## The Computation Graph\n",
        "\n",
        "Now we have our functions for $\\mathbf{L}$ and $\\mathbf{Z}$, and initialised $\\mathbf{W}$, we are finally in a position to compute a forward and backward pass. Computation graphs can help us to this by tracking the order of operations. The computation graph for logistic regression is:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1tWrFwh_lT_RVfYmodkvR_uRP8JOGfOu0\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "We can estimate the backwards pass using the chain rule:\n",
        "\n",
        "> > > > > >  <img src=\"https://drive.google.com/uc?id=14tKMEhXhlP2psxfdxrNWTt8M-LcfZL9m\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
        "\n",
        "Working backwards from the right side, this determines that to calculate the gradient of the loss with respect to the parameters we need;\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1R3qEtBPHZCwJ_e3uWwWoO5vx050vjGgg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "And don't forget that the full cost equates to the mean of the loss over all examples $J=\\frac{1}{n_T}\\sum_i L_I$ , $\\dfrac{dJ}{dW}=\\frac{1}{n_T} \\sum_i \\dfrac{dL_i}{dW} $ . All calculations should be vectorised. \n",
        "\n",
        "### Task 1.5 Implement Forward Pass\n",
        "\n",
        "\n",
        "We now have all the components of the forward pass for our logistic regression. Write a full forward pass that takes data, targets and a weight matrix and performs the forward pass, with vectorisation calculating the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAYzwvT7Um3v"
      },
      "source": [
        "# task 1.5 implement the forwards pass to calculate F and then print loss and accuracy\n",
        "def forward_pass(X, y, W):\n",
        "    #### STUDENT'S CODE HERE - replace Nones to calculate F (loss and accruacy is printed for you) - ####\n",
        "    F = None\n",
        "    print('Loss: {}'.format(loss(y,y_pred)))\n",
        "    print('Accuracy: {}'.format(accuracy(y,y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8wYS0MMUm3x"
      },
      "source": [
        "#perform forward pass\n",
        "forward_pass(X_train,y_train, W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UieT9ii0Um3z"
      },
      "source": [
        "### Task 1.6 Implement backwards pass\n",
        "\n",
        "We're now ready to try and adjust our parameters $\\mathbf{W}$ in order to optimise our predictions. To do this we need to calculate the change in our loss function with respect to our parameters, $\\dfrac{\\partial L}{\\partial \\mathbf{W}}$. \n",
        "\n",
        "Recalling our staged calculation of the logistic regression (in vectorised form):\n",
        "\n",
        "$$\n",
        "\\mathbf{Z} = \\mathbf{W} \\mathbf{X} \\\\\n",
        "\\mathbf{F}= \\dfrac{1}{1+e^{- \\mathbf{Z}}} \\\\\n",
        "\\mathbf{L}  =  - \\mathbf{Y} \\ln(\\mathbf{F}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F})\n",
        "$$\n",
        "\n",
        "We can write the vectorised gradients for each individual stage (see lecture slides and keats quiz): \n",
        "\n",
        "$$\n",
        "\\dfrac{\\partial L}{\\partial f} = \\dfrac{\\mathbf{F} - \\mathbf{Y}}{\\mathbf{F}(1-\\mathbf{F})}\\\\\n",
        "\\dfrac{\\partial f}{\\partial z} = \\mathbf{F}(1-\\mathbf{F}) \\\\\n",
        "\\dfrac{\\partial z}{\\partial w} = \\mathbf{X}^T\n",
        "$$\n",
        "\n",
        "And compose through the chain rule:\n",
        "\n",
        "$$ \n",
        "\\dfrac{\\partial L}{\\partial w} = \\dfrac{\\partial L}{\\partial f} \\cdot \\dfrac{\\partial f}{\\partial z} \\cdot\\dfrac{\\partial z}{\\partial w} \\\\\n",
        "\\dfrac{\\partial L}{\\partial w} = \\dfrac{\\mathbf{F} - \\mathbf{Y}}{\\mathbf{F}(1-\\mathbf{F})} \\cdot \\mathbf{F}(1-\\mathbf{F}) \\cdot \\mathbf{X}^T\n",
        "$$\n",
        "\n",
        "Which can be simplified by cancelling $ \\mathbf{F}(1-\\mathbf{F})$ terms in both the numerator and the denominator: \n",
        "\n",
        "$$ \\dfrac{\\partial L}{\\partial w} = (\\mathbf{F} - \\mathbf{Y}) \\mathbf{X}^T $$\n",
        "\n",
        "Let's calculate the gradient of our loss, $\\dfrac{\\partial L}{\\partial \\mathbf{W}}$, for a **single** input, $\\mathbf{x}$. \n",
        "\n",
        "**To do** Fill in the calculations of the backward pass in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TeNvOH1Um3z"
      },
      "source": [
        "W = np.zeros((1,X_train.shape[0]))\n",
        "\n",
        "# select just the first example here\n",
        "x = X_train[:,0]\n",
        "y_single = y[0]\n",
        "print('The true value of y is: {}'.format(y_single))\n",
        "\n",
        "# calculate the forward pass, and store the outputs at each stage\n",
        "Z = z(W,x)\n",
        "F = f(Z)\n",
        "print('Our prediction for y is: {}'.format(F))\n",
        "\n",
        "l = loss(y_single,F)\n",
        "print('The loss is {}'.format(l))\n",
        "\n",
        "#### STUDENT'S CODE HERE - replace Nones - ####\n",
        "# now enter the backwards pass here, \n",
        "#implementing using the equations above:\n",
        "dl_dw = None\n",
        "\n",
        "print(dl_dw.shape,W.shape,x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgLcN7B1Um31"
      },
      "source": [
        "**To do - run below code cells to check your implementation** \n",
        "\n",
        "We can check this gradient calculation is correct by updating our weights vector and looking at our new prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI3PalmtUm31"
      },
      "source": [
        "# The gradient is in the direction of increasing loss,\n",
        "# so we subtract the gradient from w.\n",
        "W = W - 0.001 * dl_dw\n",
        "Z = z(W,x)\n",
        "F = f(Z)\n",
        "l = loss(y_single,F)\n",
        "print('Our updated prediction for y is: {}'.format(F))\n",
        "print('The loss is {}'.format(l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghCcWZz0Um33"
      },
      "source": [
        "Looks good! We have only updated $\\mathbf{w}$ using information from a single data point. In practice we want to use all the data available. The following implements the gradient calculation for all data points, using vectorisation. Make sure you understand what is happening here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLNQKa0tUm34"
      },
      "source": [
        "w = np.zeros((1,X_train.shape[0]))\n",
        "\n",
        "# calculate the forward pass, and store the outputs at each stage\n",
        "Z = z(w,X_train)\n",
        "F = f(Z)\n",
        "l = loss(y_train, F)\n",
        "\n",
        "#Â To do - implement the backward pass\n",
        "dL_dw = np.matmul((F-y_train),X_train.T) \n",
        "\n",
        "print('dL_dw has shape: {}'.format(dL_dw.shape))\n",
        "\n",
        "grad_mean = dL_dw/n_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymGZxeArUm38"
      },
      "source": [
        "### Task 1.7 -  Putting it all together: the training loop\n",
        "\n",
        "We now have everything we need to train a logistic regression classifier using backprop.\n",
        "\n",
        "**To do** Fill out the training loop below referencing the code you have already written (or been given) in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRHEJ7jyUm39"
      },
      "source": [
        "# initialise w to all zeros\n",
        "w = np.zeros((1,X_train.shape[0]))\n",
        "epsilon=1e-5\n",
        "# centre X\n",
        "X_centred = np.ones_like(X_train)\n",
        "X_centred[1:] = (X_train[1:] -X_train[1:].mean(axis=1,keepdims=True)) / (X_train[1:].std(axis=1,keepdims=True)+epsilon)\n",
        "\n",
        "# we'll store the loss and accuracy in these lists during training\n",
        "loss_record = []\n",
        "accuracy_record = []\n",
        "\n",
        "num_iterations = 2000\n",
        "learning_rate = 1e-3\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # forward pass - get predictions\n",
        "    #### STUDENT CODE HERE -replace nones ####\n",
        "    # answer\n",
        "    Z = None\n",
        "    F = None\n",
        "    l = None\n",
        "    \n",
        "    # store the loss/ accuracy at this iteration\n",
        "    accuracy_it=accuracy(y_train,F)\n",
        "    loss_record.append(l)\n",
        "    accuracy_record.append(accuracy_it)\n",
        "    \n",
        "    #backwards pass to get gradients\n",
        "    #### STUDENT CODE HERE - replace nones #### \n",
        "    # answer:\n",
        "    dL_dw = None\n",
        "\n",
        "    grad_mean = None\n",
        "    \n",
        "    # update the \n",
        "    w = w - learning_rate * grad_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETOhNWPEUm3-"
      },
      "source": [
        "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
        "ax[0].plot(loss_record)\n",
        "ax[1].plot(accuracy_record)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy');\n",
        "\n",
        "print(np.max(accuracy_record))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciZE5Ps1Um4A"
      },
      "source": [
        "###  Task 1.8 Now Testing on left out set\n",
        "\n",
        "**To do** test the performance of your logistic regression on your left out test set by running below code cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kykn9UOcUm4A"
      },
      "source": [
        "# centre X\n",
        "X_test_centred = np.ones_like(X_test)\n",
        "X_test_centred[1:] = (X_test[1:] -X_test[1:].mean(axis=1,keepdims=True)) / (X_test[1:].std(axis=1,keepdims=True)+epsilon)\n",
        "\n",
        "Z_test = z(w,X_test_centred)\n",
        "F_test = f(Z_test)\n",
        "l = loss(y_test,F_test)\n",
        "\n",
        "print(l,accuracy(y_test,F_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCivWhgHUm4C"
      },
      "source": [
        "## Exercise 2 - The multi-layer perceptron (MLP)\n",
        "\n",
        "We Now want to extend this model to create a single hidden layer neural network:\n",
        "\n",
        ">  > > >  <img src=\"https://drive.google.com/uc?id=1-6-7Md_WFhe728yyDMfMfkjXavvl8jj9\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "The forward pass through such a network may be written as\n",
        "\n",
        "$$ \\hat{y} = f_2 \\left( \\mathbf{W_2} f_1 \\left(\\mathbf{W_1}\\mathbf{X}\\right) \\right) $$\n",
        "\n",
        "where $f_2$ is a non-linear activation function for the hidden layer (we use ReLu), \n",
        "\n",
        "$$ \\text{Relu}(x) = \\text{max}(0,x)$$\n",
        "\n",
        "$f_1$ is  a non-linear activation function for the output layer (we use sigmoid for classification) and  $\\mathbf{W_1}$ and $\\mathbf{W_2}$ are the weights matrices for each layer. The generic shapes of each matrix are demonstrated in the figure \n",
        "\n",
        "**In this toy example we ask you to instead create a network with 5 hidden neurons** \n",
        "\n",
        "\n",
        "**Question** Given the shape of our input data, and the fact that we are still seeking the solution to a binary classification what are the number of input and output units for this problem (answer below)? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFblPZjYUm4C"
      },
      "source": [
        "**Students Answer here**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seymm6tzUm4C"
      },
      "source": [
        "We now go about implementing our simple network from scratch with gradient descent based optimisation\n",
        "\n",
        "### The forward pass\n",
        "\n",
        "Once again, we can write the forward pass as a staged computation:\n",
        "\n",
        "$$\n",
        "\\mathbf{Z}_1 = \\mathbf{W}_1 \\mathbf{X} \\\\\n",
        "\\mathbf{F}_1 = \\text{max}(0,\\mathbf{Z_1}) \\\\\n",
        "\\mathbf{Z}_2 = \\mathbf{W}_2 \\mathbf{F}_1 \\\\\n",
        "\\mathbf{F}_2 = \\dfrac{1}{1+e^{- \\mathbf{Z_2}}} \\\\\n",
        "\\mathbf{L}  =  - \\mathbf{Y} \\ln(\\mathbf{F_2}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F_2})\n",
        "$$\n",
        "\n",
        "we give you the code for the ReLU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrrGMfaTUm4D"
      },
      "source": [
        "def relu(x):\n",
        "    return x * (x>=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9dDY7_tUm4E"
      },
      "source": [
        "Let's implement the forward pass. \n",
        "\n",
        "\n",
        "### Task 2.1 Implement a forward pass of the MLP below: \n",
        "\n",
        "Use the vectorised expressions detailed above.  What dimension must  $\\mathbf{W_1}$ and $\\mathbf{W_2}$ be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxJKRMvBUm4F"
      },
      "source": [
        "#### STUDENTS CODE HERE - replace nones ####\n",
        "# Answer\n",
        "W1 = None\n",
        "W2 = None\n",
        "Z1 = None\n",
        "F1 = None\n",
        "Z2 = None\n",
        "F2 = None # recall f is the sigmoid function\n",
        "l = loss(y_train,F2) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38DgGYSlUm4G"
      },
      "source": [
        "### The backwards pass\n",
        "\n",
        "The vectorised gradients of our MLP computation graph are, in reverse order, as follows:\n",
        "\n",
        "$$\\frac{\\delta L}{\\delta \\mathbf{F}_2}=\\frac{\\mathbf{F}_2-\\mathbf{Y}}{\\mathbf{F}_2(1-\\mathbf{F}_2)} \\\\\n",
        "\\frac{\\delta  \\mathbf{F}_2}{\\delta  \\mathbf{Z}_2}=\\mathbf{F}_2(1-\\mathbf{F}_2) \\\\\n",
        "\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{W}_2}=\\mathbf{X} \\\\\n",
        "\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{F}_1}=\\mathbf{W}^T_2\\\\\n",
        "\\frac{\\delta  \\mathbf{F}_1}{\\delta  \\mathbf{Z}_1}=1(\\mathbf{Z}_1 >0)\\\\\n",
        "\\frac{\\delta  \\mathbf{Z}_1}{\\delta  \\mathbf{W}_1}=\\mathbf{X}\\\\\n",
        "$$\n",
        "\n",
        "\n",
        "Combining these together using the chain rule we get (from lecture)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1xG9N-o0UYP836Ehr4A6FrBuq4sgsYqqW\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "\n",
        "**Task** implement the backward pass of the MLP in numpy code, and copy in the forward pass from above.\n",
        "\n",
        "**Hint** carefully consider the order in which the stages are combined (covered in the lecture). Check the dimensions of the outputs are as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1Pqke5MUm4H"
      },
      "source": [
        "epsilon = 1e-5\n",
        "\n",
        "# centre X\n",
        "X_centred = np.ones_like(X_train)\n",
        "X_centred[1:] = (X_train[1:] -X_train[1:].mean(axis=1,keepdims=True)) / (X_train[1:].std(axis=1,keepdims=True)+epsilon)\n",
        "\n",
        "# initialise w1, w2\n",
        "W1 = np.random.randn(5,X_train.shape[0])\n",
        "W2 = np.random.randn(1,5)\n",
        "\n",
        "# we'll store the loss and accuracy in these lists during training\n",
        "loss_record_mlp = []\n",
        "accuracy_record_mlp = []\n",
        "\n",
        "num_iterations = 2000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # forward pass - get predictions\n",
        "    \n",
        "    #### STUDENT CODE HERE - replace Nones copying code from above ####\n",
        "    Z1 = None\n",
        "    F1 = None\n",
        "    Z2 = None\n",
        "    F2 = None  # recall f is the sigmoid function\n",
        "    l = loss(y_train,F2) \n",
        "\n",
        "    # store the loss/ accuracy at this iteration\n",
        "    loss_record_mlp.append(l)\n",
        "    accuracy_record_mlp.append(accuracy(y_train,F2))\n",
        "\n",
        "    \n",
        "    #backwards pass to get gradients\n",
        "    #### STUDENT CODE HERE - replace Nones  ####\n",
        "\n",
        "    dL_dW2=None\n",
        "    dL_df2=None \n",
        "    df2_dZ1  = 1.0 *(Z1> 0)\n",
        "    \n",
        "    dL_dZ1=None\n",
        "    dL_dW1 = None\n",
        "    dJ_dW2=(1/W2.shape[0])*dL_dW2 \n",
        "    dJ_dW1=(1/W1.shape[0])*dL_dW1 \n",
        "\n",
        "    # update the weights\n",
        "    W2 = W2 - learning_rate * dJ_dW2    \n",
        "    W1 = W1 - learning_rate * dJ_dW1\n",
        "    \n",
        "# plot loss and accuracy    \n",
        "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
        "ax[0].plot(loss_record_mlp)\n",
        "ax[1].plot(accuracy_record_mlp)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy');\n",
        "\n",
        "print(accuracy(y_train,F2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEAXgVTyUm4I"
      },
      "source": [
        "### Testing the performance of the MLP\n",
        "\n",
        "**To do** test the performance of your logistic regression by running the code on your left out test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65itxO2xUm4J"
      },
      "source": [
        "Z1_test = np.matmul(W1,X_test_centred)\n",
        "F1_test = relu(Z1_test)\n",
        "Z2_test = np.matmul(W2,F1_test)\n",
        "F2_test = f(Z2_test) \n",
        "l = loss(y_test,F2_test) \n",
        "\n",
        "print(l,accuracy(y_test,F2_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPiM5AgUm4L"
      },
      "source": [
        "## Exercise 3 (week 2) Adding Regularisation\n",
        "\n",
        "One problem with neural networks is that they can involve the training of very high numbers of parameters (defined by the total number of elements in all our weights matrices). The more parameters we can chose from the greater the chance of overfitting \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ZKuwPsBML5vU41hQI0jM8YGwWFwdOP6v\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "There are several ways of controlling the capacity of Neural Networks to prevent overfitting. These include\n",
        "\n",
        "1. L1 and L2 regularisation â€“ penalise the network through addition of a penalty term i.e. \n",
        "\n",
        "$$ J =\\frac{1}{n} (\\sum_i L_i + \\lambda <\\textrm{penalty term}>)$$\n",
        "\n",
        "2. Dropout - during training keep only a subset of neurons active (with probability ð‘); set to zero otherwise.\n",
        "\n",
        "Dropout will be considered in more detail in lecture 4. Here, we will consider the inclusion of a penalty term. Of these L2 is the most common.  This requires a penalty of $\\lambda/2 â€–\\mathbf{W}â€–^2$ (where the 1/2  term is used to make gradient $\\lambda â€–\\mathbf{W}â€– $ rather than $2 \\lambda â€–\\mathbf{W}â€– $). L2 regularisation encourages the network to learn diffuse weights (small weights spread across all units). On the other hand, L1 has a penalty $\\lambda â€–\\mathbf{W}â€– $ and this encourages the learning of sparse weights, where many individual weights are set to zero.\n",
        "\n",
        "**To do** lets try adding L2 regularisation to our MLP network. First write a new loss function which estimates a L2 regularised loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcV1kwfUm4L"
      },
      "source": [
        "def regularised_loss(y, y_pred, W1,W2, lambda_term,epsilon):\n",
        "    '''\n",
        "        Estimate loss with L2 regularisation\n",
        "        \n",
        "    input:\n",
        "        y: True label\n",
        "        y_pred: estimate label\n",
        "        W1: weights matrix for hidden layer\n",
        "        W2: weights matrix for output layer\n",
        "        lambda_term: equals weighting for penalty term\n",
        "        epsilon: small term to prevent log(0)\n",
        "    returns:\n",
        "        J: L2 regularised cross entropy loss\n",
        "        \n",
        "    '''\n",
        "    # STUDENTS IMPLEMENT REGULARISATION TERM\n",
        "    reg=None\n",
        "    L = - y * np.log(y_pred+epsilon) - (1-y) * np.log(1-y_pred+epsilon)\n",
        "    J = np.mean(L) + reg\n",
        "    return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIZKVwYwUm4N"
      },
      "source": [
        "Now making a copy of the code from above but changing the call to the loss function \n",
        "\n",
        "**To do** update the gradients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY59w324Um4N"
      },
      "source": [
        "epsilon = 1e-5\n",
        "\n",
        "# define lambda - penalty term weighting \n",
        "#(can't use variable name lambda as it is a python operation)\n",
        "lambda_term=5e-4 \n",
        "# centre X\n",
        "X_centred = np.ones_like(X_train)\n",
        "X_centred[1:] = (X_train[1:] -X_train[1:].mean(axis=1,keepdims=True)) / (X_train[1:].std(axis=1,keepdims=True)+epsilon)\n",
        "\n",
        "# initialise w1, w2\n",
        "W1 = np.random.randn(5,X_train.shape[0])\n",
        "W2 = np.random.randn(1,5)\n",
        "\n",
        "# we'll store the loss and accuracy in these lists during training\n",
        "loss_record_mlp = []\n",
        "accuracy_record_mlp = []\n",
        "\n",
        "num_iterations = 1000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # forward pass - get predictions\n",
        "    \n",
        "    Z1 = np.matmul(W1,X_centred)\n",
        "    F1 = relu(Z1)\n",
        "    Z2 = np.matmul(W2,F1)\n",
        "    F2 = f(Z2) # recall f is the sigmoid function\n",
        "    \n",
        "    l = regularised_loss(y_train,F2,W1,W2,lambda_term,epsilon)\n",
        "\n",
        "    # store the loss/ accuracy at this iteration\n",
        "    loss_record_mlp.append(l)\n",
        "    accuracy_record_mlp.append(accuracy(y_train,F2))\n",
        "\n",
        "    \n",
        "    #backwards pass to get gradients\n",
        "    dL_dW2=np.matmul(F2-y_train,F1.T) \n",
        "    dL_df2=np.matmul(W2.T,F2-y_train)  \n",
        "    df2_dZ1  = 1.0 *(Z1> 0)\n",
        "    \n",
        "    dL_dZ1=np.multiply(dL_df2,df2_dZ1)\n",
        "    dL_dW1 = np.matmul(dL_dZ1,X_centred.T)\n",
        "    dJ_dW2=(1/W2.shape[0])*dL_dW2 \n",
        "    dJ_dW1=(1/W1.shape[0])*dL_dW1 \n",
        "    \n",
        "    # STUDENTS CODE HERE - replace nones ###\n",
        "    #update the loss to add the gradient of the penalty term\n",
        "    dJ_dW2+=None\n",
        "    dJ_dW1+=None\n",
        "\n",
        "    # update the weights\n",
        "    W2 = W2 - learning_rate * dJ_dW2    \n",
        "    W1 = W1 - learning_rate * dJ_dW1\n",
        "    \n",
        "# plot loss and accuracy    \n",
        "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
        "ax[0].plot(loss_record_mlp)\n",
        "ax[1].plot(accuracy_record_mlp)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy');\n",
        "\n",
        "print(accuracy(y_train,F2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itxyG7FPUm4P"
      },
      "source": [
        "Testing ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY7YtW9PUm4P"
      },
      "source": [
        "Z1_test = np.matmul(W1,X_test_centred)\n",
        "F1_test = relu(Z1_test)\n",
        "Z2_test = np.matmul(W2,F1_test)\n",
        "F2_test = f(Z2_test) \n",
        "l = loss(y_test,F2_test) \n",
        "\n",
        "print(l,accuracy(y_test,F2_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK8ZiUE6Um4R"
      },
      "source": [
        "## Homework\n",
        "\n",
        "1. Using multiclass data implement a softmax multi-class classifier as \n",
        "    a) a single neuron\n",
        "    b) an MLP\n",
        "2. Try using a tanh or leaky relu in place of the relu function in the MLP classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACmMAgoAUm4R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}