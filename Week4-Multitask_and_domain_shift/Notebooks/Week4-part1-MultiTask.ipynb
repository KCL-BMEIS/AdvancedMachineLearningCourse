{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Week4-part1-MultiTask.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VG1nz3PA0P5d"},"source":["**NOTE:** before to start this notebook go to edit-> Notebook settings and select GPU as a hardware accelerator. Now your ready to go!\n","\n","# Part 0: Coding a basic multi-task network\n","\n","We will begin today's exercises by converting a standard single output network into a multi-task one. Our base network is a simplified version of a ResNet (Residual network), designed to carry out age regression. We will seek to extend it by adding an additional branch that will learn to classify the images according to sex.\n","<Include image here of network before and after>"]},{"cell_type":"code","metadata":{"id":"ZKONk82a0P5Z"},"source":["%matplotlib inline\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K55VrYsO0P5e"},"source":["# Training and the data\n","\n","We will be using a large-scale face dataset, [UTKface](https://susanqq.github.io/UTKFace/). It consists of 23705 images of faces with ages ranging between 0 and 116, labelled according to age, sex, and ethnicity. While the dataset contains images of varying poses and zooms, we will only be using a subset of cropped and aligned faces, to make our jobs a little easier.\n","\n","### Importing the data\n","The dataset consists of jpeg images, where the labels are contained in the name with the format: __[age]\\_[sex]\\_[ethnicity]_[date+time].jpg__  \n","Let's begin by importing the necessary libraries for the notebook and showcasing the functions that will allow you to load in the dataset and extract the labels contained in each image:"]},{"cell_type":"code","metadata":{"id":"nDm8fhGE1P1O"},"source":["file_download_link = \"https://github.com/KCL-BMEIS/AdvancedMachineLearningCourse/blob/main/Week4-Multitask_and_domain_shift/Data/UTKFace.zip?raw=true\"\n","!wget -O UTKFace.zip --no-check-certificate \"$file_download_link\"\n","!unzip UTKFace.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hItmsKoP0P5i"},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data.dataloader import default_collate\n","import glob\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# A list containing common image file extensions for verification purposes\n","IMG_EXTENSIONS = [\n","    '.jpg', '.JPG', '.jpeg', '.JPEG',\n","    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n","]\n","\n","\n","# Checks if the passed file is an image\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n","\n","\n","# A function to extract each of the relevant labels contained in the filename: We exclude data and time\n","def extract_labels(filename):\n","    age, sex, ethnicity = filename.split('_')[:3]\n","    return age, sex, ethnicity\n","\n","\n","# A function for creating tuples of filenames and their corresponding labels for loaded in data\n","def make_dataset(dir):\n","    images = []\n","    for filename in glob.glob(os.path.join(dir, '*_*_*_*')):\n","        if is_image_file(filename):\n","            filename = os.path.basename(filename)\n","            age, sex, race = extract_labels(filename)\n","            item = (filename, [age, sex, race])\n","            images.append(item)\n","    return images\n","\n","\n","# Simple image RGB converter\n","def default_loader(path):\n","    return Image.open(path).convert('RGB')\n","\n","\n","# Class for image loading\n","class ImageFolder(torch.utils.data.Dataset):\n","    def __init__(self, root, transform=None, target_transform=None,\n","                 loader=default_loader):\n","        imgs = make_dataset(root)\n","\n","        self.root = root\n","        self.imgs = imgs\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.loader = loader\n","\n","    def __getitem__(self, index):\n","        # Target is label(s)\n","        path, target = self.imgs[index]\n","        try:\n","          img = self.loader(os.path.join(self.root, path))\n","        except OSError:\n","          return None\n","        # Convert strings to float\n","        target = [float(tar) for tar in target]\n","        # Let's divide the age by the maximum, 116, to normalize ages between\n","        # 0 and 1\n","        target[0] = target[0] / 116.0\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    \n","# Function for actually loading the dataset according to minibatch size and train/ test splits\n","def load_dataset(path, batch_size=4, test_split=0.2):\n","    # Need to call class instance, NOT just class:\n","    dataset = ImageFolder(root=path, transform=transforms.ToTensor(), target_transform=None)\n","    dataset_size = len(dataset)\n","    print('Dataset size is {}'.format(dataset_size))\n","    indices = list(range(dataset_size))\n","    split = np.int(np.floor(test_split * dataset_size))\n","    test_indices, train_indices = indices[:split], indices[split:]\n","    train_sampler = SubsetRandomSampler(train_indices)\n","    test_sampler = SubsetRandomSampler(test_indices)\n","\n","    def my_collate(batch):\n","      batch = list(filter(lambda x:x is not None, batch))\n","      return default_collate(batch)\n","    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2, collate_fn=my_collate)\n","    test_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=2, collate_fn=my_collate)\n","\n","    return train_dataloader, test_dataloader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"21OGbS1S0P5k"},"source":["### Visualising the dataset\n","Let's have a look at some example images to give us some insight into the dataset. For each image we also want to see the corresponding labels."]},{"cell_type":"code","metadata":{"id":"IOVvChf_0P5l"},"source":["def imshow(img):\n","    npimg = img.numpy()\n","    # (RGB, x, y) -> (x, y, RGB)\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# Get some random training images\n","# Calls trainloader which grabs 4 images (same size as batch_size)\n","img_path = '/content/UTKFace'\n","trainloader, testloader = load_dataset(img_path)\n","\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# Show images\n","imshow(torchvision.utils.make_grid(images))\n","# print labels\n","age_labels = [age*116 for age in labels[0]]\n","print(' '.join('Age: %5s' % int(j.item()) for j in age_labels))\n","print(' '.join('Sex: %5s' % labels[1][j].numpy() for j in range(4)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTJA-roP0P5n"},"source":["# The base network\n","The network is based on [ResNet-18](https://arxiv.org/pdf/1512.03385.pdf):\n","\n","<br>\n","<div>\n","<center>\n","<img src=\"https://github.com/pedrob37/AML-Lecture-Materials/blob/master/AML_images/ResNet18.png?raw=true\" width=\"300\"/>\n","</center>\n","</div>\n","\n","A few changes have been made (due to the limited resources available in this session) that will still allow for the network to make good predictions in a reasonable period of time.\n","\n","### The Residual Block\n","ResNet-18 consists of a few residual blocks: Two sequential convolutions that downsample the features followed by a concatenation of the downsampled original features:\n","\n","<br>\n","<div>\n","<center>\n","<img src=\"https://github.com/pedrob37/AML-Lecture-Materials/blob/master/AML_images/ResBlock.png?raw=true\" width=\"500\"/>\n","</center>\n","</div>\n","\n","While not strictly necessary, it is advantageous for clarity reasons to have the residual blocks be instances of a separate class. Similarly, because we make frequent use of both 3x3 and 1x1 convolutions it is worth creating functions for these as illustrated below:"]},{"cell_type":"code","metadata":{"id":"O3JKg_mX0P5o"},"source":["def conv3x3(in_channels, out_channels, stride=1, dilation=1, padding=1):\n","    \"\"\"3x3 convolution: Supports padding\"\"\"\n","    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, padding=padding,\n","                     kernel_size=3, stride=stride, dilation=dilation)\n","def conv1x1(in_channels, out_channels, stride=1):\n","    \"\"\"1x1 convolution: Supports padding\"\"\"\n","    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, padding=0,\n","                     kernel_size=1, stride=stride)\n","class ResBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, norm_layer=None, downsample_flag=True):\n","        super(ResBlock, self).__init__()\n","        # Set the normalisation to batch norm if none is set\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        # Define a ReLU layer\n","        self.relu = nn.ReLU(inplace=True)\n","        # Define a 3x3 conv layer\n","        self.conv1 = conv3x3(in_channels=in_channels, out_channels=out_channels, stride=stride)\n","        # Define first batch norm layer\n","        self.bn1 = norm_layer(out_channels)\n","        # Don't want to reduce dimensionality further in second conv, hence stride 1\n","        self.conv2 = conv3x3(in_channels=out_channels, out_channels=out_channels, stride=1)\n","        # Define second batch norm layer\n","        self.bn2 = norm_layer(out_channels)\n","        # Downsampling\n","        self.downsample_flag = downsample_flag\n","        if downsample_flag is not None:\n","            self.downsample = nn.Sequential(\n","                conv1x1(in_channels, out_channels, stride),\n","                norm_layer(out_channels),\n","            )\n","    def forward(self, x):\n","        # Move forward through the network layers\n","        identity = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        if self.downsample_flag is not None:\n","            # Need to downsample x due to shape incompatibility after convolutions if stride > 1\n","            identity = self.downsample(x)\n","        # This is a residual block so we concatenate the original (downsampled) x to the previous layer's output\n","        out += identity\n","        out = self.relu(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qAk_Entn0P5q"},"source":["Having created our ResBlock we can move onto the network, which simply consists of a series of ResBlocks followed by some fully connected layers."]},{"cell_type":"code","metadata":{"id":"-frSfTgc0P5r"},"source":["# Network design\n","class ResNet(nn.Module):\n","    def __init__(self, block, first_output_channels=64, norm_layer=None):\n","        # Initialises the parent class: nn.module\n","        super(Net, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","        self.first_output_channels = first_output_channels\n","        self.conv1 = nn.Conv2d(3, self.first_output_channels, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = norm_layer(self.first_output_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        # Each layer here is a residual block, which we can call\n","        self.layer1 = block(self.first_output_channels, self.first_output_channels, stride=1)\n","        self.layer2 = block(self.first_output_channels, self.first_output_channels*2, stride=2)\n","        self.layer3 = block(self.first_output_channels*2, self.first_output_channels*4, stride=2)\n","        self.layer4 = block(self.first_output_channels*4, self.first_output_channels*8, stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc1 = nn.Linear(512, 100)\n","        self.fc2 = nn.Linear(100, 20)\n","        self.fc3 = nn.Linear(20, 1)\n","\n","    def forward(self, image):\n","        x = self.conv1(image)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        # ResBlocks\n","        x = self.layer1.forward(x)\n","        x = self.layer2.forward(x)\n","        x = self.layer3.forward(x)\n","        x = self.layer4.forward(x)\n","\n","        x = self.avgpool(x)\n","        # In preparation to pass through a fully connected layer, flatten all features into 1D\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","\n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UowRbJix0P5t"},"source":["# Part 1: Modify the network to allow for multi-task\n","Having shown that the network works for a single task, it is now time for you to make some modifications to allow for multi-task to be supported. In this case I would like you to modify the network so that in addition to age regression a sex prediction is also made. While there are many valid multi-task architectures you could opt for, for consistency I would like each task-specific branch to have one ResBlock, and the shared layers to have two.\n","\n","Our total loss will be:\n","\n","$\\mathcal{L}_{total} = \\mathcal{L}_{age} + \\lambda \\mathcal{L}_{sex}$\n","\n","where $\\mathcal{L}_{age}$ is the L1 loss (see Losses lecture) and $\\mathcal{L}_{sex}$ is the binary cross-entropy (see Losses lecture). The $\\lambda$ determines the relative contribution of each task to the total loss.\n","\n","Remember that for most multi-task networks the first few layers will be shared and the final few will be task-specific. Also be careful about choosing the output size, consider the task at hand."]},{"cell_type":"code","metadata":{"id":"r85d9bX90P5t"},"source":["class MultiNet(nn.Module):\n","    # Remember to set the num_classes variable!\n","    def __init__(self, block, first_output_channels=128, num_classes=None, norm_layer=None):\n","        # Initialises the parent class: nn.module\n","        super(MultiNet, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","        self.first_output_channels = first_output_channels\n","        # Shared layers: Similar to before \n","        self.conv1 = nn.Conv2d(3, self.first_output_channels, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = norm_layer(self.first_output_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = block(self.first_output_channels, self.first_output_channels, stride=1)\n","        self.layer2 = block(self.first_output_channels, self.first_output_channels * 2, stride=2)\n","        self.layer3 = block(self.first_output_channels * 2, self.first_output_channels * 4, stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Task 1 specific layers\n","        self.fc1_x = nn.Linear(self.first_output_channels * 4, 100)\n","        self.fc2_x = nn.Linear(100, 20)\n","        self.fc3_x = nn.Linear(20, 1)\n","\n","        # Task 2 specific layers\n","        # YOUR CODE HERE\n","\n","    def forward(self, image):\n","        shared_xy = self.conv1(image)\n","        shared_xy = self.bn1(shared_xy)\n","        shared_xy = self.relu(shared_xy)\n","        shared_xy = self.maxpool(shared_xy)\n","        shared_xy = self.layer1.forward(shared_xy)\n","        shared_xy = self.layer2.forward(shared_xy)\n","        shared_xy = self.layer3.forward(shared_xy)\n","        shared_xy = self.avgpool(shared_xy)\n","        shared_xy = torch.flatten(shared_xy, 1)\n","\n","        # Task 1 specific layers: Regression as before\n","        x = self.fc1_x(shared_xy)\n","        x = self.fc2_x(x)\n","        x = self.fc3_x(x)\n","\n","        # Task 2 specific layers: Classification\n","        # YOUR CODE HERE\n","        return x, y\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mHMcWKji0P5v"},"source":["# Network evaluation\n","Now that you've modified the network, let's see how well it trains:"]},{"cell_type":"code","metadata":{"id":"zVWoh8t90P5x"},"source":["# Model training\n","# Create network instance\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","multi_resnet = MultiNet(ResBlock)\n","multi_resnet.cuda(device)\n","number_of_epochs = 2\n","\n","# Define a loss and optimiser\n","task1_criterion = nn.MSELoss()\n","# Cross entropy is used because this is a classification task\n","task2_criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(multi_resnet.parameters(), lr=0.001)\n","\n","# Train the network\n","task1_loss_array_stag = []\n","task2_loss_array_stag = []\n","loss_array = []\n","for epoch in range(number_of_epochs):\n","  # Keep track of some running losses\n","    running_loss = 0\n","    task1_running_loss = 0\n","    task2_running_loss = 0\n","    for iteration, data in enumerate(trainloader):\n","        inputs, full_labels = data\n","        if inputs is None or full_labels is None:\n","          continue\n","        # Extract the labels for each task\n","        age_labels = full_labels[0]\n","        sex_labels = full_labels[1]\n","        # Pass the inputs and labels to the CUDA device\n","        inputs = inputs.to(device, dtype=torch.float)\n","        age_labels = age_labels.to(device, dtype=torch.float)\n","        sex_labels = sex_labels.to(device, dtype=torch.long)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        age_regression, sex_classification = multi_resnet(inputs)\n","\n","        # Calculate loss\n","        task1_loss = task1_criterion(age_regression.squeeze(), age_labels) ** 0.5\n","        task2_loss = task2_criterion(sex_classification.squeeze(), sex_labels)\n","        total_loss = task1_loss + task2_loss\n","\n","        # Back-propagation of gradients\n","        total_loss.backward()\n","\n","        # Optimize\n","        optimizer.step()\n","\n","        # Print statistics\n","        loss_array.append(total_loss)\n","        running_loss += total_loss.item()\n","        task1_running_loss += task1_loss.item()\n","        task2_running_loss += task2_loss.item()\n","        if iteration % 50 == 49:  # Print every 50 mini-batches, weird choice of numbers to exclude zero\n","            print('Epoch: {:.0f}, Iteration: {:.0f}, Total Loss: {:.3f}, '\n","                  'Task1 Loss: {:.3f}, '\n","                  'Task2 Loss: {:.3f}'.format(epoch + 1, iteration + 1,\n","                                              running_loss / 50,\n","                                              task1_running_loss / 50,\n","                                              task2_running_loss / 50))\n","            task1_loss_array_stag.append(task1_running_loss)\n","            task2_loss_array_stag.append(task2_running_loss)\n","            task1_running_loss = 0\n","            task2_running_loss = 0\n","            running_loss = 0\n","\n","print('Finished Training')\n","\n","# Plots\n","fig, ax1 = plt.subplots()\n","ax1.plot(list(range(len(task1_loss_array_stag))), [loss_arr for loss_arr in task1_loss_array_stag], label='Task 1 loss', c='r')\n","ax1.set_ylabel('MSE')\n","ax2 = ax1.twinx()\n","ax2.plot(list(range(len(task2_loss_array_stag))), [loss_arr for loss_arr in task2_loss_array_stag], label='Task 2 loss')\n","ax2.set_ylabel('Accuracy')\n","plt.title('Losses for both tasks')\n","plt.xlabel('Iterations')\n","ax1.legend(loc='upper left')\n","ax2.legend(loc='best')\n","plt.grid()\n","fig.tight_layout()  # otherwise the right y-label is slightly clipped\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ph-memZK0P50"},"source":["Let's evaluate the network's performance on both tasks across all of the test images:"]},{"cell_type":"code","metadata":{"id":"5_Qsxqj80P50"},"source":["def evaluate_model(model, testloader):\n","  print('EVALUATION')\n","  total_correct = 0\n","  task1_correct = 0\n","  task2_correct = 0\n","  total = 0\n","  with torch.no_grad():\n","      for data in testloader:\n","          images, labels = data\n","          labels = torch.stack(labels)\n","          images = images.to(device)\n","          labels = labels.to(device)\n","          # Evaluate model predictions for test examples\n","          age_regression, sex_classification = model(images)\n","          total += images.size(0)\n","          task1_correct += task1_criterion(torch.squeeze(age_regression), labels[0].float()).sum().item()\n","          task2_correct += (torch.max(sex_classification, 1)[1] == labels[1].long()).sum().item()\n","          total_correct = task1_correct + task2_correct\n","  # print('Total Accuracy of the network on the test images: {}'.format(total_correct / total))\n","  print('Task 1 (Age) MSE of the network on the test images: {}'.format((task1_correct / total * 116.0)**0.5))\n","  print('Task 2 (Sex) accuracy of the network on the test images: {}'.format(task2_correct / total))\n","evaluate_model(multi_resnet, testloader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v0-aIJWhVXmf"},"source":["# Part 2: Manual loss weighting\n","\n","We've trained our network and evaluated it. Note down these numbers and repeat the training with a different choice of lambda. Try $\\lambda = 10$ and $\\lambda = 0.1$ . How do you expect the performance on Task 1 and Task 2 to differ? Try and find the value of $\\lambda$ which maximises performance across both tasks."]},{"cell_type":"markdown","metadata":{"id":"9ni98ruT0P53"},"source":["# Part 3: Gradient Normalisation\n","One of the techniques used to address task dominance imbalances is gradient normalisation, or GradNorm. In GradNorm the weights used to modulate the task losses are learnable parameters themselves that are adjusted according to the relative size of the gradients that each of the tasks propagates.  \n","\n","<br>\n","<div>\n","<center>\n","<img src=\"https://github.com/pedrob37/AML-Lecture-Materials/blob/master/AML_images/GradNorm.png?raw=true\" width=\"800\"/>\n","</center>\n","</div>\n","\n","\n","<br>\n","<div>\n","<center>\n","<img src=\"https://github.com/pedrob37/AML-Lecture-materials/blob/master/AML_images/GradNormLoop.png?raw=true\" width=\"500\"/>\n","</center>\n","</div>\n","\n","You will be partially implementing GradNorm. The main steps are:\n","1. Calculate, for each loss, its value relative to its starting value: $\\hat{L_i}$\n","2. Calculate an average between these two relative losses: $L_{avg}$\n","3. Calculate, for each task, the inverse training rate: $r_i=\\frac{\\hat{L_i}}{L_{avg}}$\n","4. Calculate, for each task, the norm of the gradient at the FINAL shared layer: $|G_{i}|$\n","5. Calculate the average of these gradients: $G_{avg}$\n","6. Calculate the desired gradient for each task: $G_{des:i}=G_{avg}r_i^{\\alpha}$\n","7. Compute a loss that shows the difference between the current and desired gradient: $L_{grad:i}=|G_{i}-G_{des:i}|$\n","8. Average this loss between the tasks and propagate it: $L_{avg}$\n","9. Renormalise the weights\n","\n","If $r_i$ is small, it means that the loss has decreased considerably with regards to its starting value, or with regards to the average, which takes into account other losses $L_j$. Thus, the smaller this value, the \"higher\" the training rate is. This is why we speak about **inverse** training rate.\n","\n","The standard training loop is provided:"]},{"cell_type":"code","metadata":{"id":"o_8bkdw80P54"},"source":["# Initialise the loss weight parameters\n","w1 = torch.FloatTensor([1]).to(device).detach().requires_grad_(True)\n","w2 = torch.FloatTensor([1]).to(device).detach().requires_grad_(True)\n","params = [w1, w2]\n","scaling = w1 + w2\n","\n","# GradLoss\n","GradLoss = nn.L1Loss()\n","\n","# Optimisations\n","opt1 = torch.optim.Adam(multi_resnet.parameters(), lr=0.001)\n","opt2 = torch.optim.Adam(params, lr=0.0005)\n","\n","# Rate initialisation\n","rate_1 = 1\n","rate_2 = 1\n","alpha = 0.5\n","\n","# Train the network\n","loss_array = []\n","loss_array_stag = []\n","task1_loss_array_stag = []\n","task2_loss_array_stag = []\n","# Given: List of shared layers\n","shared_params = [p[1] for p in multi_resnet.named_parameters() if 'fc' not in p[0] and 'downsample' not in p[0]]\n","for epoch in range(number_of_epochs):\n","    running_loss = 0\n","    task1_running_loss = 0\n","    task2_running_loss = 0\n","    for iteration, data in enumerate(trainloader):\n","        inputs, full_labels = data\n","        # Load in labels and inputs\n","        age_labels = full_labels[0]\n","        sex_labels = full_labels[1]\n","        # Pass labels and inputs to device\n","        inputs = inputs.to(device, dtype=torch.float)\n","        age_labels = age_labels.to(device, dtype=torch.float)\n","        sex_labels = sex_labels.to(device, dtype=torch.long)\n","\n","        # Forward pass\n","        age_regression, sex_classification = multi_resnet(inputs)\n","\n","        # Current Losses\n","        L1_t = torch.mul(params[0], task1_criterion(age_regression.squeeze(), age_labels) ** 0.5)\n","        L2_t = torch.mul(params[1], task2_criterion(sex_classification.squeeze(), sex_labels))\n","        total_loss = torch.add(L1_t, L2_t)\n","\n","        # Calculating the initial value for each task loss\n","        if iteration == 0:\n","            L1_0 = L1_t.detach()\n","            L2_0 = L2_t.detach()\n","\n","        opt1.zero_grad()\n","        # Back-propagation of gradients\n","        total_loss.backward(retain_graph=True)\n","\n","        # Step 1: Relative losses\n","        Lhat_1 = # YOUR CODE HERE\n","        Lhat_2 = # YOUR CODE HERE\n","\n","        # Step 2: Average relative losses\n","        Lhat_avg = # YOUR CODE HERE\n","\n","        # Step 3: Calculate rates\n","        rate_1 = # YOUR CODE HERE\n","        rate_2 = # YOUR CODE HERE\n","\n","        # Step 4: Gradient calculation for FINAL shared layer\n","        # Task 1:\n","        G1 = torch.autograd.grad(L1_t, shared_params[-1], retain_graph=True, create_graph=True)[0]\n","        G1_norm = torch.norm(G1, 2)\n","\n","        # Task 2:\n","        # YOUR CODE HERE\n","\n","        # Step 5: Gradient averaging\n","        G_avg = # YOUR CODE HERE\n","\n","        # Step 6: Desired gradient computation\n","        G1_des = # YOUR CODE HERE\n","        G2_des = # YOUR CODE HERE\n","\n","        # Step 7: Gradient losses\n","        L1_G = GradLoss(G1_norm, G1_des.detach())\n","        L2_G = # YOUR CODE HERE\n","\n","        opt2.zero_grad()\n","\n","        # Step 8: Gradient loss averaging and backprop\n","        L_G_avg = # YOUR CODE HERE\n","        L_G_avg.backward()\n","\n","        # Update steps\n","        opt1.step()\n","        opt2.step()\n","\n","        # Step 9: Renormalisation: To ensure that the sum of the weights is always 2\n","        renorm = torch.div(torch.add(w1, w2), scaling)\n","        params = [torch.div(w1, renorm), torch.div(w2, renorm)]\n","\n","        # Print statistics\n","        # loss_array.append(total_loss)\n","        running_loss += total_loss.item()\n","        task1_running_loss += L1_t.item()\n","        task2_running_loss += L2_t.item()\n","        if iteration % 50 == 49:  # Print every 500 mini-batches, weird choice of numbers to exclude zero\n","            print('Epoch: {:.0f}, Iteration: {:.0f}, Total Loss: {:.3f}, '\n","                  'Task1 Loss: {:.3f}, '\n","                  'Task2 Loss: {:.3f}'.format(epoch + 1, iteration + 1,\n","                                              running_loss / 50,\n","                                              task1_running_loss / 50,\n","                                              task2_running_loss / 50))\n","            # print('Parameters are currently: {}'.format([param.item() for param in params]))\n","            print('Rates 1 and 2: [{:.3f}] [{:.3f}]'.format(rate_1.cpu().detach().numpy()[0], rate_2.cpu().detach().numpy()[0]))\n","            loss_array_stag.append(running_loss)\n","            task1_loss_array_stag.append(task1_running_loss)\n","            task2_loss_array_stag.append(task2_running_loss)\n","            running_loss = 0\n","            task1_running_loss = 0\n","            task2_running_loss = 0\n","\n","print('Finished Training')\n","fig, ax1 = plt.subplots()\n","# Loss array stag: Running loss for every 500 iterations\n","# loss_arr.cpu().detach().numpy(): Don't need to do this if call loss.item()\n","# ax1.plot(list(range(len(loss_array_stag))), [loss_arr for loss_arr in loss_array_stag])\n","ax1.plot(list(range(len(task1_loss_array_stag))), [loss_arr for loss_arr in task1_loss_array_stag], label='Task 1 loss', c='r')\n","ax1.set_ylabel('MSE')\n","ax2 = ax1.twinx()\n","ax2.plot(list(range(len(task2_loss_array_stag))), [loss_arr for loss_arr in task2_loss_array_stag], label='Task 2 loss')\n","ax2.set_ylabel('Accuracy')\n","plt.title('GradNorm losses for both tasks')\n","plt.xlabel('Iterations')\n","ax1.legend(loc='best')\n","ax2.legend(loc='best')\n","plt.grid()\n","fig.tight_layout()  # otherwise the right y-label is slightly clipped\n","plt.show()\n","\n","evaluate_model(multi_resnet, testloader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ONqFt2vh0P56"},"source":["# PArt 4: Gradient cosine similarity\n","We'll tackle using cosine similarity to modulate auxiliary gradients next. Remember that with auxiliary tasks we are not concerned with their optimisation, we only hope that their inclusion in the network can aid in increasing the feature richness and robustness only insofar as they benefit the main task. One of the ways in which this can be accomplished is by comparing the gradients being propagated by each task. Since we want the auxiliary task to aid the main we do not want its gradient to cancel out its counterpart's. Calculating the cosine similarity tells us exactly this, a negative outcome signals pathological behaviour (The angle subtended between the gradients is greater than 90 degrees) while a positive outcome suggests cooperation (The angle subtended between the gradients is smaller than 90 degrees). \n","\n","![alt text](https://github.com/pedrob37/AML_Lecture4/blob/master/AML_images/unweighted_cosine.png?raw=true)\n","\n","<br>\n","<div>\n","<center>\n","<img src=\"https://github.com/pedrob37/AML-Lecture-Materials/blob/master/AML_images/CosSimilarity.png?raw=true\" width=\"1000\"/>\n","</center>\n","</div>\n","\n","I would like you to implement both unweighted and weighted gradient cosine similarities. As a reminder, the unweighted version simply zeros any pathological gradients while fully keeping potentially beneficial ones, while the weighted version scales the beneficial ones according to the cosine similarity. A function that computes the cosine similarity between two gradients is provided, *cos_similarity_calc()*. You should modify the training loop to include the cosine gradient similarity calculation, and to propagate the result according to its value. Remember that the gradient modification should only affect the shared layers. The standard training loop has been provided:  \n","  \n","**Hint**: Consider following these steps:  \n","1. Calculate the gradients of each task separately for only the shared layers, use the [torch.autograd.grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) method for this (These flags should be set as such: retain_graph=True, create_graph=True, allow_unused=True)\n","2. Calculate the gradient similarity between these gradients\n","3. Write an if statement conditioned on the outcome of the cosine similarity."]},{"cell_type":"code","metadata":{"id":"h4uLqhFW0P56"},"source":["cos_mode = 'unweighted'  # set to 'unweighted' or 'weighted'\n","\n","def cos_similarity_calc(grad1, grad2):\n","  # Calculates the cosine similarity between two gradients\n","    cos_operation = nn.CosineSimilarity(dim=0)\n","    flat_grad1 = torch.cat([grad.flatten() for grad in grad1], dim=-1)\n","    flat_grad2 = torch.cat([grad.flatten() for grad in grad2], dim=-1)\n","    return cos_operation(flat_grad1, flat_grad2)\n","\n","# Define a loss and optimiser\n","task1_criterion = nn.MSELoss()\n","\n","# Cross entropy is used because this is a classification task\n","aux_criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(multi_resnet.parameters(), lr=0.001)\n","\n","# Train the network\n","task1_loss_array_stag = []\n","aux_loss_array_stag = []\n","cos_similarity_array = []\n","# Given: The list of layers which are shared between tasks\n","shared_params = [p[1] for p in multi_resnet.named_parameters() if 'fc' not in p[0] and 'downsample' not in p[0]]\n","for epoch in range(number_of_epochs):\n","    task1_running_loss = 0\n","    aux_running_loss = 0\n","    for iteration, data in enumerate(trainloader):\n","      # Load in the inputs and the labels\n","        inputs, full_labels = data\n","        age_labels = full_labels[0]\n","        sex_labels = full_labels[1]\n","        # Pass labels and inputs to the CUDA device\n","        inputs = inputs.to(device, dtype=torch.float)\n","        age_labels = age_labels.to(device, dtype=torch.float)\n","        sex_labels = sex_labels.to(device, dtype=torch.long)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        age_regression, sex_classification = multi_resnet(inputs)\n","\n","        # Calculate loss\n","        task1_loss = task1_criterion(age_regression.squeeze(), age_labels) ** 0.5\n","        aux_loss = aux_criterion(sex_classification.squeeze(), sex_labels)\n","        \n","        # Calculate the gradients with respect to the SHARED layers for each task:\n","        # The torch.autograd.grad method allows you to calculate the gradients with respect to a subset of parameters\n","        G_main_shared = # YOUR CODE HERE\n","        G_aux_shared = # YOUR CODE HERE\n","\n","        # Getting rid of None, don't worry about these lines\n","        G_main_shared = [grad for grad in G_main_shared if grad is not None]\n","        G_aux_shared = [grad for grad in G_aux_shared if grad is not None]\n","        \n","        # Calculate the cosine similarity\n","        cos_similarity = cos_similarity_calc(G_main_shared, G_aux_shared)\n","\n","        # Accumulate the gradients for the auxiliary task\n","        aux_loss.backward(retain_graph=True)\n","\n","        # Modify the gradients according to the mode\n","        if cos_mode == 'weighted':\n","            # If cosine sim. is -ve don't want the auxiliary gradients to be propagated through the shared layers\n","            # Therefore zero the auxiliary gradients in the shared layers\n","            if cos_similarity < 0:\n","              for layer in shared_params:\n","                if layer.grad is None:\n","                    continue\n","                else:\n","                    # YOUR CODE HERE\n","            else:\n","                # In this instance, if the cosine similarity is positive we still want to scale it by the value\n","                for layer in shared_params:\n","                    # YOUR CODE HERE\n","        \n","        elif cos_mode == 'unweighted':\n","            if cos_similarity < 0:\n","                # This would be exactly the same as before\n","                for layer in shared_params:\n","                    if layer.grad is None:\n","                        continue\n","                    else:\n","                        # YOUR CODE HERE\n","        task1_loss.backward()\n","        optimizer.step()\n","\n","        # Print statistics\n","        task1_running_loss += task1_loss.item()\n","        # Keeping a track of this only for curiosity's sake since we aren't optimising directly for it\n","        aux_running_loss += aux_loss.item()\n","        if iteration % 50 == 49:  # Print every 50 mini-batches, weird choice of numbers to exclude zero\n","            print('Epoch: {:.0f}, Iteration: {:.0f}, '\n","                  'Task1 Loss: {:.3f}, '\n","                  'Task2 Loss: {:.3f}'.format(epoch + 1, iteration + 1,\n","                                              task1_loss.item(),\n","                                              aux_loss.item()), \n","                  'CosSim: {:.3f}'.format(cos_similarity))\n","            task1_loss_array_stag.append(task1_running_loss)\n","            aux_loss_array_stag.append(aux_running_loss)\n","            task1_running_loss = 0\n","            aux_running_loss = 0\n","\n","fig, ax1 = plt.subplots()\n","# Loss array stag: Running loss for every 500 iterations\n","# loss_arr.cpu().detach().numpy(): Don't need to do this if call loss.item()\n","# ax1.plot(list(range(len(loss_array_stag))), [loss_arr for loss_arr in loss_array_stag])\n","ax1.plot(list(range(len(task1_loss_array_stag))), [loss_arr for loss_arr in task1_loss_array_stag], label='Task 1 loss', c='r')\n","ax1.set_ylabel('MSE')\n","ax2 = ax1.twinx()\n","ax2.plot(list(range(len(aux_loss_array_stag))), [loss_arr for loss_arr in aux_loss_array_stag], label='Task 2 loss')\n","ax2.set_ylabel('Accuracy')\n","plt.title('Main and Auxiliary losses for both tasks')\n","plt.xlabel('Iterations')\n","ax1.legend(loc='best')\n","ax2.legend(loc='best')\n","plt.grid()\n","fig.tight_layout()  # otherwise the right y-label is slightly clipped\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OCHsEBQw3nXw"},"source":["# Part 5: What about Multi-input training?\n","\n","So far we've covered learning multiple tasks from a single image. But what if we have access to other information which might make the task easier? Do you think you would find it easier to predict sex if you had an image and the age of the person? Maybe, maybe not. We're going to see how we can feed these multimodal inputs to a convolutional neural network despite the big differences in their sizes.\n","\n","The images that we're using are of size 3 x 200 x 200. We've chosen a convolutional neural network to extract features from that image. Age is a single scalar value. We don't need a feature extractor applied to it as it is already a feature.\n","\n","In order to incorporate age we can simply concatenate it to the flattened fully connected layer at the end of our ResNet. \n","\n","Try concatenating the age onto that feature vector in the code below and see whether you can improve on the sex classification task.\n","\n"]},{"cell_type":"code","metadata":{"id":"kgqZAU1o3mbv"},"source":["# Network design\n","class MultiInputResNet(nn.Module):\n","    def __init__(self, block, first_output_channels=64, norm_layer=None):\n","        # Initialises the parent class: nn.module\n","        super(MultiInputResNet, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","        self.first_output_channels = first_output_channels\n","\n","        self.conv1 = nn.Conv2d(3, self.first_output_channels, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = norm_layer(self.first_output_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = block(self.first_output_channels, self.first_output_channels, stride=1)\n","        self.layer2 = block(self.first_output_channels, self.first_output_channels*2, stride=2)\n","        self.layer3 = block(self.first_output_channels*2, self.first_output_channels*4, stride=2)\n","        self.layer4 = block(self.first_output_channels*4, self.first_output_channels*8, stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc1 = nn.Linear(512, 100)\n","        # This linear layer needs to be changed to size 101\n","        self.fc2 = nn.Linear(100, 20)\n","        self.fc3 = nn.Linear(21, 2)\n","\n","    def forward(self, image, age):\n","        x = self.conv1(image)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        # ResBlocks\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        # In preparation to pass through a fully connected layer, flatten all features into 1D\n","        x = nn.Flatten()(x) # torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        # TODO: Concatenate the age here.\n","        x = self.fc3(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1xwGsnx8dZ2"},"source":["# Model training\n","# Create network instance\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = MultiInputResNet(ResBlock)\n","model.cuda(device)\n","\n","# Define a loss and optimiser\n","# Cross entropy is used because this is a classification task\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train the network\n","loss_array = []\n","for epoch in range(10):\n","    running_loss = 0\n","    for iteration, data in enumerate(trainloader):\n","        inputs, full_labels = data\n","        if inputs is None or full_labels is None:\n","          continue\n","        age_labels = full_labels[0]\n","        sex_labels = full_labels[1]\n","        inputs = inputs.to(device, dtype=torch.float)\n","        age_labels = age_labels.to(device, dtype=torch.float)\n","        sex_labels = sex_labels.to(device, dtype=torch.long)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        sex_classification = model(inputs, age_labels)\n","\n","        # Calculate loss\n","        total_loss = criterion(sex_classification.squeeze(), sex_labels)\n","\n","        # Back-propagation of gradients\n","        total_loss.backward()\n","\n","        # Optimize\n","        optimizer.step()\n","\n","        # Print statistics\n","        loss_array.append(total_loss)\n","        running_loss += total_loss.item()\n","        if iteration % 50 == 49:  # Print every 50 mini-batches, weird choice of numbers to exclude zero\n","            print('{:.0f} {:.0f}, Total Loss: {:.3f}'.format(\n","                epoch + 1, iteration + 1, running_loss / 50))\n","            running_loss = 0\n","\n","print('Finished Training')\n","plt.figure()\n","# Loss array stag: Running loss for every 50 iterations\n","plt.plot(list(range(len(loss_array))), [loss_arr for loss_arr in loss_array])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdZkmCfJA231"},"source":["def evaluate_model(model, testloader):\n","  print('EVALUATION')\n","  total_correct = 0\n","  total = 0\n","  with torch.no_grad():\n","      for data in testloader:\n","          images, labels = data\n","          label_set = set(labels[0].cpu().detach().numpy())\n","          labels = torch.stack(labels)\n","          images = images.to(device)\n","          labels = labels.to(device)\n","          sex_classification = model(images, labels[0].type(torch.FloatTensor).to(device))\n","          total += images.size(0)\n","          total_correct += (torch.max(sex_classification, 1)[1] == labels[1].long()).sum().item()\n","  print('Sex accuracy of the network on the test images: {}'.format(total_correct / total))\n","\n","evaluate_model(model, testloader)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STCJmtpNGlHu"},"source":["# Does our network even use Age as a feature?\n","\n","Our CNN can be thought of as a feature extractor. If we look at the penultimate layer, `fc2`, we get a 20 dimensional vector which describes the image. In the section below we'll use our trained model as a feature extractor and try and understand if age is a useful feature for sex classification.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"srkkwJRKKLzv"},"source":["# Get the average activation vectors across the test set for each of the 20 image features in the penultimate layer + the age feature to normalise the weights and biases\n","batch_iter = 0\n","sexes = []\n","features = []\n","with torch.no_grad():\n","      for data in testloader:\n","          images, labels = data\n","          labels = torch.stack(labels)\n","          images = images.to(device)\n","          labels = labels.to(device)\n","          # Loop through each image batch in the test set and extract the activation\n","          feature_extractor = nn.Sequential(*[\n","                                             model.conv1,\n","                                             model.bn1,\n","                                             model.relu,\n","                                             model.maxpool,\n","                                             model.layer1,\n","                                             model.layer2,\n","                                             model.layer3,\n","                                             model.layer4,\n","                                             model.avgpool,\n","                                             nn.Flatten(),\n","                                             model.fc1,\n","                                             model.fc2])\n","          # Extract age and sex labels\n","          age = labels[0].cpu().detach().numpy()\n","          sexes.append(labels[1].cpu().detach().numpy())\n","          # Calculate the activations for the current image batch by calling the feature extractor\n","          image_features = # YOUR CODE HERE\n","          # Loop through each image in the current batch\n","          for i in range(image_features.shape[0]):\n","              # Create array of features == 20 original features + age\n","              f = np.array(list(image_features[i]) + [age[i]])\n","              # Append to overall feature list\n","              features.append(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZcaLvW-elRf"},"source":["# Convert our input features and output ground truths into numpy arrays\r\n","X = np.array(features)\r\n","y = np.concatenate(sexes)\r\n","# Let's check the shape of our input and output\r\n","print('X shape is: {}, Y shape is: {}'.format(X.shape, y.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9st7Lkc6Kkbx"},"source":["# We will calculate relative feature importance using a random forest classifier\n","# Reference: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html (Read the relevant documentation!)\n","from sklearn.ensemble import ExtraTreesClassifier\n","\n","# Build a forest and compute the feature importances\n","forest = ExtraTreesClassifier(n_estimators=100)\n","\n","# Fit the trees to X and y\n","# YOUR CODE HERE\n","\n","# Calculate the feature importances\n","importances = # YOUR CODE HERE\n","\n","# Calculate the standard deviation across the estimators, for each feature\n","std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n","             axis=0)\n","\n","# Sort according to importance (descending)\n","indices = np.argsort(importances)[::-1]\n","\n","# Plot the feature importances of the forest\n","plt.figure()\n","plt.title(\"Feature importances\")\n","plt.bar(range(X.shape[1]), importances[indices],\n","       color=\"r\", yerr=std[indices], align=\"center\")\n","xticks = list(indices)\n","xticks[xticks.index(20)] = 'age'\n","plt.xticks(range(X.shape[1]), xticks)\n","plt.xlim([-1, X.shape[1]])\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZBRfv3jzmPX4"},"source":["What did you find? Is age a particularly good feature? If the tasks are swapped do you arrive at the same conclusion?"]}]}