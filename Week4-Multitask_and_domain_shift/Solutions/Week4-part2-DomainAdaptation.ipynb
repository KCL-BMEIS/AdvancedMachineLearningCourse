{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "with_solution_DomainAdaptationAndCurriculumLearning_tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dWljmhDjKiM"
      },
      "source": [
        "**NOTE:** before to start this notebook go to edit-> Notebook settings and select GPU as a hardware accelerator. Now your ready to go!\n",
        "\n",
        "#  Domain adaptation and curriculum learning\n",
        "\n",
        "In this notebook, we will see in practice, how the data-shift distribution affects the performance of deep learning models and some state-of-the-art strategies to solve this problem. Specifically, we will explore:\n",
        "\n",
        "\n",
        "* Supervised Domain Adaptation\n",
        "* Augmentation-based Domain adaptation (ADA)\n",
        "* Adversarial Domain Adaptation \n",
        "* Scheduling losses and curriculum learning\n",
        "\n",
        "We particularly focus on medical imaging data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKONk82a0P5Z"
      },
      "source": [
        "%matplotlib inline\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSbCoqurN7aS"
      },
      "source": [
        "Let's Download the data locally, import the libraries we need and define some plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWdGNV7I_8Ym"
      },
      "source": [
        "file_download_link = \"https://github.com/KCL-BMEIS/AdvancedMachineLearningCourse/blob/main/Week4-Multitask_and_domain_shift/Data/WMLesions.zip?raw=true\"\n",
        "!wget -nc -O WMLesions.zip --no-check-certificate \"$file_download_link\"\n",
        "!unzip -n WMLesions.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWsiTTa9jZvd"
      },
      "source": [
        "# Setting up various plot functions to be used throughout the notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "def plot_images(images, title=None, figsize=(15,15)):\n",
        "    f, axes = plt.subplots(1, len(images), figsize=figsize)\n",
        "    for image_id, image in enumerate(images):\n",
        "        axes[image_id].imshow(np.rot90(np.rot90(np.rot90(image))), cmap='gray')\n",
        "        axes[image_id].axis('off')\n",
        "        if not title:\n",
        "            axes[image_id].set_title('Clinic {}'.format(image_id+1),\n",
        "                                     fontsize=20)\n",
        "        elif len(title) == 1:\n",
        "            axes[image_id].set_title('Image {}: {}'.format(\n",
        "                image_id, title[0]), fontsize=20)\n",
        "        else:\n",
        "            axes[image_id].set_title('Image {}: {}'.format(\n",
        "                image_id, title[image_id]), fontsize=20)\n",
        "    f.tight_layout()\n",
        "    \n",
        "\n",
        "def plot_grids(grids, figsize=(15,15)):\n",
        "    f, axes = plt.subplots(1, len(grids), figsize=figsize)\n",
        "    for grid_id, grid in enumerate(grids):\n",
        "        grid_array = sitk.GetArrayViewFromImage(grid)\n",
        "        axes[grid_id].imshow(np.flip(grid_array, axis=0),\n",
        "               interpolation='hamming',\n",
        "               cmap='gray',\n",
        "               origin='lower')\n",
        "        axes[grid_id].set_title('Grid {}'.format(grid_id), fontsize=20)\n",
        "    f.tight_layout()\n",
        "\n",
        "    \n",
        "def plot_histograms(images, figsize=(15,7.5)):\n",
        "    f, axes = plt.subplots(1, len(images), figsize=figsize)\n",
        "    for image_id, image in enumerate(images):\n",
        "        histogram, bins = np.histogram(image, bins=40)\n",
        "        axes[image_id].set_xlim([0, 140])\n",
        "        axes[image_id].hist(histogram, bins)\n",
        "        axes[image_id].set_title('Clinic {} Histogram'.format(image_id),\n",
        "                                 fontsize=20)\n",
        "    f.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyr5tGbZPG3D"
      },
      "source": [
        "# PART 1: Differences in Data distributions\n",
        "\n",
        "\n",
        "Supervised learning models assume that training and testing data come from the\n",
        "same distribution to achieve good performance. However, when this assumption is not fulfilled, the performance of these methods experienced a drop in their performance.\n",
        "\n",
        "\n",
        "Specifically in medical imaging, differences in distribution can come from different protocols modalities or settings when acquired. Let' load 3 datasets acquired  whit  different protocols and plot samples from each one. (\n",
        "This is a common scenario in clinical practice as usually different clinics use different settings for image acquisition.)\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/MauricioOrbes/AML_lecture_5/master/images/datashiftComparison.png\" width=\"1000\"/>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "> __Figure__: Data-shift problem example in a segemntation of medical images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyX3nyjhTMH7"
      },
      "source": [
        "Let's plot samples from 3 different clinics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB81zEeXj9iP"
      },
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "\n",
        "def read_file(filename):\n",
        "    img = nib.load(filename)\n",
        "    data = img.get_fdata()\n",
        "    aff = img.affine\n",
        "    return data, aff\n",
        "\n",
        "# Build the 2D dataset: Let's take 20 images\n",
        "dataset = []\n",
        "clinic1_dir = 'WMH_3D/Clinic1'\n",
        "clinic2_dir = 'WMH_3D/Clinic2'\n",
        "clinic3_dir = 'WMH_3D/Clinic3'\n",
        "for clinic_dir in [clinic1_dir, clinic2_dir,clinic3_dir]:\n",
        "    image, _ = read_file(clinic_dir + '/' + os.listdir(clinic_dir)[0])\n",
        "    # Let's take the middle slice\n",
        "    dataset.append(image[:, :, 25])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNAvUlV8lIm4"
      },
      "source": [
        "plot_images(dataset[:3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoLmufiSmXEF"
      },
      "source": [
        "As you can see the images acquired at the three clinics are qualitatively different. Let's look at the histograms!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt3pbpzXlJQC"
      },
      "source": [
        "plot_histograms(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPpKL4uJ532E"
      },
      "source": [
        "\n",
        "<!-- As you can see not only are the images qualitatively different we can also see a difference when looking at the image histograms.   -->\n",
        "\n",
        "# Let's see how this differences affects  the performance of a supervised learning model. \n",
        "\n",
        "We are going to train a Neural Network to perform WMH hyperintensity segmentation. We use labeled samples from the clinic 1 to carry out the training. Then, inference will be carried out in validation set from both **clinic 1** (Same domain) and **clinic 2** (different domain)\n",
        "\n",
        "To perform this task we chose the well known [U-Net](https://arxiv.org/pdf/1505.04597.pdf) architecture which has achieved outstanding results for semantic segmentation.\n",
        "\n",
        "Let's define the model in PyTorch. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BD9kWxS0NU5"
      },
      "source": [
        "# import required libraries\n",
        "import torch \n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def initialize_weights(*models):\n",
        "    for model in models:\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                module.weight.data.fill_(1)\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "class _EncoderBN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels,k ,padd , dropout=False):\n",
        "        super(_EncoderBN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=padd)\n",
        "        self.BN1a = nn.BatchNorm2d(out_channels)\n",
        "        self.BN1b = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=k, padding=padd)\n",
        "        self.BN2a = nn.BatchNorm2d(out_channels)\n",
        "        self.BN2b = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self,x,d):\n",
        "        if d == 'source':\n",
        "            x = F.leaky_relu(self.BN1a(self.conv1(x)),inplace=True)\n",
        "            x = F.leaky_relu(self.BN2a(self.conv2(x)),inplace=True)\n",
        "        elif d == 'target':\n",
        "            x = F.leaky_relu(self.BN1b(self.conv1(x)),inplace=True)\n",
        "            x = F.leaky_relu(self.BN2b(self.conv2(x)),inplace=True)\n",
        "        return x\n",
        "\n",
        "class _DecoderBN(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "        super(_DecoderBN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=3,padding=1)\n",
        "        self.BN1a = nn.BatchNorm2d(middle_channels)\n",
        "        self.BN1b = nn.BatchNorm2d(middle_channels)\n",
        "        self.conv2 = nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1)\n",
        "        self.BN2a = nn.BatchNorm2d(middle_channels)\n",
        "        self.BN2b = nn.BatchNorm2d(middle_channels)\n",
        "        self.convT = nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x, d):\n",
        "        if d == 'source':\n",
        "            x = F.leaky_relu(self.BN1a(self.conv1(x)),inplace=True)\n",
        "            x = F.leaky_relu(self.BN2a(self.conv2(x)),inplace=True)\n",
        "        elif d == 'target':\n",
        "            x = F.leaky_relu(self.BN1b(self.conv1(x)),inplace=True)\n",
        "            x = F.leaky_relu(self.BN2b(self.conv2(x)),inplace=True)\n",
        "\n",
        "        return self.convT(x)\n",
        "\n",
        "class prefinalBN(nn.Module):\n",
        "      def __init__(self, in_channels,out_channels):\n",
        "          super(prefinalBN,self).__init__()\n",
        "\n",
        "          self.conv1 = nn.Conv2d(in_channels , out_channels, kernel_size=3, padding=1)\n",
        "          self.BN1a = nn.BatchNorm2d(out_channels)\n",
        "          self.BN1b = nn.BatchNorm2d(out_channels)\n",
        "          self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "          self.BN2a = nn.BatchNorm2d(out_channels)\n",
        "          self.BN2b = nn.BatchNorm2d(out_channels)\n",
        "          nn.InstanceNorm1d\n",
        "\n",
        "      def forward(self, x ,d):\n",
        "          if d == 'source':\n",
        "\n",
        "              x = F.leaky_relu(self.BN1a(self.conv1(x)),inplace=True)\n",
        "              x = F.leaky_relu(self.BN2a(self.conv2(x)),inplace=True)\n",
        "          elif d == 'target':\n",
        "              x = F.leaky_relu(self.BN1b(self.conv1(x)),inplace=True)\n",
        "              x = F.leaky_relu(self.BN2b(self.conv2(x)),inplace=True)\n",
        "          return x\n",
        "\n",
        "class ADABN(nn.Module):\n",
        "    def __init__(self, num_classes, num_channels):\n",
        "        super(ADABN, self).__init__()\n",
        "\n",
        "        self.enc1 = _EncoderBN(num_channels, 64, 5, 2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.enc2 = _EncoderBN(64, 96, 3, 1)\n",
        "        self.enc3 = _EncoderBN(96, 128, 3, 1)\n",
        "        self.enc4 = _EncoderBN(128, 256, 3, 1)\n",
        "        self.center = _DecoderBN(256, 512, 256)\n",
        "        self.dec4 = _DecoderBN(512, 256, 128)\n",
        "        self.dec3 = _DecoderBN(256, 128, 96)\n",
        "        self.dec2 = _DecoderBN(96 * 2, 96, 64)\n",
        "        self.dec1 = prefinalBN(128,64)\n",
        "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        initialize_weights(self)\n",
        "\n",
        "    def forward(self, x,d='source'):\n",
        "        enc1 = self.enc1(x,d)\n",
        "        enc2 = self.enc2(self.pool(enc1),d)\n",
        "        enc3 = self.enc3(self.pool(enc2),d)\n",
        "        enc4 = F.dropout(self.enc4(self.pool(enc3),d))\n",
        "\n",
        "        center = self.center(self.pool(enc4),d)\n",
        "\n",
        "        dec4 = self.dec4(torch.cat([center, enc4], 1),d)\n",
        "        dec3 = self.dec3(torch.cat([dec4, enc3], 1),d)\n",
        "        dec2 = self.dec2(torch.cat([dec3, enc2], 1),d)\n",
        "        dec1 = self.dec1(torch.cat([dec2, enc1], 1),d)\n",
        "\n",
        "        final = self.final(dec1)\n",
        "        return (final, enc1, enc2, enc3, enc4, center, dec4, dec3, dec2, dec1)\n",
        "\n",
        "model = ADABN(1,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqdLGaq8n0Mn"
      },
      "source": [
        "<!-- Lets load some weights from a model trained only on clinic 1 -->\n",
        "Training a neural network to perform segmentation would require many iterations. To save time we are going to load some already pre-trained weights. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmzJpPQs2M9F"
      },
      "source": [
        "model.load_state_dict(torch.load('clinic1B2assource2load.pt'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6GHIT8xn9G5"
      },
      "source": [
        "Now our model is ready to perform segmentation on validations sets from clinic 1 and clinic2. But before to do that we need to split our data (training and validation)  We're going to use a great package called `torchio` developed at KCL to save time\n",
        "\n",
        "\n",
        "<!-- # Lets run that model on clinic 1 and clinic 2 and analyse the results\n",
        "\n",
        "We're going to use a great package called `torchio` developed at KCL to save time. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdpvuMoqqHIN"
      },
      "source": [
        "!pip install torchio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X0TavsBGb5y"
      },
      "source": [
        "import torchio\n",
        "from torchio import Subject\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import RandomCrop\n",
        "import re\n",
        "def sorted_aphanumeric(data):\n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
        "    return sorted(data, key=alphanum_key)\n",
        "\n",
        "\n",
        "# define the Validation and Training sets for source and target Domains\n",
        "\n",
        "# CLINIC1 will be the source domain whereas CLINIC2 will be the target domain \n",
        "# We create a a list of subjects for each clinic\n",
        "\n",
        "def getiosubjects(path_dir):\n",
        "  subjects_list = sorted_aphanumeric(os.listdir( path_dir + '/flair/'))\n",
        "  \n",
        "  iosubjects = []\n",
        "  for slices in subjects_list:\n",
        "   \n",
        "      subject = torchio.Subject(\n",
        "          flair=torchio.ScalarImage(path_dir + '/flair/' + slices),\n",
        "          label=torchio.LabelMap(path_dir + '/labels/wmh' +  slices.split('FLAIR')[1])\n",
        "      )\n",
        "      iosubjects.append(subject)\n",
        "  \n",
        "  return iosubjects\n",
        "                       \n",
        "source_training_dir = 'WMH_DATABASE/Clinic1/Training'\n",
        "source_validation_dir = 'WMH_DATABASE/Clinic1/Validation'\n",
        "\n",
        "target_training_dir = 'WMH_DATABASE/Clinic2/Training'\n",
        "target_validation_dir= 'WMH_DATABASE/Clinic2/Validation'\n",
        "\n",
        "source_training_subjects = getiosubjects(source_training_dir)\n",
        "source_validation_subjects = getiosubjects(source_validation_dir)\n",
        "\n",
        "target_training_subjects = getiosubjects(target_training_dir) \n",
        "target_validation_subjects = getiosubjects(target_validation_dir) \n",
        "\n",
        "\n",
        "# Training Sets\n",
        "source_dataset_training = torchio.SubjectsDataset(source_training_subjects)\n",
        "source_training_loader = DataLoader(source_dataset_training, shuffle=True, batch_size=6)\n",
        "\n",
        "target_dataset_training = torchio.SubjectsDataset(target_training_subjects)\n",
        "target_training_loader = DataLoader(target_dataset_training, shuffle=True, batch_size=6)\n",
        "\n",
        "# Validation Sets \n",
        "source_dataset_validation = torchio.SubjectsDataset(source_validation_subjects)\n",
        "source_validation_loader = DataLoader(source_dataset_validation, shuffle=False, batch_size=6)\n",
        "\n",
        "target_dataset_validation = torchio.SubjectsDataset(target_validation_subjects)\n",
        "target_validation_loader = DataLoader(target_dataset_validation, shuffle=False, batch_size=6)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exc7tj5Hw_HF"
      },
      "source": [
        "### Let's Define a similarity measure \n",
        "To evaluate and compare performances we need a similarity measure between ground truth and predicted segmentation. To this end we are going to use the  [dice similarity index](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient), Let's define the dice in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-IX6rJKxUiX"
      },
      "source": [
        "\n",
        "def pairwisedice(output,target):\n",
        "    s = (10e-20)\n",
        "    output = output > 0.8\n",
        "    output = output.type(torch.FloatTensor)\n",
        "    \n",
        "    target = target == 1\n",
        "    target = target.type(torch.FloatTensor)\n",
        "\n",
        "    intersect = torch.sum(output * target)\n",
        "\n",
        "    dice = (2 * intersect) / (torch.sum(output) + torch.sum(target) + s)\n",
        "    \n",
        "    return dice\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C3cYROYWbSO"
      },
      "source": [
        "import random\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "\n",
        "# We define the inference function which get the prediction for all subjects.\n",
        "# Note, the predictions are obtained on batch base\n",
        "def inference(data_loader,batch_size=6,slices_per_image=24):\n",
        "  slice_counter = 0   \n",
        "  dice = []\n",
        "  with tqdm(total=len(data_loader), file=sys.stdout) as pbar:\n",
        "    for slices in data_loader: \n",
        "        if (slice_counter % slices_per_image) == 0:\n",
        "          pred = torch.Tensor([])\n",
        "          ref = torch.Tensor([])        \n",
        "        batch_images = slices['flair']['data'][...,0].float()\n",
        "        batch_labels = slices['label']['data'][...,0]\n",
        "        model.eval()\n",
        "        outputs, _, _, _, _, _, _, _, _, _ = model(Variable(batch_images))\n",
        "        \n",
        "        pred = torch.cat((pred,torch.sigmoid(outputs)),0)\n",
        "        ref = torch.cat((ref,batch_labels),0)\n",
        "       \n",
        "        if pred.size(0)==slices_per_image:\n",
        "          dice.append(pairwisedice(torch.squeeze(pred,1),torch.squeeze(ref,1)).item())\n",
        "        slice_counter += batch_size\n",
        "        pbar.update(1)\n",
        "  return dice\n",
        "\n",
        "# First we get the inference for  Clinic1 validation set\n",
        "dice_source_noadaptation=inference(source_validation_loader)\n",
        "# We do the same for Clinic 2 validation set\n",
        "dice_target_noadaptation=inference(target_validation_loader)\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.set_title('Source vs Target Performance')\n",
        "ax1.boxplot([dice_source_noadaptation,dice_target_noadaptation]);\n",
        "\n",
        "ax1.set_xticklabels(['Source', 'Target'])\n",
        "ax1.get_xaxis().tick_bottom()\n",
        "ax1.set_ylabel('Dice')\n",
        "\n",
        "print('Source dice performance {}' .format(np.mean(dice_source_noadaptation)))\n",
        "print('Target dice performance {}' .format(np.mean(dice_target_noadaptation)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-emzVX2MAgD"
      },
      "source": [
        "As you can see the performance of our model on data from a different domain (Target) is considerably lower when compared to the performance on data belonging to the same domain (Source) used during training. This fact motivates the development of domain adaptation techniques to get our model to perform well across different domains.\n",
        "\n",
        "**What do we mean by domain adaptation?**\n",
        "\n",
        "Domain adaptation is the process of adapting a model from a source domain to a target domain in order to ovoid this dropp in performance.\n",
        "\n",
        "Domain adaptation usually uses some information from the target domain to perform adaptation. Depending on the kind of information the adaptation could be:\n",
        "\n",
        "**Supervised Adaptation:** When there is access to some labeled data from the target domain.\n",
        "\n",
        "**Unsupervised Domain adaptation:** When we only have access to unlabeled data from the target domain\n",
        "\n",
        "\n",
        "<br>\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/MauricioOrbes/AML_lecture_5/master/images/SupVsUnsupervised.png\" width=\"1200\"/>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "> __Figure__: Supervised Vs Unsupervised Domain Adaptation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH5lLIFkoe5p"
      },
      "source": [
        "# PART 2: Lets try supervised domain adaptation between clinic 1 and clinic 2 and analyse the results\n",
        "\n",
        "A straightforward way to carry out this adaptation, is by using a smaller amount\n",
        "of annotated data from the target domain to refine a pre-trained classifier on a source domain. This process is also known as [fine-tuning](https://). \n",
        "\n",
        "Assuming a model $f(\\cdot)$ has been training on a source domain using the following training loss.\n",
        "\n",
        "* $Source\\_Loss =  soft\\_dice(f(source\\_images),source\\_labels) \\ \\ \\ \\ \\ \\ \\ (1)$\n",
        "\n",
        "The adaptation is done by computing the same loss on target images and add it to the previous one.\n",
        "\n",
        "* $Target\\_Loss = soft\\_dice(f(target\\_images),target\\_labels) \\ \\ \\ \\ \\ \\  \\ \\ \\ (2) $\n",
        "\n",
        "* $Total\\_loss =  \\alpha* Source\\_loss + \\beta * Target\\_loss \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ (3)$\n",
        "\n",
        "**Exercise:** Write the loss function to carry out domain adaptation. \n",
        "\n",
        "The code below is already implemented to continue the optimization based on the source loss. As the training goes the model will overfit the source domain, which will increase the gap between source and target performance.\n",
        "\n",
        "I would like you to implement the **$Target\\_loss$** which will be added to the **$Source\\_loss$**. Also, try modifying the parameters  $\\alpha$ and $\\beta$ and see the impact it has on the performance.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPtUe7WiZHD_"
      },
      "source": [
        "# Supervised domain adaptation\n",
        "\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Define the optimizer (we choose adam with learning rate (1e-4))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4 )\n",
        "\n",
        "# Soft dice is used as cost function\n",
        "def dice_soft_loss(output, target):\n",
        "    s = (10e-20)\n",
        "\n",
        "    intersect = torch.sum(output * target)\n",
        "    dice = (2 * intersect) / (torch.sum(output) + torch.sum(target) + s)\n",
        "\n",
        "    return 1 - dice\n",
        "\n",
        "\n",
        "# Set the number of epochs here:\n",
        "number_of_epochs = 2\n",
        "\n",
        "# EXERCISE; Try different values for this parameters\n",
        "\n",
        "alpha = 0.5 # YOUR CODE HERE\n",
        "beta  = 0.5 # YOUR CODE HERE\n",
        "\n",
        "model.train()\n",
        "for epoch in range(number_of_epochs):\n",
        "  \n",
        "  with tqdm(total=len(source_training_loader), file=sys.stdout) as pbar:\n",
        "\n",
        "    start_time = time.time()\n",
        "    running_loss = 0\n",
        "    indb =0\n",
        "    for patch_s,patch_t in zip(source_training_loader,target_training_loader):\n",
        "         \n",
        "        #Get a batch of source slices \n",
        "        source_batch_images = patch_t['flair']['data'][...,0].float()\n",
        "        source_batch_labels = patch_t['label']['data'][...,0]\n",
        "        #Get a batch of target slices\n",
        "        target_batch_images = patch_s['flair']['data'][...,0].float()\n",
        "        target_batch_labels = patch_s['label']['data'][...,0]\n",
        "        \n",
        "\n",
        "        outputs_source, _, _, _, _, _, _, _, _, _ = model(Variable(source_batch_images,requires_grad=True))\n",
        "        #Supervised loss eq (1)\n",
        "        supervised_source_loss = dice_soft_loss(torch.sigmoid(outputs_source), Variable(source_batch_labels,requires_grad=True))\n",
        "        \n",
        "\n",
        "        #Supervised Adaptation loss goes here\n",
        "        outputs_target, _, _, _, _, _, _, _, _, _ = model(Variable(target_batch_images,requires_grad=True))\n",
        "        \n",
        "        \n",
        "        #EXERCISE: Implement the supervised target loss according to eq(2) \n",
        "        supervised_target_loss = dice_soft_loss(\n",
        "            torch.sigmoid(outputs_target), Variable(target_batch_labels,requires_grad=True)) # YOUR CODE HERE \n",
        "        \n",
        "        total_loss = alpha * supervised_source_loss + beta * supervised_target_loss\n",
        "        \n",
        "        model.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "        indb += 1\n",
        "        pbar.update(1)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    print('Training: [epoch %d, loss %.3f] time:%.3f ' % (epoch + 1, running_loss / indb, (end_time-start_time) / 60 ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhCrAG5yi6Tv"
      },
      "source": [
        "# Now lets do inference again and compare agains no adaptation. \n",
        "\n",
        "dice_source_supervised_adaptation=inference(source_validation_loader)\n",
        "dice_target_supervised_adaptation=inference(target_validation_loader)\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.set_title('No adaptation vs Supervised Adaptation')\n",
        "ax1.boxplot([dice_target_noadaptation, dice_target_supervised_adaptation]);\n",
        "\n",
        "ax1.set_xticklabels(['No adaptation', 'Supervised Adaptation'])\n",
        "ax1.get_xaxis().tick_bottom()\n",
        "ax1.set_ylabel('Dice')\n",
        "\n",
        "\n",
        "print('No adaptatiion  dice performance {}' .format(np.mean(dice_target_noadaptation)))\n",
        "print('Supervised adaptatiion dice performance {}' .format(np.mean(dice_target_supervised_adaptation)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iwTn7Jp19bi"
      },
      "source": [
        "As you can see the model performs much better now in the target domain data. However, labeled data from the source domain would not be always available, this motivates the development of unsupervised domain adaptation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S8g-mHloh0c"
      },
      "source": [
        "<!-- # Lets run try unsupervised domain adaptation between clinic 1 and clinic 2 and analyse the results -->\n",
        "\n",
        "# PART 3: Augmentation-based unsupervised Domain Adaptation\n",
        "\n",
        "\n",
        "One smart way to perform domain adaptation when not labels are available for the target domain is through a consistency loss.  This idea was firstly used in the classification task [here](https://arxiv.org/pdf/1904.12848.pdf) and adapted for segmentation task  [here](https://arxiv.org/pdf/1904.12848.pdf)\n",
        "\n",
        "This loss enforces the output consistency between the model predictions of one image and a perturbed or augmented version of it. As consistency is measured between predictions, no labels are needed, therefore, it can be computed on unlabeled data. Similarly to supervised domain adaptation, the optimization is carried out by the addition of supervised loss and a consistency loss as:  \n",
        "* $Total\\_loss =  \\alpha* Source\\_loss + \\beta * Target\\_loss \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (4)$\n",
        "\n",
        "Where \n",
        "* $Source\\_Loss =  soft\\_dice(f(source\\_images),source\\_labels) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  (5)$\n",
        "\n",
        "* $Consistency\\_Loss =  soft\\_dice(f(target\\_images),f(\\phi(target\\_labels))) \\ \\ \\ \\ \\ \\ \\ (6)$\n",
        "\n",
        "Where $\\phi(\\cdot)$ performs a transformation to input image.\n",
        "\n",
        "Although, the consistency loss as in $(5)$ would work for classification tasks (e.g a car rotated or in a different color is still a car).  A problem arises in a segmentation task, as the inconsistency introduced between original and augmented predictions when spatial transformations (e.g., translation, rotation or any similar spatial transformation for data augmentation) is applied with the input images.      \n",
        "\n",
        "This problem can be solve by a simple trick, the same transformation $\\phi(\\cdot)$ applied to the image before to feed the model, will be applied to the prediction of the model for the original image. The consistency loss can be then rewritten as:\n",
        "\n",
        " $Consistency\\_Loss =  soft\\_dice(\\phi(f(target\\_images)),f(\\phi(target\\_labels))) \\ \\ \\ \\ \\ \\ \\ (7)$\n",
        "\n",
        "## Exercise: Let's perform augmentation-based unsueprvised adaptation between clinic 1 and clinic2 \n",
        "\n",
        "\n",
        "First of all, we need to define a transformation to perturb the image, for simplicity we are going to use [affine](https://en.wikipedia.org/wiki/Affine_transformation) transformations, which perform rotations, scaling and shearing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYRbvZgPAjJz"
      },
      "source": [
        "# Generate affine tranformations\n",
        "# This function generate an affine transformation matrix for your bach\n",
        "\n",
        "def GenerateAffine(inputs, degreeFreedom=5, scale=[0.9, 1.2], shearingScale=[0.01, 0.01], Ngpu=0):\n",
        "    degree = torch.FloatTensor(inputs.size(0)).uniform_(-degreeFreedom, degreeFreedom) * 3.1416 / 180;\n",
        "    Theta_rotations = torch.zeros(inputs.size(0), 3, 3)\n",
        "\n",
        "    Theta_rotations[:, 0, 0] = torch.cos(degree);\n",
        "    Theta_rotations[:, 0, 1] = torch.sin(degree);\n",
        "    Theta_rotations[:, 1, 0] = -torch.sin(degree);\n",
        "    Theta_rotations[:, 1, 1] = torch.cos(degree);\n",
        "    Theta_rotations[:, 2, 2] = 1\n",
        "\n",
        "    degree = torch.FloatTensor(inputs.size(0), 2).uniform_(scale[0], scale[1])\n",
        "\n",
        "    Theta_scale = torch.zeros(inputs.size(0), 3, 3)\n",
        "\n",
        "    Theta_scale[:, 0, 0] = degree[:, 0]\n",
        "    Theta_scale[:, 0, 1] = 0\n",
        "    Theta_scale[:, 1, 0] = 0\n",
        "    Theta_scale[:, 1, 1] = degree[:, 1]\n",
        "    Theta_scale[:, 2, 2] = 1\n",
        "\n",
        "    degree = torch.cat((torch.FloatTensor(inputs.size(0), 1).uniform_(-shearingScale[0], shearingScale[0]),\n",
        "                        torch.FloatTensor(inputs.size(0), 1).uniform_(-shearingScale[1], shearingScale[1])), 1)\n",
        "\n",
        "    Theta_shearing = torch.zeros(inputs.size(0), 3, 3)\n",
        "\n",
        "    Theta_shearing[:, 0, 0] = 1\n",
        "    Theta_shearing[:, 0, 1] = degree[:, 0]\n",
        "    Theta_shearing[:, 1, 0] = degree[:, 1]\n",
        "    Theta_shearing[:, 1, 1] = 1\n",
        "    Theta_shearing[:, 2, 2] = 1\n",
        "\n",
        "    Theta = torch.matmul(Theta_rotations, Theta_scale)\n",
        "    Theta = torch.matmul(Theta_shearing, Theta)\n",
        "\n",
        "    Theta_inv = torch.inverse(Theta)\n",
        "\n",
        "    Theta = Theta[:, 0:2, :]\n",
        "    Theta_inv = Theta_inv[:, 0:2, :]\n",
        "\n",
        "    return Theta, Theta_inv\n",
        "\n",
        "\n",
        "# We need a function that transform the batch using the the affine tranformation matrix\n",
        "\n",
        "def apply_trasform(inputs, theta):\n",
        "    grid = F.affine_grid(theta, inputs.size())\n",
        "\n",
        "    if len(inputs.size()) < 4:\n",
        "        outputs = F.grid_sample(inputs, grid, mode='nearest', padding_mode=\"border\")\n",
        "    else:\n",
        "        outputs = F.grid_sample(inputs, grid, padding_mode=\"border\")\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jWCyCNoBw9u"
      },
      "source": [
        "<!-- ## Let's perform Aumentation-based Domain adaptation between clinic1 and clinic2 \n",
        "\n",
        "We are going to use affine transformations to perturb the images -->\n",
        "\n",
        "I would like you to implement the consistency loss which will be added to the source supervised loss. Note you will have to apply transformations as in eq $(7)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb5xL4dxaVyy"
      },
      "source": [
        "# As the network has been trained supervised, we load the weights again for fair comparison.\n",
        "model.load_state_dict(torch.load('clinic1B2assource2load.pt'))\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4 )\n",
        "\n",
        "sj =0\n",
        "\n",
        "# Set parameteres here:\n",
        "number_of_epochs = 2\n",
        "alpha =0.5\n",
        "beta  =0.5\n",
        "model.train()\n",
        "for epoch in range(number_of_epochs):\n",
        "  \n",
        "  with tqdm(total=len(source_training_loader), file=sys.stdout) as pbar:\n",
        "\n",
        "    start_time = time.time()\n",
        "    running_loss = 0\n",
        "    indb =0\n",
        "    for patch_s,patch_t in zip(source_training_loader,target_training_loader):\n",
        "\n",
        "        source_batch_images = patch_t['flair']['data'][...,0].float()\n",
        "        source_batch_labels = patch_t['label']['data'][...,0]\n",
        "        \n",
        "        target_batch_images = patch_s['flair']['data'][...,0].float()\n",
        "        \n",
        "        # Get the predictions of source batch and computed supervised loss\n",
        "        outputs_source, _, _, _, _, _, _, _, _, _ = model(Variable(source_batch_images,requires_grad=True))\n",
        "        supervised_source_loss = dice_soft_loss(torch.sigmoid(outputs_source), Variable(source_batch_labels,requires_grad=True))\n",
        "        \n",
        "\n",
        "        #EXERCISE: adaptation start here\n",
        "\n",
        "        # Predictions for the original image are computed here \n",
        "        outputs_target, _, _, _, _, _, _, _, _, _ = model(Variable(target_batch_images,requires_grad=True))\n",
        "        # Also the matrix transormation has been computed\n",
        "        Theta, Theta_inv = GenerateAffine(Variable(target_batch_images,requires_grad=True))\n",
        "    \n",
        "        #Now you need to  get a perturbed version of the target_batch_images f(phi(target images)) .\n",
        "        target_batch_images_aug = apply_trasform(\n",
        "            Variable(target_batch_images,requires_grad=True), Theta) # YOUR CODE  HERE \n",
        "        \n",
        "\n",
        "        #Once we have a perturbed batch we can get its prediction (f(phi(target images)))\n",
        "        outputs_target_aug,_, _, _, _, _, _, _, _, _ = model(target_batch_images_aug)\n",
        "        \n",
        "        #Now you need to tranform the predictions of the original image phi(f(target_images))\n",
        "        outputs_target_transformed = apply_trasform(\n",
        "            outputs_target,Theta) # YOUR CODE HERE\n",
        "\n",
        "        \n",
        "        #Now you need to compute the the consistency loss as in eq (7)\n",
        "        consitency_loss = dice_soft_loss(\n",
        "            torch.sigmoid(outputs_target_aug),torch.sigmoid(outputs_target_transformed)) # YOUR CODE HERE \n",
        "        \n",
        "        total_loss = alpha * supervised_source_loss + beta * consitency_loss\n",
        "\n",
        "        model.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "        indb += 1\n",
        "        pbar.update(1)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    print('Training: [epoch %d, loss %.3f] time:%.3f ' % (epoch + 1, running_loss / indb, (end_time-start_time) / 60 ))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDEqPlJrtDeZ"
      },
      "source": [
        "As before we infer on the validation sets to see the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2o72g3h6ZdP"
      },
      "source": [
        "\n",
        "dice_source_auda=inference(source_validation_loader)\n",
        "dice_target_auda=inference(target_validation_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.set_title('No adaptation vs Supervised Adaptation vs Unsupervised adaptation')\n",
        "ax1.boxplot([dice_target_noadaptation, dice_target_supervised_adaptation,dice_target_auda]);\n",
        "\n",
        "ax1.set_xticklabels(['No adaptation', 'Supervised Adaptation','ADA'])\n",
        "ax1.get_xaxis().tick_bottom()\n",
        "ax1.set_ylabel('Dice')\n",
        "\n",
        "\n",
        "print('No adaptatiion  dice performance {}' .format(np.mean(dice_target_noadaptation)))\n",
        "print('Supervised adaptatiion dice performance {}' .format(np.mean(dice_target_supervised_adaptation)))\n",
        "print('Augmentation-Unsupervised adaptatiion dice performance {}' .format(np.mean(dice_target_auda)))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDHkUXw6XFGk"
      },
      "source": [
        "As we can see the performance on the target domain has increased. Adaptation using only unlabeled data has been successful!. Now we are going to see another popular method to perform  Unsupervised domain adaptation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwfiO9GpfpHx"
      },
      "source": [
        "\n",
        "# PART 4: Adversarial Domain Adaptation\n",
        "\n",
        "One popular solution for semi-supervised domain adaptation is through [adversarial learning](https://arxiv.org/abs/1612.08894). ​\n",
        "​\n",
        "The overall idea of adversarial domain adaptation is learning feature representations that are agnostic to the data domain. ​\n",
        "​\n",
        "This is achieved by learning an adversarial network that attempts to discriminate the domain of the input data coming from both domains.\n",
        "\n",
        "<!-- The accuracy of a binary classifier that distinguishes between samples from two domains can serve as a proxy of the divergence of distributions p(Xs) and P(Xt) which otherwise is not straightforward to compute.  THis idea sas first introduce in....Insiperd by this , the authors of presented a method for simultaneously learning a domain-invariant representation and a task-related by a single network. this is done by minimizing the accuracy of an auxiliary network , a domain-discriminator that processes a hidden representation of the main network and tries to classify the domain of the input sample. -->\n",
        "\n",
        "\n",
        "<br>\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/MauricioOrbes/AML_lecture_5/master/images/adversarial.png\" width=\"800\"/>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "> __Figure__: Adversarial Domain Adaptation Scheme: The accuracy of a binary classifier can be used to measure differences from source or target distributions which could be not straightforward to computed by other means. The method simultaneously learns a domain-invariant representation $h_\\theta( \\cdot)$ and a task-related (domain classification) by a single network. This is done by minimizing the accuracy of an auxiliary network or domain-discriminator $d_\\theta(\\cdot)$ that processes a hidden representation $h_\\theta( \\cdot)$ of the main network and tries to classify the domain of the input sample\n",
        "\n",
        "The optimization loss used to carry out the adaptation is the addition between a segmentation loss and the adversarial loss as follows\n",
        "\n",
        "* $Total\\_loss = Segmentation\\_loss + \\alpha * Adversarial\\_loss \\ \\ \\ (8)$\n",
        "\n",
        "Where the segmentation loss is the same as in equations (1) and (5). \n",
        "\n",
        "* $Segmentation\\_loss =   soft\\_dice(f(source\\_images),source\\_labels) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ (9)$\n",
        "\n",
        "For the adversarial loss, we are going to use Cross-Entropy (**CE**) as a cost function as it widely used in most of the classification tasks. Note the output of the discriminator is compared against the vector $[0,1]$ as we have assigned the labels $0$ and $1$ for the source and target domains.  \n",
        "* $Adversarial\\_loss = CE(d(h(source\\_images,target\\_images)),[0,1])   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (10)$\n",
        "\n",
        "## Training schedule:\n",
        "A complication of adversarial training concerns the strength with which the segmenter is adapting its features in order to counter the discriminator, which is controlled by the parameter $\\alpha$ eq $(10)$. Setting $\\alpha=0$ will let both networks learn independently. This will allows the segmenter to initially lear features for the segmentation of the source domain with of being affected by the noisy adversarial gradients from an initially poorly performing domain-discriminator.  As proposed in[ Kamnistast et al 2014](https://) a good practice of learning scheduling follows the next steps: \n",
        "\n",
        "1. Set $\\alpha = 0$ during the first $e_1$ epochs \n",
        "\n",
        "2. after $e_1$ increase $\\alpha$ according to a linear schedule as ($e_{curr}= $ current epoch): \n",
        "\n",
        "$$\\alpha = \\alpha_{max}\\frac{e_{curr}-e_1}{e_2-e_1} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (11)$$\n",
        "\n",
        "Where $\\alpha_{max}$ is the maximun weighting, \n",
        "\n",
        "3. set  $\\alpha = \\alpha_{max}$ after epoch  $e_2$\n",
        "\n",
        "\n",
        "## Excersise: Schedule Adversarial Domain adaptation learning.\n",
        "\n",
        "The code below performs adversarial domain adaptation. However as currently implemented both networks trains simultaneously from scratch. We would like you to implement the above-mentioned steps in order to get proper training. Try different values for $e_1, $ $e_2$, and $e_{max}$ and see the impact on the performance.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1bGJionfQnR"
      },
      "source": [
        "\n",
        "\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "# Adversarial Domain Adaptation\n",
        "\n",
        "# let's define the Discriminator model\n",
        "\n",
        "class DiscriminatorDomain(nn.Module):\n",
        "    def __init__(self, num_channels,num_classes,complexity):\n",
        "        super(DiscriminatorDomain, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, int(8*complexity), kernel_size=3, stride=2)\n",
        "        self.BN1   = nn.BatchNorm2d(int(8*complexity))\n",
        "        self.conv2 = nn.Conv2d(int(8*complexity), int(16 * complexity) , kernel_size=3, stride=2)\n",
        "        self.BN2   = nn.BatchNorm2d(int(16* complexity))\n",
        "        self.conv3 = nn.Conv2d(int(16* complexity), int(32* complexity),  kernel_size=3, stride=2 )\n",
        "        self.BN3   = nn.BatchNorm2d(int(32* complexity))\n",
        "        self.conv4 = nn.Conv2d(int(32* complexity),int(64* complexity),kernel_size=3, stride=2)\n",
        "        self.BN4   = nn.BatchNorm2d(int(64* complexity))\n",
        "        \n",
        "        self.fc1 = nn.Linear(int(64 * 7 * 7 * complexity), int(128* complexity))\n",
        "        self.drop_1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(int(128* complexity), int(64* complexity))\n",
        "        self.drop_2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(int(64* complexity), num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.leaky_relu(self.BN1(self.conv1(x)),0.2)\n",
        "        x = F.leaky_relu(self.BN2(self.conv2(x)),0.2)\n",
        "        x = F.leaky_relu(self.BN3(self.conv3(x)),0.2)\n",
        "\n",
        "        x = F.leaky_relu(self.BN4(self.conv4(x)),0.2)\n",
        "        complexity =x.size(1)\n",
        "        x = x.view(-1, int( 7 * 7 * complexity))\n",
        "\n",
        "        x = F.relu(self.drop_1(self.fc1(x)))\n",
        "        x = F.relu(self.drop_2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "Ninitialfilters = 4\n",
        "complexity = Ninitialfilters / 8 \n",
        "\n",
        "# Set The discriminator model\n",
        "discriminator = DiscriminatorDomain(352,2,complexity)\n",
        "\n",
        "# Let's load the pretrained model again\n",
        "model.load_state_dict(torch.load('clinic1B2assource2load.pt'))\n",
        "\n",
        "number_of_epochs = 10\n",
        "\n",
        "#EXERCISE: initialize the parameters alpha, alpha_max, e1, and e2 \n",
        "\n",
        "alpha_max= 0.3 # YOUR CODE HERE\n",
        "e1= 2 # YOUR CODE HERE\n",
        "e2= 5 # YOUR CODE HERE\n",
        "\n",
        "# We need to initialized the optimazers\n",
        "optimizer_model = optim.Adam(model.parameters(), lr=1e-4 )\n",
        "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=1e-4 )\n",
        "\n",
        "# Let's define the loss function to train the discriminator\n",
        "lf_discriminator  =  nn.CrossEntropyLoss(size_average=True)\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "  \n",
        "  # The Scheduling is governed by the parameter alpha in function of epochs\n",
        "  # EXERCISE: Fill each one of the following conditions in order to shedule the training.\n",
        "\n",
        "  if epoch < e1:\n",
        "    alpha = 0 # YOUR CODE HERE\n",
        "  elif epoch > e1 and epoch <= e2:\n",
        "    alpha = alpha_max*(epoch-e1)/(e2-e1) # YOUR CODE HERE\n",
        "  else:\n",
        "    alpha = alpha_max # YOUR CODE HERE\n",
        "\n",
        "  with tqdm(total=len(source_training_loader), file=sys.stdout) as pbar:\n",
        "\n",
        "    start_time = time.time()\n",
        "    running_loss = 0\n",
        "    indb =0\n",
        "    for patch_s,patch_t in zip(source_training_loader,target_training_loader):\n",
        "        \n",
        "        model.eval()\n",
        "        discriminator.train()\n",
        "        source_batch_images = patch_t['flair']['data'][...,0].float()\n",
        "        source_batch_labels = patch_t['label']['data'][...,0]\n",
        "        \n",
        "        target_batch_images = patch_s['flair']['data'][...,0].float()\n",
        "        \n",
        "        #  First step the discriminator is updated\n",
        "\n",
        "        # The input of the discriminator is composed of slices of Target and source \n",
        "        # Domain, so the ideas is clasify them\n",
        "        inputs_model_adv = torch.cat((source_batch_images,target_batch_images),0)\n",
        "        \n",
        "        # We generate the labels to train the discriminator(0: for source domain 1: target domain)\n",
        "        labels_discriminator = torch.cat((torch.zeros(6),torch.ones(6)),0).type(torch.LongTensor)\n",
        "        \n",
        "        # This is equivalent to get h(x) from the model (basicaly takes the 4 last \n",
        "        # decoder representation from the U-net)\n",
        "        _,_,_,_,_,_,dec4,dec3,dec2,dec1 =  model(Variable(inputs_model_adv))        \n",
        "        \n",
        "        dec1 = F.interpolate(dec1, size = dec2.size()[2:], mode = 'bilinear')\n",
        "        dec2 = F.interpolate(dec2, size = dec2.size()[2:], mode = 'bilinear')\n",
        "        dec3 = F.interpolate(dec3, size = dec2.size()[2:], mode = 'bilinear')\n",
        "        dec4 = F.interpolate(dec4, size = dec2.size()[2:], mode = 'bilinear')\n",
        "\n",
        "        inputs_discriminator = torch.cat((dec1,dec2,dec3,dec4),1)\n",
        "        outputs_discriminator = discriminator(inputs_discriminator)\n",
        "\n",
        "        loss_classifier = lf_discriminator(Variable(outputs_discriminator,requires_grad=True), Variable(labels_discriminator))\n",
        "        \n",
        "        discriminator.zero_grad()\n",
        "        loss_classifier.backward()\n",
        "        optimizer_discriminator.step()\n",
        "            \n",
        "\n",
        "        # TRAIN THE SEGMENTER\n",
        "        # ideally it should be used a diferent batch to feed the network.\n",
        "        # for simplicity we use the same batch (the performance seems not \n",
        "        # be affected too much according to previous experiments).\n",
        "        \n",
        "        model.train()\n",
        "        discriminator.eval()\n",
        "\n",
        "\n",
        "        outputs_source, _, _, _, _, _, _, _, _, _ = model(Variable(source_batch_images,requires_grad=True))\n",
        "        \n",
        "\n",
        "        _,_,_,_,_,_,dec4,dec3,dec2,dec1 =  model(Variable(inputs_model_adv))        \n",
        "        \n",
        "        dec1 = F.interpolate(dec1, size = dec2.size()[2:], mode = 'bilinear')\n",
        "        dec2 = F.interpolate(dec2, size = dec2.size()[2:], mode = 'bilinear')\n",
        "        dec3 = F.interpolate(dec3, size = dec2.size()[2:], mode = 'bilinear')\n",
        "        dec4 = F.interpolate(dec4, size = dec2.size()[2:], mode = 'bilinear')\n",
        "\n",
        "        inputs_discriminator = torch.cat((dec1,dec2,dec3,dec4),1)\n",
        "        outputs_discriminator = discriminator(inputs_discriminator)\n",
        "        \n",
        "        supervised_source_loss = dice_soft_loss(torch.sigmoid(outputs_source), Variable(source_batch_labels,requires_grad=True))\n",
        "        loss_adv = lf_discriminator(Variable(outputs_discriminator,requires_grad=True), Variable(labels_discriminator))\n",
        "\n",
        "        total_loss = supervised_source_loss -alpha*loss_adv\n",
        "        \n",
        "        model.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += total_loss.item()\n",
        "        indb += 1       \n",
        "        \n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    print('Training: [epoch %d, loss %.3f] time:%.3f ' % (epoch + 1, running_loss / indb, (end_time-start_time) / 60 ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgr6hWAli00w"
      },
      "source": [
        "Let's do inference and compare with the previous methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A3_jTAkitxI"
      },
      "source": [
        "\n",
        "\n",
        "dice_source_adversarial=inference(source_validation_loader)\n",
        "dice_target_adversarial=inference(target_validation_loader)\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.set_title('No adaptation vs Supervised Adaptation vs ADA vs Adversarial')\n",
        "ax1.boxplot([dice_target_noadaptation, dice_target_supervised_adaptation,dice_target_auda,dice_target_adversarial]);\n",
        "\n",
        "ax1.set_xticklabels(['No adaptation', 'Supervised Adaptation','ADA', 'Adversarial'])\n",
        "ax1.get_xaxis().tick_bottom()\n",
        "ax1.set_ylabel('Dice')\n",
        "\n",
        "\n",
        "print('No adaptatiion  dice performance {}' .format(np.mean(dice_target_noadaptation)))\n",
        "print('Supervised adaptatiion dice performance {}' .format(np.mean(dice_target_supervised_adaptation)))\n",
        "print('Augmentation-Unsupervised adaptatiion dice performance {}' .format(np.mean(dice_target_auda)))\n",
        "print('Adversatial adaptation dice performance {}' .format(np.mean(dice_target_adversarial)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}