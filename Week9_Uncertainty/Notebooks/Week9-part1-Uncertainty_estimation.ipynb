{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Uncertainty_estimation_(Foundations)_Incomplete.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"T_E_x3zY0RIT"},"source":["# Uncertainty Estimation Notebook (Foundations)\n","\n","This notebook will introduce you to uncertainty estimatiton using neural networks. It is highly inpsired by JavierAntoran's Bayesian-Neural-Networks GITHUB [repo](https://github.com/JavierAntoran/Bayesian-Neural-Networks/blob/master/README.md#bayes-by-backprop-bbp). \n","\n","We will use Gaussian processes as the backbone to create functions with ground truth uncertainty in them. We will model different types of uncertainties.\n","\n","A sequence (or a vector) of random variables is denoted homoscedastic if all its random variables have the same finite variance - also known as homogeneity of variance. \n","\n","The complementary notion is called heteroscedasticity, where the assumption is that there are sub-populations of random variables that have different variabilities from others."]},{"cell_type":"markdown","metadata":{"id":"SncwXcpz2bAs"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","# Getting set up\n","\n","Let's start by installing and importing all packages we need, including GPy for gaussian processes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAYPKSFrG8AF","executionInfo":{"status":"ok","timestamp":1616064416707,"user_tz":0,"elapsed":44838,"user":{"displayName":"Mark Graham","photoUrl":"","userId":"09933508565499474713"}},"outputId":"4b131b60-8b37-4f14-8fac-059f10cc713d"},"source":["\n","! pip install GPy\n","import GPy\n","import time\n","import copy\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.optim import Optimizer\n","from torch.optim.sgd import SGD\n","\n","from tqdm import tqdm, trange\n","from google.colab import files\n","%config InlineBackend.figure_format = 'svg'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting GPy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/976598f98adbfa918a480cb2d643f93fb555ca5b6c5614f76b69678114c1/GPy-1.9.9.tar.gz (995kB)\n","\u001b[K     |████████████████████████████████| 1.0MB 10.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.19.5)\n","Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n","Collecting paramz>=0.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/37/4abbeb78d30f20d3402887f46e6e9f3ef32034a9dea65d243654c82c8553/paramz-0.9.5.tar.gz (71kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n","\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n","Building wheels for collected packages: GPy, paramz\n","  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for GPy: filename=GPy-1.9.9-cp37-cp37m-linux_x86_64.whl size=2626953 sha256=b53c752978ec4945808935176a79b4f2efe31179ebefe37506cee7943092b3f0\n","  Stored in directory: /root/.cache/pip/wheels/5d/36/66/2b58860c84c9f2b51615da66bfd6feeddbc4e04d887ff96dfa\n","  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for paramz: filename=paramz-0.9.5-cp37-none-any.whl size=102552 sha256=c4118b2320dd6b4dab32429e187283d7cf5203e4659a19abd0d68d3214ce2ac4\n","  Stored in directory: /root/.cache/pip/wheels/c8/4a/0e/6e0dc85541825f991c431619e25b870d4b812c911214690cf8\n","Successfully built GPy paramz\n","Installing collected packages: paramz, GPy\n","Successfully installed GPy-1.9.9 paramz-0.9.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_1S5kt0omQ-N"},"source":["def to_variable(var=(), cuda=True, volatile=False):\n","    out = []\n","    for v in var:\n","        \n","        if isinstance(v, np.ndarray):\n","            v = torch.from_numpy(v).type(torch.FloatTensor)\n","\n","        if not v.is_cuda and cuda:\n","            v = v.cuda()\n","\n","        if not isinstance(v, Variable):\n","            v = Variable(v, volatile=volatile)\n","\n","        out.append(v)\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OJfVnYt4fYz"},"source":["Let's now define the gaussian loss between output (x) and target ($\\mu$), given a covariance matrix $\\Sigma$ and $k$ dimensions\n","\n","$\\Large{\\displaystyle (2\\pi )^{-{\\frac {k}{2}}}\\det({\\boldsymbol {\\Sigma }})^{-{\\frac {1}{2}}}\\,e^{-{\\frac {1}{2}}(\\mathbf {x} -{\\boldsymbol {\\mu }})^{\\!{\\mathsf {T}}}{\\boldsymbol {\\Sigma }}^{-1}(\\mathbf {x} -{\\boldsymbol {\\mu }})},}$\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"Va8V78eFFsc9","executionInfo":{"status":"error","timestamp":1616064668967,"user_tz":0,"elapsed":690,"user":{"displayName":"Mark Graham","photoUrl":"","userId":"09933508565499474713"}},"outputId":"e21af861-e2b4-43ba-a98f-58099a97c166"},"source":["def log_gaussian_loss(output, target, sigma, no_dim):\n","    # To ensure non-negativity of the gaussian loss, let's define it in log space\n","    # Take the log of the gaussian function and estimate the log of the exponent and the log of the coefficient\n","    # TODO\n","    exponent = #YOUR CODE HERE\n","    log_coeff = #YOUR CODE HERE\n","    \n","    return #YOUR CODE HERE\n","\n","\n","def get_kl_divergence(weights, prior, varpost):\n","    prior_loglik = prior.loglik(weights)\n","    \n","    varpost_loglik = varpost.loglik(weights)\n","    varpost_lik = varpost_loglik.exp()\n","    \n","    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n","\n","\n","class gaussian:\n","    def __init__(self, mu, sigma):\n","        self.mu = mu\n","        self.sigma = sigma\n","        \n","    def loglik(self, weights):\n","        # Do the same as a\n","        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n","        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n","        \n","        return (exponent + log_coeff).sum()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-9ce5f5d66b44>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    exponent = #YOUR CODE HERE\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"pyPMIIn2pu-l"},"source":["# Part 0 - Generating a Ground Truth with GP\n","\n","Let's first generate a function with some ground truth variance"]},{"cell_type":"code","metadata":{"id":"y9piOiU4ptnH"},"source":["np.random.seed(2)\n","number_of_points = 400 \n","lengthscale = 1 #the smoothness of the function\n","variance = 1.0 # the variance of the random variable (Epistemic)\n","sig_noise = 0.3 # extra noise on top of the random variable noise (Aletoric)\n","x = np.random.uniform(-3, 3, number_of_points)[:, None]\n","x.sort(axis = 0)\n","\n","# First let's generate the data\n","k = GPy.kern.RBF(input_dim = 1, variance = variance, lengthscale = lengthscale)\n","C = k.K(x, x) + np.eye(number_of_points)*sig_noise**2\n","y = np.random.multivariate_normal(np.zeros((number_of_points)), C)[:, None]\n","y = (y - y.mean())\n","\n","# Get the central points of vector given the number_of_points above\n","# E.g. remove the first and last 75 points\n","# TODO\n","x_train = #YOUR CODE HERE\n","y_train = #YOUR CODE HERE\n","\n","# Now let's fit is assuming a RBF kernel plus white noise\n","rbf = GPy.kern.RBF(input_dim=1, variance=1, lengthscale=1.)\n","white = GPy.kern.White(input_dim=1, variance = 0.09)\n","m = GPy.models.GPRegression(x_train, y_train, rbf + white)\n","\n","means, total_unc = m.predict(np.linspace(-5, 5, 200)[:, None])\n","means = means.reshape(-1)\n","aleatoric = m.parameters[0].white.parameters[0][0]**0.5\n","total_unc = (total_unc/2).reshape(-1)**0.5\n","\n","# The epistemic uncertainty is the total uncertainty minus the aletoric one\n","# TODO\n","epistemic = #YOUR CODE HERE\n","\n","\n","plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n","plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = '#1f77b4', alpha = 0.3, label = 'Epistemic + Aleatoric')\n","plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = '#1f77b4', alpha = 0.3)\n","plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = '#ff7f0e', alpha = 0.4, label = 'Aleatoric')\n","plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n","plt.xlim([-5, 5])\n","plt.ylim([-5, 7])\n","plt.xlabel('$x$', fontsize=15)\n","plt.title('GP Ground truth', fontsize=20)\n","plt.tick_params(labelsize=10)\n","plt.xticks(np.arange(-4, 5, 2))\n","plt.yticks(np.arange(-4, 7, 2))\n","plt.gca().yaxis.grid(alpha=0.3)\n","plt.gca().xaxis.grid(alpha=0.3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rIcVMl1XuM6G"},"source":["# Part 1 - Bayes by Backprop - Homoscedastic\n","\n","This section is based on the work of Blundell et al. ICML 2015\n","\n","https://arxiv.org/pdf/1505.05424.pdf\n","\n","In this work, the weights themselves have an uncertainty associated with them.\n","\n","Now please read section 3.2 of the paper.\n","Let's implement the bayes layer and the forward pass. \n","\n"]},{"cell_type":"code","metadata":{"id":"Il-AkG-auPtt"},"source":["class BayesLinear_Normalq(nn.Module):\n","    def __init__(self, input_dim, output_dim, prior):\n","        super(BayesLinear_Normalq, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.prior = prior\n","        \n","        scale = (2/self.input_dim)**0.5\n","        rho_init = np.log(np.exp((2/self.input_dim)**0.5) - 1)\n","        # Initialise the MUs weights and biases as a uniform(-0.05,0.05) and\n","        # the RHOs weights and biases as a uniform(-2,-1)\n","        # TODO\n","        self.weight_mus = #YOUR CODE HERE\n","        self.weight_rhos = #YOUR CODE HERE\n","        self.bias_mus = #YOUR CODE HERE\n","        self.bias_rhos = #YOUR CODE HERE\n","        \n","    def forward(self, x, sample = True):\n","        \n","        if sample:\n","            # Implement the sampling scheme of section 3.2 of the paper, namely steps 1 and 2\n","            # sample gaussian noise for each weight and each bias \n","            # TODO\n","            weight_epsilons = #YOUR CODE HERE\n","            bias_epsilons =  #YOUR CODE HERE\n","            \n","            # calculate the weight and bias stds from the rho parameters\n","            # TODO\n","            weight_stds = #YOUR CODE HERE\n","            bias_stds = #YOUR CODE HERE\n","            \n","            # calculate samples from the posterior from the sampled noise and mus/stds\n","            # TODO\n","            weight_sample = #YOUR CODE HERE\n","            bias_sample = #YOUR CODE HERE\n","            \n","            # Now add build the fully connected layer with the weight and bias samples\n","            # TODO\n","            output = #YOUR CODE HERE\n","            \n","            # Computing the KL loss term for the weights\n","            prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n","            KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n","            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n","            KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n","            \n","            # Now add the KL loss term for the biases\n","            prior_cov, varpost_cov = self.prior.sigma**2, bias_stds**2\n","            KL_loss = KL_loss + 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*bias_stds.numel()\n","            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n","            KL_loss = KL_loss + 0.5*((self.bias_mus - self.prior.mu)**2/prior_cov).sum()\n","            \n","            return output, KL_loss\n","        \n","        else:\n","            output = torch.mm(x, self.weight_mus) + self.bias_mus\n","            return output, KL_loss\n","        \n","    def sample_layer(self, no_samples):\n","        all_samples = []\n","        for i in range(no_samples):\n","            # sample gaussian noise for each weight and each bias\n","            weight_epsilons = #YOUR CODE HERE\n","            \n","            # calculate the weight and bias stds from the rho parameters\n","            weight_stds = #YOUR CODE HERE\n","            \n","            # calculate samples from the posterior from the sampled noise and mus/stds\n","            weight_sample = #YOUR CODE HERE\n","            all_samples += weight_sample.view(-1).cpu().data.numpy().tolist()\n","            \n","        return all_samples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iXbLv-aUuSFH"},"source":["Now write the actual model itself and wrap it in a single function\n"]},{"cell_type":"code","metadata":{"id":"J-GrXX2UuTuc"},"source":["class BBP_Homoscedastic_Model(nn.Module):\n","    def __init__(self, input_dim, output_dim, no_units, init_log_noise):\n","        super(BBP_Homoscedastic_Model, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        \n","        # network with two hidden and one output layer\n","        # TODO\n","        self.layer1 = #YOUR CODE HERE\n","        self.layer2 = #YOUR CODE HERE\n","        \n","        # activation to be used between hidden layers\n","        self.activation = nn.ReLU(inplace = True)\n","        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([init_log_noise]))\n","\n","    \n","    def forward(self, x):\n","        # Now let's implement the network as a layer->actiavation->layer \n","        # Dont forget to collect the KL_Loss as it is generated by each layer\n","        # YOUR CODE HERE\n","        \n","        return x, KL_loss_total\n","\n","\n","class BBP_Homoscedastic_Model_Wrapper:\n","    def __init__(self, input_dim, output_dim, no_units, learn_rate, batch_size, no_batches, init_log_noise):\n","        \n","        self.learn_rate = learn_rate\n","        self.batch_size = batch_size\n","        self.no_batches = no_batches\n","        \n","        self.network = BBP_Homoscedastic_Model(input_dim = input_dim, output_dim = output_dim,\n","                                               no_units = no_units, init_log_noise = init_log_noise)\n","        self.network.cuda()\n","        \n","        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n","        self.loss_func = log_gaussian_loss\n","    \n","    def fit(self, x, y, no_samples):\n","        x, y = to_variable(var=(x, y), cuda=True)\n","        \n","        self.optimizer.zero_grad()\n","        fit_loss_total = 0\n","        \n","        for i in range(no_samples):\n","            output, KL_loss_total = self.network(x)\n","\n","            # calculate fit loss based on mean and standard deviation of output\n","            fit_loss_total = fit_loss_total + self.loss_func(output, y, self.network.log_noise.exp(), self.network.output_dim)\n","        \n","        KL_loss_total = KL_loss_total/self.no_batches\n","        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n","        total_loss.backward()\n","        self.optimizer.step()\n","\n","        return fit_loss_total/no_samples, KL_loss_total"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeUzbdWcuXhL"},"source":["Train the model\n"]},{"cell_type":"code","metadata":{"id":"0VB_HHlpuroc"},"source":["np.random.seed(2)\n","no_points = 400\n","lengthscale = 1.0\n","variance = 1.0\n","sig_noise = 0.3\n","x = np.random.uniform(-3, 3, no_points)[:, None]\n","x.sort(axis = 0)\n","\n","\n","k = GPy.kern.RBF(input_dim = 1, variance = variance, lengthscale = lengthscale)\n","C = k.K(x, x) + np.eye(no_points)*sig_noise**2\n","\n","y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n","y = (y - y.mean())\n","x_train = x[75:325]\n","y_train = y[75:325]\n","\n","x_mean, x_std = x_train.mean(), x_train.var()**0.5\n","y_mean, y_std = y_train.mean(), y_train.var()**0.5\n","\n","x_train = (x_train - x_mean)/x_std\n","y_train = (y_train - y_mean)/y_std\n","\n","\n","num_epochs, batch_size, nb_train = 2000, len(x_train), len(x_train)\n","\n","net = BBP_Homoscedastic_Model_Wrapper(input_dim = 1, output_dim = 1, no_units = 100, learn_rate = 1e-1,\n","                                      batch_size = batch_size, no_batches = 1, init_log_noise = 0)\n","\n","fit_loss_train = np.zeros(num_epochs)\n","KL_loss_train = np.zeros(num_epochs)\n","total_loss = np.zeros(num_epochs)\n","\n","best_net, best_loss = None, float('inf')\n","\n","for i in range(num_epochs):\n","    \n","    fit_loss, KL_loss = net.fit(x_train, y_train, no_samples = 10)\n","    fit_loss_train[i] += fit_loss.cpu().data.numpy()\n","    KL_loss_train[i] += KL_loss.cpu().data.numpy()\n","    \n","    total_loss[i] = fit_loss_train[i] + KL_loss_train[i]\n","    \n","    if fit_loss < best_loss:\n","        best_loss = fit_loss\n","        best_net = copy.deepcopy(net.network)\n","        \n","    if i % 100 == 0 or i == num_epochs - 1:\n","        \n","        print(\"Epoch: %5d/%5d, Fit loss = %8.3f, KL loss = %8.3f, noise = %6.3f\" %\n","              (i + 1, num_epochs, fit_loss_train[i], KL_loss_train[i], net.network.log_noise.exp().cpu().data.numpy()))\n","\n","        samples = []\n","        for i in range(100):\n","            preds = net.network.forward(torch.linspace(-3, 3, 200).cuda())[0]\n","            samples.append(preds.cpu().data.numpy()[:, 0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJ8I9ov0uvRt"},"source":["And Finally test it "]},{"cell_type":"code","metadata":{"id":"4pcybDMzutrR"},"source":["samples = []\n","for i in range(100):\n","    preds = (best_net.forward(torch.linspace(-5, 5, 200).cuda())[0] * y_std) + y_mean\n","    samples.append(preds.cpu().data.numpy()[:, 0])\n","\n","samples = np.array(samples)\n","means = samples.mean(axis = 0)\n","\n","aleatoric = best_net.log_noise.exp().cpu().data.numpy()\n","epistemic = samples.var(axis = 0)**0.5\n","total_unc = (aleatoric**2 + epistemic**2)**0.5\n","\n","\n","\n","plt.scatter((x_train * x_std) + x_mean, (y_train * y_std) + y_mean, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n","plt.fill_between(np.linspace(-5, 5, 200)*x_std + x_mean, means + aleatoric, means + total_unc, color = '#1f77b4', alpha = 0.3, label = r'$\\sigma(y^*|x^*)$')\n","plt.fill_between(np.linspace(-5, 5, 200)*x_std + x_mean, means - total_unc, means - aleatoric, color = '#1f77b4', alpha = 0.3)\n","plt.fill_between(np.linspace(-5, 5, 200)*x_std + x_mean, means - aleatoric, means + aleatoric, color = '#ff7f0e', alpha = 0.4, label = r'$\\EX[\\sigma^2]^{1/2}$')\n","plt.plot(np.linspace(-5, 5, 200)*x_std + x_mean, means, color = 'black', linewidth = 1)\n","plt.xlim([-5, 5])\n","plt.ylim([-5, 7])\n","plt.xlabel('$x$', fontsize=10)\n","plt.title('BBP Homoscedastic', fontsize=15)\n","plt.tick_params(labelsize=10)\n","plt.xticks(np.arange(-4, 5, 2))\n","plt.gca().yaxis.grid(alpha=0.3)\n","plt.gca().xaxis.grid(alpha=0.3)\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1MwDcfKduujg"},"source":["# Part 2 - Bayes by Backprop - Heteroscedastic\n","\n","Now let's the same for the heteroscedastic model"]},{"cell_type":"code","metadata":{"id":"QmYDIQgjvEvB"},"source":["class BBP_Heteroscedastic_Model(nn.Module):\n","    def __init__(self, input_dim, output_dim, num_units):\n","        super(BBP_Heteroscedastic_Model, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        \n","        # Network with two hidden and one output layer\n","        # Note that we now want 2 outputs, one for the prediction and one for the variance\n","        # TODO\n","        self.layer1 = #YOUR CODE HERE\n","        self.layer2 = #YOUR CODE HERE\n","        \n","        # Activation to be used between hidden layers\n","        self.activation = nn.ReLU(inplace = True)\n","    \n","    def forward(self, x):\n","        \n","        KL_loss_total = 0\n","        x = x.view(-1, self.input_dim)\n","        \n","        x, KL_loss = self.layer1(x)\n","        KL_loss_total = KL_loss_total + KL_loss\n","        x = self.activation(x)\n","        \n","        x, KL_loss = self.layer2(x)\n","        KL_loss_total = KL_loss_total + KL_loss\n","        \n","        return x, KL_loss_total\n","\n","class BBP_Heteroscedastic_Model_Wrapper:\n","    def __init__(self, network, learn_rate, batch_size, no_batches):\n","        \n","        self.learn_rate = learn_rate\n","        self.batch_size = batch_size\n","        self.no_batches = no_batches\n","        \n","        self.network = network\n","        self.network.cuda()\n","        \n","        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n","        self.loss_func = log_gaussian_loss\n","    \n","    def fit(self, x, y, no_samples):\n","        x, y = to_variable(var=(x, y), cuda=True)\n","        \n","        # reset gradient and total loss\n","        self.optimizer.zero_grad()\n","        fit_loss_total = 0\n","        \n","        for i in range(no_samples):\n","            output, KL_loss_total = self.network(x)\n","\n","            # calculate fit loss based on mean and standard deviation of output\n","            #YOUR CODE HERE\n","        \n","        KL_loss_total = KL_loss_total/self.no_batches\n","        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n","        total_loss.backward()\n","        self.optimizer.step()\n","\n","        return fit_loss_total/no_samples, KL_loss_total\n","    \n","    def get_loss_and_rmse(self, x, y, no_samples):\n","        x, y = to_variable(var=(x, y), cuda=True)\n","        \n","        means, stds = [], []\n","        for i in range(no_samples):\n","            output, KL_loss_total = self.network(x)\n","            means.append(output[:, :1, None])\n","            stds.append(output[:, 1:, None].exp())\n","            \n","        means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n","        # Calculate the mean of the means, \n","        # and the variance of the mean + the mean of the variances\n","        # TODO\n","        mean = #YOUR CODE HERE\n","        std = #YOUR CODE HERE\n","            \n","        # calculate fit loss based on mean and standard deviation of output\n","        # TODO\n","        logliks = #YOUR CODE HERE\n","        rmse = #YOUR CODE HERE\n","\n","        return logliks, rmse"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hEDOJ4vHydGu"},"source":["Lastly, let's train it and see the outcome\n","Compare to the other networks. What do you see?"]},{"cell_type":"code","metadata":{"id":"fxquYiO5vM0X"},"source":["np.random.seed(2)\n","no_points = 400\n","lengthscale = 1\n","variance = 1.0\n","sig_noise = 0.3\n","x = np.random.uniform(-3, 3, no_points)[:, None]\n","x.sort(axis = 0)\n","\n","k = GPy.kern.RBF(input_dim = 1, variance = variance, lengthscale = lengthscale)\n","C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n","\n","y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n","y = (y - y.mean())\n","x_train = x[75:325]\n","y_mean = y[75:325].mean()\n","y_std = y[75:325].var()**0.5\n","y_train = (y[75:325] - y_mean)/y_std\n","\n","\n","num_epochs, batch_size, nb_train = 2000, len(x_train), len(x_train)\n","\n","net = BBP_Heteroscedastic_Model_Wrapper(network=BBP_Heteroscedastic_Model(input_dim=1, output_dim=1, num_units=200),\n","                                        learn_rate=1e-2, batch_size=batch_size, no_batches=1)\n","\n","fit_loss_train = np.zeros(num_epochs)\n","KL_loss_train = np.zeros(num_epochs)\n","total_loss = np.zeros(num_epochs)\n","\n","best_net, best_loss = None, float('inf')\n","\n","for i in range(num_epochs):\n","    \n","    fit_loss, KL_loss = net.fit(x_train, y_train, no_samples = 10)\n","    fit_loss_train[i] += fit_loss.cpu().data.numpy()\n","    KL_loss_train[i] += KL_loss.cpu().data.numpy()\n","    \n","    total_loss[i] = fit_loss_train[i] + KL_loss_train[i]\n","    \n","    if fit_loss < best_loss:\n","        best_loss = fit_loss\n","        best_net = copy.deepcopy(net.network)\n","        \n","    if i % 100 == 0 or i == num_epochs - 1:\n","        \n","        print(\"Epoch: %5d/%5d, Fit loss = %7.3f, KL loss = %8.3f\" %\n","              (i + 1, num_epochs, fit_loss_train[i], KL_loss_train[i]))\n","\n","        samples = []\n","        for i in range(100):\n","            preds = net.network.forward(torch.linspace(-3, 3, 200).cuda())[0]\n","            samples.append(preds.cpu().data.numpy()[:, 0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YY3aRPqVvVGC"},"source":["samples, noises = [], []\n","for i in range(100):\n","    preds = best_net.forward(torch.linspace(-5, 5, 200).cuda())[0]\n","    samples.append(preds[:, 0].cpu().data.numpy()* y_std + y_mean)\n","    noises.append(preds[:, 1].exp().cpu().data.numpy()* y_std)\n","\n","samples = np.array(samples)\n","noises = np.array(noises)\n","means = samples.mean(axis = 0)\n","\n","aleatoric = (noises**2).mean(axis = 0)**0.5\n","epistemic = samples.var(axis = 0)**0.5\n","aleatoric = np.minimum(aleatoric, 10e3)\n","epistemic = np.minimum(epistemic, 10e3)\n","\n","total_unc = (aleatoric**2 + epistemic**2)**0.5\n","\n","x_mean, x_std = x_train.mean(), x_train.var()**0.5\n","\n","plt.scatter(x_train * x_std + x_mean, y_train * y_std + y_mean, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n","plt.fill_between(np.linspace(-5, 5, 200)* x_std + x_mean, means + aleatoric, means + total_unc, color = '#1f77b4', alpha = 0.3, label = 'Epistemic + Aleatoric')\n","plt.fill_between(np.linspace(-5, 5, 200)* x_std + x_mean, means - total_unc, means - aleatoric, color = '#1f77b4', alpha = 0.3)\n","plt.fill_between(np.linspace(-5, 5, 200)* x_std + x_mean, means - aleatoric, means + aleatoric, color = '#ff7f0e', alpha = 0.4, label = 'Aleatoric')\n","plt.plot(np.linspace(-5, 5, 200)* x_std + x_mean, means, color = 'black', linewidth = 1)\n","plt.xlim([-5, 5])\n","plt.ylim([-5, 7])\n","plt.xlabel('$x$', fontsize=10)\n","plt.title('BBP Heteroscedastic Gaussian', fontsize=15)\n","plt.tick_params(labelsize=10)\n","plt.xticks(np.arange(-4, 5, 2))\n","plt.yticks(np.arange(-4, 7, 2))\n","plt.gca().yaxis.grid(alpha=0.3)\n","plt.gca().xaxis.grid(alpha=0.3)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EqZLuUDBzV7S"},"source":["**QUESTION ->** How do different networks react to different parameters of no_points, lengthscale, variance, sig_noise?"]},{"cell_type":"markdown","metadata":{"id":"bnTYhhCGnflA"},"source":["# Part 3 - Dropout Homoscedastic model\n","\n","How to repruduce the above with dropout neural networks?\n","\n","Let's use Yarin Gal's Dropout-based approximation.\n","\n","First, let's start by implementing a Dropout layer by yourself"]},{"cell_type":"code","metadata":{"id":"ASGi2Ecx5G-F"},"source":["class MC_Dropout_Layer(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout_prob):\n","        super(MC_Dropout_Layer, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.dropout_prob = dropout_prob\n","        \n","        # Initialise the parameters to some uniformely distributed values close to zero\n","        # TODO\n","        self.weights = #YOUR CODE HERE\n","        self.biases = #YOUR CODE HERE\n","        \n","    def forward(self, x):\n","        \n","        # Now let's define the dropout mask as a bernouly distribution of size self.weights.shape\n","        # where the probability is (1 - self.dropout_prob) \n","        # TODO\n","        dropout_mask = #YOUR CODE HERE\n","        \n","        # Finally, let's return the masked weights\n","        # TODO\n","        return #YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nrcw6jTint0I"},"source":["Now implement the homoscedastic model using either your layer or, if you prefer, the pytorch dropout layer. "]},{"cell_type":"code","metadata":{"id":"_8dV-QIq5G-I"},"source":["class MC_Dropout_Model(nn.Module):\n","    def __init__(self, input_dim, output_dim, no_units, init_log_noise, drop_prob):\n","        super(MC_Dropout_Model, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.drop_prob = drop_prob\n","        self.output_dim = output_dim\n","\n","        # Our networ will be a 2 layer fully connected neural network \n","        # Define it below\n","        # TODO \n","        self.layer1 = #YOUR CODE HERE\n","        self.layer2 = #YOUR CODE HERE\n","        \n","        # activation to be used between hidden layers\n","        self.activation = nn.ReLU(inplace = True)\n","        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([init_log_noise]))\n","\n","    \n","    def forward(self, x):\n","        \n","        x = x.view(-1, self.input_dim)\n","        \n","        # Now link it all up\n","        # It should be x -> layer1 -> activation -> dropout -> layer 2 -> output\n","        # TODO\n","        #YOUR CODE HERE\n","        \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DHlPcBJUnYWe"},"source":["Now that we have the homoscedastic model, let's wrap the model to make it easier to train with one single line. "]},{"cell_type":"code","metadata":{"id":"oAYelw3B5G-K"},"source":["class MC_Dropout_Homoscedastic_Wrapper:\n","    def __init__(self, input_dim, output_dim, no_units, learn_rate, batch_size, no_batches, weight_decay, init_log_noise, drop_prob):\n","        \n","        self.learn_rate = learn_rate\n","        self.drop_prob  = drop_prob\n","        self.batch_size = batch_size\n","        self.no_batches = no_batches\n","        \n","        self.network = MC_Dropout_Model(input_dim = input_dim, output_dim = output_dim,\n","                                        no_units = no_units, init_log_noise = init_log_noise,\n","                                        drop_prob = drop_prob)\n","        self.network.cuda()\n","        \n","        self.optimizer = torch.optim.SGD(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n","        self.loss_func = log_gaussian_loss\n","    \n","    def fit(self, x, y):\n","        x, y = to_variable(var=(x, y), cuda=True)\n","        \n","        # Reset gradient and total loss\n","        self.optimizer.zero_grad()\n","        \n","        # Now get the output for the network and calculate the loss \n","        # (dont forget to devide by the lenght of the output vector)\n","        # TODO\n","\n","        output = #YOUR CODE HERE\n","        loss = #YOUR CODE HERE\n","        \n","        loss.backward()\n","        self.optimizer.step()\n","\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7KU1Tg3XnUhy"},"source":["Finally, we are going to train MC Homoscedastic model using a similar generative model to the GP one above"]},{"cell_type":"code","metadata":{"id":"ym6HBK-s8GnO"},"source":["np.random.seed(2)\n","no_points = 400\n","lengthscale = 1\n","variance = 1.0\n","sig_noise = 0.3\n","x = np.random.uniform(-3, 3, no_points)[:, None]\n","x.sort(axis = 0)\n","\n","\n","k = GPy.kern.RBF(input_dim = 1, variance = variance, lengthscale = lengthscale)\n","C = k.K(x, x) + np.eye(no_points)*sig_noise**2\n","\n","y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n","y = (y - y.mean())\n","x_train = x[75:325]\n","y_train = y[75:325]\n","\n","num_epochs, batch_size, nb_train = 2000, len(x_train), len(x_train)\n","\n","net = MC_Dropout_Homoscedastic_Wrapper(input_dim = 1, output_dim=1, no_units=200, \n","                                       learn_rate=1e-2, batch_size=batch_size, no_batches=1, \n","                                       init_log_noise=0, weight_decay=1e-2,drop_prob=0.5)\n","\n","for i in range(num_epochs):\n","    \n","    loss = net.fit(x_train, y_train)\n","    \n","    if i % 200 == 0:\n","        print('Epoch: %4d, Train loss = %7.3f, noise = %6.3f' % \\\n","              (i, loss.cpu().data.numpy(), torch.exp(net.network.log_noise).cpu().data.numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wHvQj0s8nQ-u"},"source":["Let's print the model predictions"]},{"cell_type":"code","metadata":{"id":"tvP-HbNH_Uen"},"source":["samples = []\n","noises = []\n","\n","# What would the network predict for an input with linspace(-5, 5, 200)?\n","for i in range(1000):\n","    # TODO \n","    # (Don't forget to run on GPU, move to CPU, and then convert to numpy)\n","    preds = #YOUR CODE HERE\n","    samples.append(preds)\n","    \n","samples = np.array(samples)\n","means = (samples.mean(axis = 0)).reshape(-1)\n","aleatoric = torch.exp(net.network.log_noise).cpu().data.numpy()\n","epistemic = (samples.var(axis = 0)**0.5).reshape(-1)\n","total_unc = (aleatoric**2 + epistemic**2)**0.5\n","\n","plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n","plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = '#1f77b4', alpha = 0.3, label = 'Epistemic + Aleatoric')\n","plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = '#1f77b4', alpha = 0.3)\n","plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = '#ff7f0e', alpha = 0.4, label = 'Aleatoric')\n","plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n","plt.xlim([-5, 5])\n","plt.ylim([-5, 7])\n","plt.xlabel('$x$', fontsize=15)\n","plt.title('GP Ground truth', fontsize=20)\n","plt.tick_params(labelsize=10)\n","plt.xticks(np.arange(-4, 5, 2))\n","plt.yticks(np.arange(-4, 7, 2))\n","plt.gca().yaxis.grid(alpha=0.3)\n","plt.gca().xaxis.grid(alpha=0.3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HvzPqT8xoRvd"},"source":["# Part 4 - Heteroscedastic model\n","Now lets to the same thing for the Heteroscedastic model.\n","Wrap it again around a function to amke it easier to train"]},{"cell_type":"code","metadata":{"id":"XnQ6ClhZlOKr"},"source":["class MC_Dropout_Heteroscedastic_Wrapper:\n","    def __init__(self, network, learn_rate, batch_size, weight_decay):\n","        \n","        self.learn_rate = learn_rate\n","        self.batch_size = batch_size\n","        \n","        self.network = network\n","        self.network.cuda()\n","        \n","        self.optimizer = torch.optim.SGD(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n","        self.loss_func = log_gaussian_loss\n","    \n","    def fit(self, x, y):\n","        x, y = to_variable(var=(x, y), cuda=True)\n","        \n","        # reset gradient and total loss\n","        self.optimizer.zero_grad()\n","        \n","        output = self.network(x)\n","        # The loss here will be similar to the homoscedastic one, \n","        # but we have a per sample variance predicted from output\n","        # TODO\n","        loss = #YOUR CODE HERE\n","        \n","        loss.backward()\n","        self.optimizer.step()\n","\n","        return loss\n","    \n","    def get_loss_and_rmse(self, x, y, num_samples):\n","        x, y = to_variable(var=(x, y), cuda=True)\n","        \n","        means, stds = [], []\n","        # Let's fetch the mean and variance predictions from the network\n","        # they will be stored as two 1D vectors\n","        for i in range(num_samples):\n","            output = self.network(x)\n","            # TODO\n","            # YOUR CODE HERE\n","        \n","        means, stds = torch.cat(means, dim=1), torch.cat(stds, dim=1)\n","        mean = means.mean(dim=-1)[:, None]\n","        \n","        # The total STD equals the variance of the mean plus the mean predicted variance\n","        # TODO\n","        std = #YOUR CODE HERE\n","        loss = #YOUR CODE HERE\n","        \n","        rmse = #YOUR CODE HERE\n","\n","        return loss.detach().cpu(), rmse.detach().cpu()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hjr3IoZo4tB"},"source":["Let's now train the model, but first, we need to change the generative model so that the noise variance changes at each location X"]},{"cell_type":"code","metadata":{"id":"viFCe6gUlXTz"},"source":["np.random.seed(2)\n","no_points = 400\n","lengthscale = 1\n","variance = 1.0\n","sig_noise = 0.3\n","\n","# X is a uniform variable between -3 and 3\n","# which will be sorted to give us a varying X\n","# TODO\n","x = #YOUR CODE HERE\n","x.sort(axis = 0)\n","\n","\n","k = GPy.kern.RBF(input_dim=1, variance=variance, lengthscale=lengthscale)\n","# Now let's increase the variance of C by (x + 2)^2 multiplied by a fixed sigma noise\n","# TODO\n","C = #YOUR CODE HERE\n","\n","y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n","y = (y - y.mean())\n","x_train = x[75:325]\n","y_train = y[75:325]\n","\n","print(x_train.shape, y_train.shape)\n","num_epochs, batch_size = 4000, len(x_train)\n","\n","net = MC_Dropout_Heteroscedastic_Wrapper(network=MC_Dropout_Model(input_dim=1, output_dim=2, \n","                                                                  no_units=200, init_log_noise=0, \n","                                                                  drop_prob=0.5),\n","                                         learn_rate=1e-4, \n","                                         batch_size=batch_size, \n","                                         weight_decay=1e-2)\n","\n","fit_loss_train = np.zeros(num_epochs)\n","best_net, best_loss = None, float('inf')\n","nets, losses = [], []\n","\n","for i in range(num_epochs):\n","    \n","    loss = net.fit(x_train, y_train)\n","    \n","    if i % 200 == 0:\n","        total_loss = net.get_loss_and_rmse(x_train, y_train,100)\n","        print('Epoch: %4d, Train loss = %7.3f, Total loss = %7.3f, RMSE = %7.3f' % (i, \n","                                                                                    loss.cpu().data.numpy()/batch_size, \n","                                                                                    total_loss[0], \n","                                                                                    total_loss[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MXGNYVtYFx8b"},"source":["Before moving to the next section, play with the variance and sig_noise parameters and see the effect on the total loss and RMSE. Is there a relationship?"]},{"cell_type":"markdown","metadata":{"id":"CelmVN5ko87B"},"source":["### Display\n","Display the results and compare to the homoscedastic section.\n","\n","What differences do you see?\n"]},{"cell_type":"code","metadata":{"id":"TXuJlFvSldV2"},"source":["samples = []\n","noises = []\n","for i in range(1000):\n","    preds = net.network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n","    samples.append(preds[:, 0])\n","    noises.append(np.exp(preds[:, 1]))\n","    \n","samples = np.array(samples)\n","noises = np.array(noises)\n","means = (samples.mean(axis = 0)).reshape(-1)\n","aleatoric = (noises**2).mean(axis = 0)**0.5\n","epistemic = (samples.var(axis = 0)**0.5).reshape(-1)\n","total_unc = (aleatoric**2 + epistemic**2)**0.5\n","\n","\n","plt.figure(figsize = (6, 5))\n","plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n","plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = '#1f77b4', alpha = 0.3, label = 'Epistemic + Aleatoric')\n","plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = '#1f77b4', alpha = 0.3)\n","plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = '#ff7f0e', alpha = 0.4, label = 'Aleatoric')\n","plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n","plt.xlim([-5, 5])\n","plt.ylim([-5, 7])\n","plt.xlabel('$x$', fontsize=10)\n","plt.title('MC Heteroscedastic Dropout', fontsize=20)\n","plt.tick_params(labelsize=10)\n","plt.xticks(np.arange(-4, 5, 2))\n","plt.yticks(np.arange(-4, 7, 2))\n","plt.gca().yaxis.grid(alpha=0.3)\n","plt.gca().xaxis.grid(alpha=0.3)\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]}]}