{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQnIeitQ-GlI"
   },
   "source": [
    "# Week 8 - Unsupervised Learning\n",
    "\n",
    "### ***CODING STYLE EXPLANATION | PLEASE READ CAREFULLY TO MAKE YOUR LIFE EASIER | THIS CODE IS FOR DESCIPTIVE PURPOSES ONLY***\n",
    "\n",
    "In this notebook a different coding style is being used compared to the other ones. Generally one can trivially write a classifier network as follow:w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHPpKJhp-hqF"
   },
   "outputs": [],
   "source": [
    "class ToyClassifierNetwork(nn.Module):\r\n",
    "  def __init__(self, in_channels, conv_out_channels, no_classes, image_size=96):\r\n",
    "\r\n",
    "    super(ToyClassifierNetwork, self).__init__()\r\n",
    "    self.conv1 = nn.Conv2d(\r\n",
    "        in_channels=in_channels, \r\n",
    "        out_channels=conv_out_channels, \r\n",
    "        kernel_size=3, \r\n",
    "        stride= 2,\r\n",
    "        padding=1, \r\n",
    "    )\r\n",
    "    \r\n",
    "    self.conv2 =  nn.Conv2d(\r\n",
    "        in_channels=conv_out_channels, \r\n",
    "        out_channels=conv_out_channels*2, \r\n",
    "        kernel_size=3, \r\n",
    "        stride=2,\r\n",
    "        padding=1, \r\n",
    "    ) \r\n",
    "\r\n",
    "    in_features = (image_size/2**2)**2*conv_out_channels*2\r\n",
    "\r\n",
    "    self.linear1 = nn.Linear(\r\n",
    "        in_features=in_features\r\n",
    "        out_features=in_features//2\r\n",
    "    )\r\n",
    "\r\n",
    "    self.linear2 = nn.Linear(\r\n",
    "        in_features=in_features//2,\r\n",
    "        out_features=3\r\n",
    "    )\r\n",
    "    \r\n",
    "    self.activation = nn.LeakyReLU(inplace=False)\r\n",
    "\r\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight.data)\r\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight.data)\r\n",
    "    torch.nn.init.xavier_normal_(self.linear1.weight.data)\r\n",
    "    torch.nn.init.xavier_normal_(self.linear2.weight.data)\r\n",
    "\r\n",
    "  def forward(self, x):\r\n",
    "    out = self.conv1(x)\r\n",
    "    out = self.activation(out)\r\n",
    "\r\n",
    "    out = self.conv2(out)\r\n",
    "    out = self.activation(out)\r\n",
    "\r\n",
    "    out = torch.flatten(out, start_dim=1)\r\n",
    "\r\n",
    "    out = self.linear1(out)\r\n",
    "    out = self.activation(out)\r\n",
    "\r\n",
    "    out = self.linear2(out)\r\n",
    "\r\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRKGgQoCAHgZ"
   },
   "source": [
    "But one can use torch's Dictionaries or Lists to store programatically defined network layers. It helps you have more control over the network structure without risking introducing bugs every time you want to add or substract just a few layers. That will look as follows:\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFG360zxATBn"
   },
   "outputs": [],
   "source": [
    "class ToyClassifierNetwork(nn.Module):\r\n",
    "  def __init__(\r\n",
    "      self, \r\n",
    "      in_channels, \r\n",
    "      conv_out_channels,\r\n",
    "      no_classes,\r\n",
    "      no_convolutions=2, \r\n",
    "      no_linears=2, \r\n",
    "      image_size=96\r\n",
    "  ):\r\n",
    "\r\n",
    "    super(ToyClassifierNetwork, self).__init__()\r\n",
    "    \r\n",
    "    layers = torch.nn.ModuleDict()\r\n",
    "\r\n",
    "    self.no_convolutions=no_convolutions\r\n",
    "    self.no_linears=no_linears\r\n",
    "\r\n",
    "    out_channels = conv_out_channels\r\n",
    "\r\n",
    "    for i in range(no_convolutions):\r\n",
    "      layers[f\"conv_{i}\"] = nn.Conv2d(\r\n",
    "          in_channels=in_channels, \r\n",
    "          out_channels=out_channels, \r\n",
    "          kernel_size=3, \r\n",
    "          stride= 2,\r\n",
    "          padding=1, \r\n",
    "      )\r\n",
    "      in_channels = out_channels\r\n",
    "      out_channels = out_channels*2\r\n",
    "    \r\n",
    "    in_features = (image_size/2**no_convolutions)**2*out_channels\r\n",
    "    out_features = in_channels // 2 \r\n",
    "    for i in range(no_linears-1):\r\n",
    "      layers[f\"linear_{i}\"] = nn.Linea,r(\r\n",
    "        in_features=in_features\r\n",
    "        out_features=out_features\r\n",
    "      )\r\n",
    "      in_features = out_features\r\n",
    "      out_features = out_features//2\r\n",
    "\r\n",
    "\r\n",
    "    layers[f\"linear_{i+1}\"] =, nn.Linear(\r\n",
    "      in_features=out_features\r\n",
    "      out_features=3\r\n",
    "    )\r\n",
    "    \r\n",
    "    self.activation = nn.LeakyReLU(inplace=False)\r\n",
    "\r\n",
    "    for m in self.modules():\r\n",
    "      torch.nn.init.xavier_normal_(self.m.weight.data)\r\n",
    "\r\n",
    "  def forward(self, x):\r\n",
    "    out=x\r\n",
    "    for i in range(self.no_convolutions):\r\n",
    "        out=layers[f\"conv_{i}\"](out)\r\n",
    "        out = self.activation(out)\r\n",
    "\r\n",
    "    for i in range(self.no_linears-1)\r\n",
    "        out=layers[f\"linear_{i}\"](out)\r\n",
    "        out = self.activation(out)\r\n",
    "\r\n",
    "    out = layers[f\"linear_{i+1}\"](out)\r\n",
    "\r\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atOmyZDeGuFd"
   },
   "source": [
    "# Needed Setup\n",
    "To setup the data folder, just run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "oDz0vikeGssH",
    "outputId": "76614cae-a369-478d-f65a-cdf695baec33"
   },
   "outputs": [],
   "source": [
    "file_download_link = \"https://github.com/KCL-BMEIS/AdvancedMachineLearningCourse/blob/main/Week7_Unsupervised_learning/Data/Week7_data.zip?raw=true\"\n",
    "!wget -O Week8_data.zip --no-check-certificate \"$file_download_link\"\n",
    "!unzip Week8_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPJNec1qtLF6"
   },
   "source": [
    "# Imports and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_v8BB_s9THq",
    "outputId": "0ed9bf6d-15d3-4b97-f450-2d1079f3afda"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.loss import L1Loss\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "PCA_PARAMETERS = {\n",
    "    \"n_components\":2,\n",
    "    \"copy\":True,\n",
    "    \"whiten\":False,\n",
    "    \"svd_solver\": \"auto\",\n",
    "    \"tol\": 0,\n",
    "    \"iterated_power\": \"auto\",\n",
    "    \"random_state\": 0\n",
    "}\n",
    "\n",
    "NETWORK_LENGTH = 2\n",
    "NETWORK_DEPTH = 4\n",
    "FEATURE_SIZE = 2\n",
    "BATCH_SIZE = 32\n",
    "WORKERS = 4\n",
    "EPOCHS = 50\n",
    "\n",
    "def numpy_from_tensor(x):\n",
    "  return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8qw9sWUtP9J"
   },
   "source": [
    "# Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "y9luZgrlLi8e",
    "outputId": "8d3dbe62-b0bc-43e0-ec1a-b7d4ee9e9e9a"
   },
   "outputs": [],
   "source": [
    "class NiftyDataset(Dataset):\n",
    "  '''\n",
    "    Class that loads nii files, resizes them to 96x96 and feeds them\n",
    "  '''\n",
    "  def __init__(self,root_dir):\n",
    "    '''\n",
    "      root_dir - string - path towards the folder containg the data\n",
    "    '''\n",
    "    # Save the root_dir as a class variable\n",
    "    self.root_dir = root_dir\n",
    "    # Save the filenames in the root_dir as a class variable\n",
    "    self.filenames = listdir(self.root_dir)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.filenames)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    # Fetch file filename\n",
    "    img_name = self.filenames[idx]\n",
    "    # Load the nifty image\n",
    "    img = nib.load(os.path.join(self.root_dir,img_name))\n",
    "    # Get the voxel values as a numpy array\n",
    "    img = np.array(img.get_fdata())\n",
    "    # Expanding the array with 1 new dimension as feature channel\n",
    "    img = np.expand_dims(img, 0)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Loading the data\n",
    "training_dataset = NiftyDataset(\n",
    "    root_dir=join(\"/content/training\")\n",
    ")\n",
    "testing_dataset = NiftyDataset(\n",
    "    root_dir=join(\"/content/testing\")\n",
    ")\n",
    "\n",
    "# Create the required DataLoaders for training and testing\n",
    "training_loader = DataLoader(\n",
    "    training_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=WORKERS,\n",
    "    drop_last=True\n",
    ")\n",
    "testing_loader = DataLoader(\n",
    "    testing_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=WORKERS,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Show a random image from training\n",
    "plt.imshow(np.squeeze(next(iter(training_dataset))), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiAs8FEvMz5l"
   },
   "source": [
    "# Traning the network\n",
    "\n",
    "Code the training loop for the network:\n",
    "*   Loss function needs to be L1 (MAE) + L2 (MSE)\n",
    "*   Compute the predictions\n",
    "*   Compute the gradients\n",
    "*   Optimize the variables\n",
    "*   Compute test predictions\n",
    "*   Compute average test dataset loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8QkVKnAM2rj"
   },
   "outputs": [],
   "source": [
    "def display_results(x, x_hat): \n",
    "  x = numpy_from_tensor(x[0])\n",
    "  x_hat = numpy_from_tensor(x_hat[0])\n",
    "\n",
    "  plt.figure()\n",
    "  f, axarr = plt.subplots(1, 3) \n",
    "\n",
    "  axarr[0].imshow(np.squeeze(x), cmap=\"gray\") \n",
    "  axarr[0].axis('off')\n",
    "  axarr[0].title.set_text(\"Input\")\n",
    "  \n",
    "  axarr[1].imshow(np.squeeze(x-x_hat), cmap=\"gray\")\n",
    "  axarr[1].axis('off')\n",
    "  axarr[1].title.set_text(\"Residual\")\n",
    "  \n",
    "  axarr[2].imshow(np.squeeze(x_hat), cmap=\"gray\")\n",
    "  axarr[2].axis('off')\n",
    "  axarr[2].title.set_text(\"Reconstruction\")\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "def train_network(training_loader, testing_loader, network):\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  network.cuda(device)\n",
    "\n",
    "  optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "  l1_loss = L1Loss()\n",
    "  l2_loss = MSELoss()\n",
    "\n",
    "  # Train the network\n",
    "  for epoch in range(EPOCHS):\n",
    "    for x in training_loader:\n",
    "      # Move input to device\n",
    "      x = x.to(device, dtype=torch.float)\n",
    "\n",
    "      # Zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # TODO:Forward pass\n",
    "      x_hat = network(x)\n",
    "\n",
    "      # TODO:Calculate losses\n",
    "      loss = l1_loss(x, x_hat) + l2_loss(x, x_hat)\n",
    "\n",
    "      # TODO:Back-propagation of gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # TODO:Optimize\n",
    "      optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      # TODO:Initialize loss acumulators\n",
    "      l1_test_loss = 0 \n",
    "      l2_test_loss = 0 \n",
    "      for i, x in enumerate(testing_loader):\n",
    "        x = x.to(device, dtype=torch.float)\n",
    "\n",
    "        # TODO:Forward pass\n",
    "        x_hat = network(x)\n",
    "\n",
    "        # TODO:Calculate losses\n",
    "        l1_test_loss = l1_test_loss + l1_loss(x, x_hat)\n",
    "        l2_test_loss = l2_test_loss + l2_loss(x, x_hat)\n",
    "    \n",
    "      print(\n",
    "          \"==== Epoch: \" + str(epoch) + \n",
    "          \" | L1 loss: \" + str(numpy_from_tensor(l1_test_loss/i)) + \n",
    "          \" | L2 loss: \" + str(numpy_from_tensor(l2_test_loss/i)) +\n",
    "          \" | Total Loss: \" + str(numpy_from_tensor((l1_test_loss+l2_test_loss)/i))+ \" =====\")\n",
    "      display_results(x, x_hat)\n",
    "  return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxHYfNPztVun"
   },
   "source": [
    "# Network structure\n",
    "The network will be composed mainly of _Residual blocks_ which are coded for you bellow and _Linear_ layers.\n",
    "\n",
    "The structure is _Encoder_ -> _Code processor_ -> _Decoder_ where the _Code processor_ will will be changed to fit different types of networks. \n",
    "\n",
    "The _Encoder_ and the _Decoder_ will be __fully convolutional neural networks__.\n",
    "\n",
    "When coding please keep in mind that you should code in a modula way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0LzrfCmG-eH"
   },
   "source": [
    "# Residual block\n",
    "\n",
    "For the purpose of this tutorial you should aim to use the *ResBlock* as the bulding block of your _Encoder_ and _Decoder_. It should **not** be used in any of the *Code Processors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fxpnm29JG9rB"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, mode=\"level\"):\n",
    "    '''\n",
    "      in_channels - integer - the number of feature channels the first \n",
    "                              convolution will receive\n",
    "      out_channels - integer - the number of feature channels the last\n",
    "                               convolution will output\n",
    "      mode - string - defines what the block will do\n",
    "           - \"upsample\" means the block wil double the spatial size\n",
    "           - \"downsample\" means the block will halve the spatial size\n",
    "           - \"level\" means the block will not change the spatial dimension\n",
    "    '''\n",
    "    super(ResBlock, self).__init__()\n",
    "\n",
    "    if mode == \"upsample\":\n",
    "      self.conv1 = nn.ConvTranspose2d(\n",
    "          in_channels=in_channels, \n",
    "          out_channels=out_channels, \n",
    "          kernel_size=4, \n",
    "          stride=2 ,\n",
    "          padding=1, \n",
    "      )\n",
    "\n",
    "      self.conv1b = nn.ConvTranspose2d(\n",
    "          in_channels=in_channels, \n",
    "          out_channels=out_channels, \n",
    "          kernel_size=4, \n",
    "          stride=2 ,\n",
    "          padding=1, \n",
    "      )\n",
    "    else:  \n",
    "      self.conv1 = nn.Conv2d(\n",
    "          in_channels=in_channels, \n",
    "          out_channels=out_channels, \n",
    "          kernel_size=3, \n",
    "          stride=2 if mode == \"downsample\" else 1,\n",
    "          padding=1, \n",
    "      )\n",
    "\n",
    "      self.conv1b = nn.Conv2d(\n",
    "          in_channels=in_channels, \n",
    "          out_channels=out_channels, \n",
    "          kernel_size=3, \n",
    "          stride=2 if mode == \"downsample\" else 1,\n",
    "          padding=1, \n",
    "      )\n",
    "    \n",
    "    self.conv2 =  nn.Conv2d(\n",
    "        in_channels=out_channels, \n",
    "        out_channels=out_channels, \n",
    "        kernel_size=3, \n",
    "        stride=1,\n",
    "        padding=1, \n",
    "    ) \n",
    "    \n",
    "    self.activation = nn.LeakyReLU(inplace=False)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight.data)\n",
    "    torch.nn.init.xavier_normal_(self.conv1b.weight.data)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight.data)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv1(x)\n",
    "    out = self.activation(out)\n",
    "\n",
    "    skip = self.conv1b(x)\n",
    "\n",
    "    out = self.conv2(out)\n",
    "    out += skip\n",
    "\n",
    "    out = self.activation(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe_WCj-8HIrW"
   },
   "source": [
    "# Encoder\n",
    "\n",
    "The *Encoder* is a fully convolutional neural network that that **takes** a tensor of size **(-1, 1, 64, 64)** and **outputs** a tensor of size **(-1, 32, 4, 4)**.\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*   Use **only** *ResBlocks* to **complete** the network\n",
    "*   While doing so you **need** to modify the *in_chanels* and *feature_size*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSJvgpYrHJ6I"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_channels, depth, length, feature_size):\n",
    "    super(Encoder, self).__init__()\n",
    "    encoder = OrderedDict()\n",
    "\n",
    "    for l in range(0, length - 1, 1):\n",
    "      #TODO:Create a ResBlock to have the desired initial\n",
    "      #     feature size\n",
    "      encoder[\"encoder-depth_0-length_\"+str(l)]=ResBlock(\n",
    "        in_channels=in_channels, \n",
    "        out_channels=feature_size, \n",
    "        mode=\"level\"\n",
    "      )\n",
    "      in_channels = feature_size      \n",
    "    for d in range(1, depth + 1, 1):\n",
    "      #TODO:Modify the in_chanels and feature_size accoringly\n",
    "      in_channels = feature_size\n",
    "      feature_size *= 2\n",
    "      #TODO:Create a ResBlock to downsample to the desired\n",
    "      #     feature size\n",
    "      encoder[\"encoder-depth_\"+str(d)+\"-downsample\"]=ResBlock(\n",
    "        in_channels=in_channels, \n",
    "        out_channels=feature_size, \n",
    "        mode=\"downsample\"\n",
    "      )\n",
    "      for l in range(0, length - 1, 1):\n",
    "        #TODO:Create a ResBlock to further process the data\n",
    "        #     keep it at the same feature depth and resolution\n",
    "        encoder[\"encoder-depth_\"+str(d)+\"-length_\"+str(l)]=ResBlock(\n",
    "          in_channels=feature_size, \n",
    "          out_channels=feature_size, \n",
    "          mode=\"level\"\n",
    "        )\n",
    "    self.encoder = nn.Sequential(encoder)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3nUMT12HNIe"
   },
   "source": [
    "# Decoder\n",
    "\n",
    "The *Decoder* is a fully convolutional network that **takes** a tensor of size **(-1, 32, 4, 4)** and **outputs** a tensor of size **(-1, 1, 64, 64)**.\n",
    "\n",
    "----\n",
    "\n",
    "To do : \n",
    "*   Use **only** *ResBlocks* to **complete** the network\n",
    "*   While doing so you **need** to modify *feature_size* and *in_channels*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4HJCLO6HRPl"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, in_channels, depth, length, reconstruction_channels):\n",
    "    super(Decoder, self).__init__()\n",
    "    decoder = OrderedDict()\n",
    "\n",
    "    #TODO:Calculate the initial feature_size\n",
    "    feature_size = in_channels//2\n",
    "\n",
    "    for d in range(depth, 0, -1):\n",
    "      #TODO:Create a ResBlock to upsample to the desired feature\n",
    "      #     size\n",
    "      decoder[\"decoder-depth_\"+str(d)+\"-upsample\"]=ResBlock(\n",
    "        in_channels=in_channels, \n",
    "        out_channels=feature_size, \n",
    "        mode=\"upsample\"\n",
    "      )\n",
    "\n",
    "      for l in range(0, length - 1, 1):\n",
    "        #TODO:Create a ResBlock to further process the data keep\n",
    "        #     it at the same feature depth and resolution\n",
    "        decoder[\"decoder-depth_\"+str(d)+\"-length_\"+str(l)]=ResBlock(\n",
    "          in_channels=feature_size, \n",
    "          out_channels=feature_size, \n",
    "          mode=\"level\"\n",
    "        )\n",
    "        \n",
    "      #TODO:Modify the in_chanels and feature_size accoringly\n",
    "      in_channels = feature_size\n",
    "      feature_size = in_channels//2\n",
    "\n",
    "    #TODO:Create the a ResBlock that outputs the required number\n",
    "    #     of channels for the output\n",
    "    decoder[\"decoder-depth_0-reconstruction\"]= ResBlock(\n",
    "      in_channels=in_channels, \n",
    "      out_channels=reconstruction_channels, \n",
    "      mode=\"level\"\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(decoder)\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.decoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oOO5Bc7HYyN"
   },
   "source": [
    "# Unsupervised Network\n",
    "The *Unsupervised Network* is the class that glues all the components (*Encoder*, *Code Processor*, *Decoder*) that you will use togheter. \n",
    "\n",
    "Its aim is to take an image and give you the reconstruction. It should also be able to encode image, and decode codes. \n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*   **Call** the code processor to process the *Encoder's* output so that is passed to the *Decoder*\n",
    "*   **Call** the needed methods to procces the latent representation of the *Encoder* and get a **code**, and a **code** to get a **spatial latent reprentation**\n",
    "\n",
    "----\n",
    "\n",
    "Tip:\n",
    "*   Any *Code processor* will **have** an **encode** and **decode** method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2TfQvisHeqq"
   },
   "outputs": [],
   "source": [
    "class UnsupervisedNetwork(nn.Module):\n",
    "  def __init__(self, encoder, code_processor, decoder):\n",
    "    super(UnsupervisedNetwork, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.code_processor = code_processor\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    \n",
    "    #TODO:Process encoder's latent representation\n",
    "    #     to go through the code and get the desired\n",
    "    #     input for the decoder\n",
    "    x = self.code_processor(x)\n",
    "    \n",
    "    x = self.decoder(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def encode(self, x):\n",
    "    x = self.encoder(x)\n",
    "    #TODO:Process encoder's latent representation\n",
    "    #     to obtain the code\n",
    "    x = self.code_processor.encode(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def decode(self, x):\n",
    "    #TODO:Process a code to get the required input\n",
    "    #     for the decoder\n",
    "    x = self.code_processor.decode(x)\n",
    "    x = self.decoder(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z1jHyNPr7Sr"
   },
   "source": [
    "# Dense Auto Encoder\n",
    "In a *Dense AutoEncoder* the *Code Processor* receives the latent representation of the *Encoder* and then flattens it into a 1D array. Following that it is further compressed until the desired code dimension. Then it decompresses it while keepint it a 1D array. Finally it reshapes it so it can be passed.\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*   Complete the Dense autoencoder with **Linear** layers \n",
    "*   Code the functionality for forward pass, encoding and decoding\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*   Look at [flatten layer](https://pytorch.org/docs/stable/nn.html#torch.nn.Flatten) and pay attention to the **start_dim** parameter.\n",
    "*   Look at the [view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method of tensors \n",
    "*   Be careful with the feature sizes, you might need to be able to calculate them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRf2Z55fSzzC"
   },
   "outputs": [],
   "source": [
    "class DenseAECodeProcessor(nn.Module):\n",
    "  def __init__(self, feature_size, depth, img_sizes, encoding_sizes, decoding_sizes):\n",
    "    super(DenseAECodeProcessor, self).__init__()\n",
    "\n",
    "    feature_depth = feature_size * np.power(2, depth)\n",
    "    img_sizes = [img_size/np.power(2,depth) for img_size in img_sizes]\n",
    "\n",
    "    flatten_size = int(feature_depth * np.prod(img_sizes))\n",
    "\n",
    "    ecoding_processor = OrderedDict()\n",
    "\n",
    "    in_features = flatten_size\n",
    "\n",
    "    #TODO:Given a list of Linear layers output sizes compress the\n",
    "    #     feature to the desired code size\n",
    "    for i in range(len(encoding_sizes)):\n",
    "      ecoding_processor[\"ecoding_flatten_\" + str(i)] = nn.Linear(\n",
    "          in_features=in_features, \n",
    "          out_features=encoding_sizes[i],\n",
    "      )\n",
    "      in_features = encoding_sizes[i]\n",
    "\n",
    "    self.ecoding_processor = nn.Sequential(ecoding_processor)\n",
    "\n",
    "    decoding_processor = OrderedDict()\n",
    "\n",
    "    #TODO:Given a list of Linear layers output sizes decomporess the\n",
    "    #     feature to the desired size\n",
    "    for i in range(len(decoding_sizes)):\n",
    "      decoding_processor[\"decoding_flatten_\" + str(i)] = nn.Linear(\n",
    "          in_features=in_features, \n",
    "          out_features=decoding_sizes[i],\n",
    "      )\n",
    "      in_features = decoding_sizes[i]\n",
    "    \n",
    "    decoding_processor[\"flatten_\" + str(i+1)] = nn.Linear(\n",
    "      in_features=in_features, \n",
    "      out_features=flatten_size,\n",
    "    )\n",
    "\n",
    "    self.decoding_processor = nn.Sequential(decoding_processor)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #TODO:Remember the sample tensor shape\n",
    "    self.spatial_shape = x.shape[1:]\n",
    "\n",
    "    #TODO:Prepare encoder's latent representation to be processed \n",
    "    #     by the Linear layers\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    \n",
    "    x = self.ecoding_processor(x)\n",
    "    \n",
    "    x = self.decoding_processor(x)\n",
    "    \n",
    "    #TODO:Prepare the tensor to be passed to the decoder\n",
    "    x = x.view((-1, ) + self.spatial_shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def encode(self, x):\n",
    "    #TODO:Prepare encoder's latent representation to be processed \n",
    "    #     by the Linear layers\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.ecoding_processor(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "  def decode(self, x):\n",
    "    #TODO:Prepare the tensor to be passed to the decoder\n",
    "    x = self.decoding_processor(x)\n",
    "    x = x.view((-1, ) + self.spatial_shape)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kq_uLYOtkPoh"
   },
   "source": [
    "Lets now train the network and see what we get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "id": "Sfgxsd_ekP1w",
    "outputId": "002e25c0-62a0-469e-cddc-9d8cc1269f06"
   },
   "outputs": [],
   "source": [
    "network = UnsupervisedNetwork(\n",
    "  encoder=Encoder(\n",
    "    in_channels=1, \n",
    "    depth=NETWORK_DEPTH, \n",
    "    length=NETWORK_LENGTH, \n",
    "    feature_size=FEATURE_SIZE\n",
    "  ),\n",
    "  code_processor=DenseAECodeProcessor(**{\n",
    "    \"feature_size\":FEATURE_SIZE, \n",
    "    \"depth\":NETWORK_DEPTH, \n",
    "    \"img_sizes\":[192,192], \n",
    "    \"encoding_sizes\":[1024,512,256],\n",
    "    \"decoding_sizes\":[512,1024],\n",
    "  }),\n",
    "  decoder=Decoder(\n",
    "    in_channels=np.power(FEATURE_SIZE,NETWORK_DEPTH+1), \n",
    "    depth=NETWORK_DEPTH, \n",
    "    length=NETWORK_LENGTH, \n",
    "    reconstruction_channels=1\n",
    "  ),\n",
    ")\n",
    "\n",
    "network = train_network(training_loader, testing_loader, network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is4XBsGTRBXt"
   },
   "source": [
    "# Dimensionality reduction visualization\n",
    "\n",
    "The *dimensionality_reduction_analysis* method given a training dataset loader, a testing dataset loader and a network will show a scatter plot of a 2D PCA dimensionality reduction of training and testing samples.\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*   Reshape the numpy arrays into the needed shape to be fed into PCA\n",
    "*   Project the *training_encodings* into 2D\n",
    "*   Project the *testing_encodings* into 2D\n",
    "\n",
    "----\n",
    "\n",
    "Tips\n",
    "*    Might wanna look at [numpy.reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJ0-7aYSRBiL"
   },
   "outputs": [],
   "source": [
    "def dimensionality_reduction_analysis(training_loader, testing_loader, network):\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  network.cuda(device)\n",
    "\n",
    "  training_encodings = None\n",
    "  for x in training_loader:\n",
    "    x = x.to(device, dtype=torch.float)\n",
    "    x = network.encode(x)\n",
    "    x = numpy_from_tensor(x)\n",
    "    training_encodings =  (\n",
    "        x if training_encodings is None\n",
    "        else np.concatenate((training_encodings, x), axis=0)\n",
    "    )\n",
    "  training_encodings = np.array(training_encodings)\n",
    "  #TODO:Reshape the data to be fed into PCA\n",
    "  training_encodings = training_encodings.reshape(\n",
    "      training_encodings.shape[0],\n",
    "      -1\n",
    "  )\n",
    "\n",
    "  testing_encodings = None\n",
    "  for x in testing_loader:\n",
    "    x = x.to(device, dtype=torch.float)\n",
    "    x = network.encode(x)\n",
    "    x = numpy_from_tensor(x)\n",
    "    testing_encodings =  (\n",
    "        x if testing_encodings is None\n",
    "        else np.concatenate((testing_encodings, x), axis=0)\n",
    "    )\n",
    "  testing_encodings = np.array(testing_encodings)\n",
    "  #TODO:Reshape the data to be fed into PCA\n",
    "  testing_encodings = testing_encodings.reshape(\n",
    "      testing_encodings.shape[0],\n",
    "      -1\n",
    "  )\n",
    "\n",
    "  pca = PCA(**PCA_PARAMETERS)\n",
    "  #TODO:Do dimensionality reduction on training encodings\n",
    "  training_reduction = pca.fit_transform(training_encodings)\n",
    "  #TODO:Do dimensionality reduction on testing encodings\n",
    "  testing_reduction = pca.transform(testing_encodings)\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.scatter(\n",
    "    x=training_reduction[:,0], \n",
    "    y=training_reduction[:,1],\n",
    "    c=\"blue\",\n",
    "    label=\"Training Data\"\n",
    "  )\n",
    "  ax.scatter(\n",
    "    x=testing_reduction[:,0], \n",
    "    y=testing_reduction[:,1],\n",
    "    c=\"red\",\n",
    "    label=\"Testing Data\"\n",
    "  )\n",
    "  ax.set_xlabel(\"Dimension 1\")\n",
    "  ax.set_ylabel(\"Dimension 2\")\n",
    "  \n",
    "  min_x = np.min([np.min(training_reduction[:,0]),np.min(testing_reduction[:,0])])\n",
    "  min_x = min_x - np.abs(min_x*0.25)\n",
    "\n",
    "  max_x = np.max([np.max(training_reduction[:,0]),np.max(testing_reduction[:,0])])\n",
    "  max_x = max_x + max_x*0.25\n",
    "  \n",
    "  ax.set_xlim(min_x,max_x)\n",
    "  \n",
    "  min_y = np.min([np.min(training_reduction[:,1]),np.min(testing_reduction[:,1])])\n",
    "  min_y = min_y - np.abs(min_y*0.25)\n",
    "\n",
    "  max_y = np.max([np.max(training_reduction[:,1]),np.max(testing_reduction[:,1])])\n",
    "  max_y = max_y + max_y*0.25\n",
    "\n",
    "  ax.set_ylim(min_y,max_y)\n",
    "  plt.legend(loc = 'best')\n",
    "  plt.show()\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SA7Y_loS-3u"
   },
   "source": [
    "Now lets see what we get if we run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "LYzC3W-lTCCI",
    "outputId": "dfa40139-995f-4ef4-b1e0-1eaaf8c834bd"
   },
   "outputs": [],
   "source": [
    "dimensionality_reduction_analysis(\n",
    "  training_loader=training_loader,\n",
    "  testing_loader=testing_loader,\n",
    "  network=network\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZXk3uYmTF3n"
   },
   "source": [
    "# Interpolation \n",
    "The *interpolation_analysis* method given a training dataset loader, a testing dataset loader and a network will show:\n",
    "*   An interpolation between two training samples\n",
    "*   An interpolation between two testing samples\n",
    "*   And two interpolations between training and testing samples\n",
    "\n",
    "For two encodings ${z}_{1}$ and ${z}_{2}$ and a decoder $D$ interpolation can be defined as: $D({z}_{1}*(1-\\alpha)+\\alpha*{z}_{2})$\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*   Calculate alpha based codes\n",
    "*   Get the samples based on those codes\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*   Do not worry if it does not look like you expect, it takes a lot of time for interpolation to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P1EnkNBTGSw"
   },
   "outputs": [],
   "source": [
    "def interpolation_analysis(training_loader, testing_loader, network):\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  network.cuda(device)\n",
    "\n",
    "  training_batch = next(iter(training_loader))\n",
    "  training_sample_0 = training_batch[0]\n",
    "  training_sample_1 = training_batch[1]\n",
    "\n",
    "  testing_batch = next(iter(testing_loader))\n",
    "  testing_sample_0 = testing_batch[0]\n",
    "  testing_sample_1 = testing_batch[1]\n",
    "\n",
    "  combinations={\n",
    "      \"Training samples\":(training_sample_0, training_sample_1),\n",
    "      \"Testing samples\": (testing_sample_0, testing_sample_1),\n",
    "      \"Training to Testing 1\": (training_sample_0, testing_sample_0),\n",
    "      \"Training to Testing 2\": (training_sample_1, testing_sample_1)\n",
    "  }\n",
    "\n",
    "  for name, samples in combinations.items():\n",
    "    sample_0 = samples[0].to(device, dtype=torch.float).unsqueeze(0)\n",
    "    sample_1 = samples[1].to(device, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    code_0 = network.encode(sample_0)\n",
    "    code_1 = network.encode(sample_1)\n",
    "    for i, alpha in enumerate(np.linspace(0,1,5)):\n",
    "      #TODO:Calcualte the interpolation code\n",
    "      alpha_code = code_0*(1-alpha) + code_1*alpha\n",
    "      #TODO:Get the interpolated sample\n",
    "      alpha_sample = numpy_from_tensor(network.decode(alpha_code))\n",
    "\n",
    "      plt.figure()\n",
    "      f, ax = plt.subplots()\n",
    "      ax.imshow(np.squeeze(alpha_sample), cmap=\"gray\") \n",
    "      ax.axis('off')\n",
    "      ax.title.set_text(name + \" | Interpolation \" + str(i))\n",
    "      plt.show()\n",
    "      plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbN2iD5KUryf"
   },
   "source": [
    "Now lets see what we get if we run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GEYJIqcFUr8l",
    "outputId": "5c3d2389-bb3c-4f31-fd4c-c649167cf27a"
   },
   "outputs": [],
   "source": [
    "interpolation_analysis(\n",
    "  training_loader=training_loader,\n",
    "  testing_loader=testing_loader,\n",
    "  network=network\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6URb_M4a-B1"
   },
   "source": [
    "# Experiment design\n",
    "Now you need to put everything togheter and make it as a single experiment.\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*    Compile the network definition, network training, dimensionality reductiona and interpolation analysis into a single method that can be later used.\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*   Do not overthink it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXMaE_TNa-IB"
   },
   "outputs": [],
   "source": [
    "def experiment(code_processor_class, code_processor_parameters, network_depth, network_length, feature_size):\n",
    "  #TODO:Put togheter the creation of the network, the training \n",
    "  #     and two analysis that you have coded up until now. This\n",
    "  #     method should only **call** the other ones\n",
    "  network = UnsupervisedNetwork(\n",
    "    encoder=Encoder(\n",
    "      in_channels=1, \n",
    "      depth=network_depth, \n",
    "      length=network_length, \n",
    "      feature_size=feature_size\n",
    "    ),\n",
    "    code_processor=code_processor_class(**code_processor_parameters),\n",
    "    decoder=Decoder(\n",
    "      in_channels=np.power(feature_size,network_depth+1), \n",
    "      depth=network_depth, \n",
    "      length=network_length, \n",
    "      reconstruction_channels=1\n",
    "    ),\n",
    "  )\n",
    "\n",
    "  network = train_network(\n",
    "      training_loader=training_loader,\n",
    "      testing_loader=testing_loader,\n",
    "      network=network,\n",
    "  )\n",
    "\n",
    "  dimensionality_reduction_analysis(\n",
    "      training_loader=training_loader,\n",
    "      testing_loader=testing_loader,\n",
    "      network=network\n",
    "  )\n",
    "\n",
    "  interpolation_analysis(\n",
    "      training_loader=training_loader,\n",
    "      testing_loader=testing_loader,\n",
    "      network=network,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bm_G4XJGw0wn"
   },
   "source": [
    "Now lets see what we get if we run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LDxqONv5w029",
    "outputId": "3af21250-ea3a-4c18-dfd3-56fb8bd80560"
   },
   "outputs": [],
   "source": [
    "experiment(\n",
    "    code_processor_class=DenseAECodeProcessor, \n",
    "    code_processor_parameters={\n",
    "      \"feature_size\":FEATURE_SIZE, \n",
    "      \"depth\":NETWORK_DEPTH, \n",
    "      \"img_sizes\":[192,192], \n",
    "      \"encoding_sizes\":[1024,512,256],\n",
    "      \"decoding_sizes\":[512,1024],\n",
    "    }, \n",
    "    network_depth=NETWORK_DEPTH, \n",
    "    network_length=NETWORK_LENGTH, \n",
    "    feature_size=FEATURE_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1VYeO5SDbL5"
   },
   "source": [
    "#Spatial AutoEncoder\n",
    "For a *Spatial AutoEncoder* the *Code Processor* only passes the latent representation of the *Encoder* to the *Decoder*\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*    Code the missing pices\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*    Do not overthink it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwsFMddquFaA"
   },
   "outputs": [],
   "source": [
    "class SpatialAECodeProcessor(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(SpatialAECodeProcessor, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "      #TODO:Code any tensor processing if needed\n",
    "      return x\n",
    "\n",
    "    def encode(self, x):\n",
    "      #TODO:Code any tensor processing if needed\n",
    "      return x\n",
    "\n",
    "    def decode(self, x):\n",
    "      #TODO:Code any tensor processing if needed\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4B6FURwN5Uf"
   },
   "source": [
    "Now lets see what we get if we run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_2_i1V6QN6Ol",
    "outputId": "71e08c3a-04fa-46f4-d59f-9fa10cfcd679"
   },
   "outputs": [],
   "source": [
    "experiment(\n",
    "    code_processor_class=SpatialAECodeProcessor, \n",
    "    code_processor_parameters={}, \n",
    "    network_depth=NETWORK_DEPTH, \n",
    "    network_length=NETWORK_LENGTH, \n",
    "    feature_size=FEATURE_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E67nL9fEOQ10"
   },
   "source": [
    "# Training Method (AE and VAE)\n",
    "It is time to now modify most of the methods you wrote and enable them to **also** receive a **Variational AutoEncoder**.\n",
    "\n",
    "The KL divergence is:\n",
    "\n",
    "$D_{K L}[N(\\mu(X), \\Sigma(X)) \\| N(0,1)]=\\frac{1}{2} \\sum_{k}\\left(\\exp (\\Sigma(X))+\\mu^{2}(X)-1-\\Sigma(X)\\right)$\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*   Modify the signature of the method if needed\n",
    "*   Modify the call to the forward pass \n",
    "*   Calculate the loss\n",
    "*   Calculate the test loss\n",
    "*   Change between training and testing modes\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*   Do not forget the method should be able to receive **either** a VAE or an AE\n",
    "*   A **VAE** behaves **differently** in training and inference/testing/validation mode so you **can assume** that the VAEs will have a **.set_is_training()** method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRTaXCcsORB3"
   },
   "outputs": [],
   "source": [
    "#TODO:Modify the signature if needed\n",
    "def train_network(training_loader, testing_loader, network, is_vae):\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  network.cuda(device)\n",
    "\n",
    "  optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "  l1_loss = L1Loss()\n",
    "  l2_loss = MSELoss()\n",
    "\n",
    "  # Train the network\n",
    "  for epoch in range(EPOCHS):\n",
    "    for x in training_loader:\n",
    "      # Move input to device\n",
    "      x = x.to(device, dtype=torch.float)\n",
    "\n",
    "      # Zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      #TODO:Forward pass such that it can take either\n",
    "      #     a VAE or an AE\n",
    "      if is_vae:\n",
    "        x_hat, code_mu, code_logvar = network(x)\n",
    "      else:\n",
    "        x_hat = network(x)\n",
    "\n",
    "      #TODO:Calculate loss such that it can take either\n",
    "      #     a VAE or an AE\n",
    "      loss = l1_loss(x, x_hat) + l2_loss(x, x_hat)\n",
    "\n",
    "      if is_vae:\n",
    "        code_logvar = torch.flatten(code_logvar, start_dim=1)\n",
    "        code_mu = torch.flatten(code_mu, start_dim=1)\n",
    "        kl_divergence = (\n",
    "          -0.5 * \n",
    "          torch.sum(\n",
    "            1 + code_logvar - \n",
    "            code_mu.pow(2) - code_logvar.exp()\n",
    "          )\n",
    "        )\n",
    "        kl_divergence = kl_divergence.mean()\n",
    "        loss += kl_divergence\n",
    "\n",
    "\n",
    "      # Back-propagation of gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # Optimize\n",
    "      optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      #TODO:Turn the network in testing mode\n",
    "      network.set_is_training(is_training=False)\n",
    "\n",
    "      #TODO:Initialize accumulators\n",
    "      l1_test_loss = 0 \n",
    "      l2_test_loss = 0 \n",
    "      kl_test_loss = 0 \n",
    "      for i, x in enumerate(testing_loader):\n",
    "        x = x.to(device, dtype=torch.float)\n",
    "\n",
    "        #TODO:Forward pass such that it can take either\n",
    "        #     a VAE or an AE\n",
    "        if is_vae:\n",
    "          x_hat, code_mu, code_logvar = network(x)\n",
    "        else:\n",
    "          x_hat = network(x)\n",
    "\n",
    "        #TODO:Calculate loss such that it can take either\n",
    "        #     a VAE or an AE\n",
    "        l1_test_loss = l1_loss(x, x_hat)\n",
    "        l2_test_loss = l2_loss(x, x_hat)\n",
    "        if is_vae:\n",
    "          code_logvar = torch.flatten(code_logvar, start_dim=1)\n",
    "          code_mu = torch.flatten(code_mu, start_dim=1)\n",
    "          kl_divergence = (\n",
    "            -0.5 * \n",
    "            torch.sum(\n",
    "              1 + code_logvar - \n",
    "              code_mu.pow(2) - code_logvar.exp()\n",
    "            )\n",
    "          )\n",
    "          kl_test_loss += kl_divergence.mean()\n",
    "\n",
    "    \n",
    "      print(\n",
    "          \"==== Epoch: \" + str(epoch) + \n",
    "          \" | L1 loss: \" + str(numpy_from_tensor(l1_test_loss/i)) + \n",
    "          \" | L2 loss: \" + str(numpy_from_tensor(l2_test_loss/i)) +\n",
    "          \" | KL loss: \" + str(numpy_from_tensor(kl_test_loss/i)) +\n",
    "          \" | Total Loss: \" + str(numpy_from_tensor((l1_test_loss+l2_test_loss+kl_test_loss)/i))+ \" =====\")\n",
    "      display_results(x, x_hat)\n",
    "    #TODO:Turn the network back into training mode\n",
    "    network.set_is_training(is_training=True)\n",
    "  return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7saMjjAtPsYH"
   },
   "source": [
    "# Unsupervised Network (AE and VAE)\n",
    "You need to modify the class *UnsupervisedNetwork* such that is able to hold either a AE or a VAE.\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*    Modify the signature of the *__init__* if needed\n",
    "*    Modify the return elements of that the *code_processor()* returns since a VAE includes also a mean and a logvar besides the code\n",
    "*    Same as above for the forward pass\n",
    "*    Implement a class variable that tells us if the network is training or not\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*    Do not overthink it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TS5EGocAPtpO"
   },
   "outputs": [],
   "source": [
    "class UnsupervisedNetwork(nn.Module):\n",
    "  #TODO:Modify the signature if needed\n",
    "  def __init__(self, encoder, code_processor, decoder, is_vae):\n",
    "    super(UnsupervisedNetwork, self).__init__()\n",
    "    #TODO:Modify the class variables if needed\n",
    "    self.is_vae = is_vae\n",
    "    self.is_training=True\n",
    "    self.encoder = encoder\n",
    "    self.code_processor = code_processor\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    \n",
    "    #TODO:Process encoder's latent representation\n",
    "    #     to go through the code and get the desired\n",
    "    #     input for the decoder\n",
    "    if self.is_vae:\n",
    "      x, mu, logvar = self.code_processor(x)\n",
    "    else:\n",
    "      x = self.code_processor(x)\n",
    "    \n",
    "    x = self.decoder(x)\n",
    "\n",
    "    #TODO:change the output such that it matches\n",
    "    #     either a VAE or an AE\n",
    "    if self.is_vae:\n",
    "      return x, mu, logvar\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "  def encode(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.code_processor.encode(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def decode(self, x):\n",
    "    x = self.code_processor.decode(x)\n",
    "    x = self.decoder(x)\n",
    "    return x\n",
    "  \n",
    "  #TODO:You might need a function to change\n",
    "  #     if a network is training or not \n",
    "  def set_is_training(self, is_training):\n",
    "    self.code_processor.set_is_training(is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRD3GxWYQI46"
   },
   "source": [
    "# Experiment design (AE and VAE)\n",
    "You need to modify the method *experiment* such that it si able to take either a AE or a VAE.\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*    Modify the signatures of the methods to adhere to all the modifications that you made\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*    Do not overthink it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTxVAA9jQJGp"
   },
   "outputs": [],
   "source": [
    "#TODO:Change the signature if needed and any other signature\n",
    "def experiment(code_processor_class, code_processor_parameters, network_depth, network_length, feature_size, is_vae):\n",
    "  network = UnsupervisedNetwork(\n",
    "    encoder=Encoder(\n",
    "      in_channels=1, \n",
    "      depth=network_depth, \n",
    "      length=network_length, \n",
    "      feature_size=feature_size\n",
    "    ),\n",
    "    decoder=Decoder(\n",
    "      in_channels=np.power(feature_size,network_depth+1), \n",
    "      depth=network_depth, \n",
    "      length=network_length, \n",
    "      reconstruction_channels=1\n",
    "    ),\n",
    "    code_processor=code_processor_class(**code_processor_parameters),\n",
    "    is_vae=is_vae\n",
    "  )\n",
    "\n",
    "  train_network(\n",
    "      training_loader=training_loader,\n",
    "      testing_loader=testing_loader,\n",
    "      network=network,\n",
    "      is_vae=is_vae\n",
    "  )\n",
    "\n",
    "  dimensionality_reduction_analysis(\n",
    "      training_loader=training_loader,\n",
    "      testing_loader=testing_loader,\n",
    "      network=network\n",
    "  )\n",
    "\n",
    "  interpolation_analysis(\n",
    "      training_loader=training_loader,\n",
    "      testing_loader=testing_loader,\n",
    "      network=network,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT_T6q1DDuBK"
   },
   "source": [
    "# Dense Variational AutoEncoder\n",
    "In a *Dense Variational AutoEncoder* the *Code Processor* receives the latent representation of the *Encoder* and then flattens it into a 1D array. Following that it is further compressed until the desired code dimension. Then a **mean** and **logvariance** are estimated and used in the **kernel trick**. The result of the kernel trick is  then decompresses while beeing keept a 1D array. Finally it reshapes it so it can be passed to the *Decoder*.\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*   Complete the Dense autoencoder with **Linear** layers \n",
    "*   Implement the *kernel trick*\n",
    "*   Code the functionality for forward pass, encoding and decoding\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*   Look at [flatten layer](https://pytorch.org/docs/stable/nn.html#torch.nn.Flatten) and pay attention to the **start_dim** parameter.\n",
    "*   Look at the [view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method of tensors \n",
    "*   Be careful with the feature sizes, you might need to be able to calculate them\n",
    "*   Might need to use [clamp](https://pytorch.org/docs/stable/torch.html#torch.clamp) on logvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaYjI3MXDgVn"
   },
   "outputs": [],
   "source": [
    "class DenseVAECodeProcessor(nn.Module):\n",
    "  #TODO:Change the signature if needed\n",
    "  def __init__(self, feature_size, depth, img_sizes, encoding_sizes, decoding_sizes, is_training):\n",
    "    super(DenseVAECodeProcessor, self).__init__()\n",
    "    self.logvars_upper_bound = 50\n",
    "    self.logvars_lower_bound = -self.logvars_upper_bound\n",
    "    \n",
    "    self.is_training = is_training\n",
    "    feature_depth = feature_size * np.power(2, depth)\n",
    "    img_sizes = [img_size/np.power(2,depth) for img_size in img_sizes]\n",
    "\n",
    "    flatten_size = int(feature_depth * np.prod(img_sizes))\n",
    "\n",
    "    ecoding_processor = OrderedDict()\n",
    "\n",
    "    in_features = flatten_size\n",
    "\n",
    "    #TODO:Given a list of Linear layers output sizes compress the\n",
    "    #     feature to the desired code size\n",
    "    for i in range(len(encoding_sizes)):\n",
    "      ecoding_processor[\"ecoding_flatten_\" + str(i)] = nn.Linear(\n",
    "          in_features=in_features, \n",
    "          out_features=encoding_sizes[i],\n",
    "      )\n",
    "      in_features = encoding_sizes[i]\n",
    "\n",
    "    self.ecoding_processor = nn.Sequential(ecoding_processor)\n",
    "\n",
    "    #TODO:Create Linear layers for both logvariance and mean\n",
    "    self.logvar = nn.Linear(\n",
    "      in_features=in_features, \n",
    "      out_features=in_features,\n",
    "    )\n",
    "\n",
    "    self.mu = nn.Linear(\n",
    "      in_features=in_features, \n",
    "      out_features=in_features,\n",
    "    )\n",
    "\n",
    "    decoding_processor = OrderedDict()\n",
    "\n",
    "    #TODO:Given a list of Linear layers output sizes decompress the\n",
    "    #     code to the needed size for the decoder\n",
    "    for i in range(len(decoding_sizes)):\n",
    "      decoding_processor[\"decoding_flatten_\" + str(i)] = nn.Linear(\n",
    "          in_features=in_features, \n",
    "          out_features=decoding_sizes[i],\n",
    "      )\n",
    "      in_features = decoding_sizes[i]\n",
    "    \n",
    "    decoding_processor[\"flatten_\" + str(i+1)] = nn.Linear(\n",
    "      in_features=in_features, \n",
    "      out_features=flatten_size,\n",
    "    )\n",
    "\n",
    "    self.decoding_processor = nn.Sequential(decoding_processor)\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.spatial_shape = x.shape[1:]\n",
    "\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.ecoding_processor(x)\n",
    "    \n",
    "    #TODO:Code the reparametrisation trick you will need to\n",
    "    #     you need to take into account if the network\n",
    "    #     is training or not\n",
    "    logvar = torch.clamp(\n",
    "        self.logvar(x),\n",
    "        self.logvars_lower_bound,\n",
    "        self.logvars_upper_bound\n",
    "    )\n",
    "\n",
    "    mu = self.mu(x)\n",
    "\n",
    "    if self.is_training:\n",
    "      std = logvar.mul(0.5).exp_()\n",
    "      esp = torch.randn_like(mu)\n",
    "      x = mu + std * esp\n",
    "    else:\n",
    "      x = mu\n",
    "\n",
    "    x = self.decoding_processor(x)\n",
    "    x = x.view((-1,) + self.spatial_shape)\n",
    "\n",
    "    return x, mu, logvar\n",
    "\n",
    "  def encode(self, x):\n",
    "    #TODO:Code the necessary processing of the latent \n",
    "    #     representation\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.ecoding_processor(x)\n",
    "    x = self.mu(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def decode(self, x):\n",
    "    #TODO:Code the necessary processing of the latent \n",
    "    #     representation\n",
    "    x = self.decoding_processor(x)\n",
    "    x = x.view((-1,)+self.spatial_shape)\n",
    "\n",
    "    return x\n",
    "  \n",
    "  #TODO:You might need a function to change\n",
    "  #     if a network is training or not\n",
    "  def set_is_training(self, is_training):\n",
    "    self.is_training = is_training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6T9e4RhOL4i"
   },
   "source": [
    "Now lets see what we get if we run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aSRmcoFBOMMe",
    "outputId": "ce205417-472f-4285-d6c2-d6595dab8c01"
   },
   "outputs": [],
   "source": [
    "#TODO:Make any signatures changes that you did in\n",
    "#     the last cell that edited this method\n",
    "experiment(\n",
    "    code_processor_class=DenseVAECodeProcessor, \n",
    "    code_processor_parameters={\n",
    "      \"feature_size\":FEATURE_SIZE, \n",
    "      \"depth\":NETWORK_DEPTH, \n",
    "      \"img_sizes\":[192,192], \n",
    "      \"encoding_sizes\":[1024,512,256],\n",
    "      \"decoding_sizes\":[512,1024],\n",
    "      \"is_training\":True,\n",
    "    }, \n",
    "    network_depth=NETWORK_DEPTH, \n",
    "    network_length=NETWORK_LENGTH, \n",
    "    feature_size=FEATURE_SIZE,\n",
    "    is_vae=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDTWPYGY5QO-"
   },
   "source": [
    "# Spatial Variational AutoEncoder\n",
    "In a *Spatial Variational AutoEncoder* the *Code Processor* receives the latent representation of the *Encoder* and then a **mean** and **logvariance** are estimated and used in the **kernel trick**. The result of the kernel trick is then passed to the *Decoder*.\n",
    "\n",
    "----\n",
    "\n",
    "To do:\n",
    "*   Estimate the mean and variance\n",
    "*   Implement the *kernel trick*\n",
    "*   Code the functionality for forward pass, encoding and decoding\n",
    "\n",
    "----\n",
    "\n",
    "Tips:\n",
    "*   Might need to use [clamp](https://pytorch.org/docs/stable/torch.html#torch.clamp) on logvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gu_fxP8QgHK"
   },
   "outputs": [],
   "source": [
    "class SpatialVAECodeProcessor(nn.Module):\n",
    "    def __init__(self, feature_size, depth, is_training):\n",
    "      super(SpatialVAECodeProcessor, self).__init__()\n",
    "      self.logvars_upper_bound = 50\n",
    "      self.logvars_lower_bound = -self.logvars_upper_bound\n",
    "      self.is_training = is_training\n",
    "      \n",
    "      feature_depth = feature_size * np.power(2, depth)\n",
    "      \n",
    "      #TODO:Create 2D Convolutional layers for the logvar\n",
    "      #     and mean\n",
    "      self.logvar = nn.Conv2d(\n",
    "        in_channels=feature_depth, \n",
    "        out_channels=feature_depth, \n",
    "        kernel_size=3, \n",
    "        stride=1,\n",
    "        padding=1, \n",
    "      )\n",
    "\n",
    "      self.mu = nn.Conv2d(\n",
    "        in_channels=feature_depth, \n",
    "        out_channels=feature_depth, \n",
    "        kernel_size=3, \n",
    "        stride=1,\n",
    "        padding=1, \n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      #TODO:Code the reparametrisation trick you will need to\n",
    "      #     you need to take into account if the network\n",
    "      #     is training or not\n",
    "      logvar = torch.clamp(\n",
    "          self.logvar(x),\n",
    "          self.logvars_lower_bound,\n",
    "          self.logvars_upper_bound\n",
    "      )\n",
    "\n",
    "      mu = self.mu(x)\n",
    "\n",
    "      if self.is_training:\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = torch.randn_like(mu)\n",
    "        x = mu + std * esp\n",
    "      else:\n",
    "        x = mu\n",
    "\n",
    "      return x, mu, logvar\n",
    "    \n",
    "    def encode(self, x):\n",
    "      #TODO:Code the necessary processing of the latent\n",
    "      #     representation\n",
    "      x = self.mu(x)\n",
    "      return x\n",
    "\n",
    "    def decode(self, x):\n",
    "      return x\n",
    "\n",
    "    def set_is_training(self, is_training):\n",
    "      self.is_training = is_training   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mssWjKTMOEtm"
   },
   "source": [
    "Now lets see what we get if we run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iugFssgIOFEH",
    "outputId": "4cddee64-47ee-44dd-8f59-87ad54ae05df"
   },
   "outputs": [],
   "source": [
    "#TODO:Make any signatures changes that you did in\n",
    "#     the last cell that edited this method\n",
    "experiment(\n",
    "    code_processor_class=SpatialVAECodeProcessor, \n",
    "    code_processor_parameters={\n",
    "      \"feature_size\":FEATURE_SIZE, \n",
    "      \"depth\":NETWORK_DEPTH+1, \n",
    "      \"is_training\":True,\n",
    "    }, \n",
    "    network_depth=NETWORK_DEPTH+1, \n",
    "    network_length=NETWORK_LENGTH, \n",
    "    feature_size=FEATURE_SIZE,\n",
    "    is_vae=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture_8_unsupervised_learning_practical_complete.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
